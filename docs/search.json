[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Camilo Martinez",
    "section": "",
    "text": "Business Analytics Master’s Student | Data Enthusiast | Finance Passionate\nI am driven by a passion for learning and a deep curiosity about data. I believe numbers offer the clearest path to understanding reality, which is why I advocate for learning through data-driven insights. Originally from Colombia, I now live in New York, where I am pursuing a Master’s in Business Analytics. My focus lies in leveraging analytics to solve complex problems, particularly within the finance industry. I am excited to continue learning and growing in this field, and I am always looking for new opportunities to apply my skills and knowledge."
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "",
    "text": "Welcome to an exciting exploration of the film industry! In this project, we dive deep into what makes movies successful, analyzing IMDb ratings, audience engagement, and revenue to create a predictive model that can foresee a film’s potential. We’ve uncovered fascinating trends in how genres evolve over time, providing valuable insights for future productions.\nFinally, we present our proposed remake of the iconic North by Northwest, a thrilling opportunity to bring a timeless classic to life for modern audiences. With its strong foundation and fresh vision, we believe this project holds the key to the next big success in cinema!"
  },
  {
    "objectID": "mp02.html#introduction",
    "href": "mp02.html#introduction",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "",
    "text": "Welcome to an exciting exploration of the film industry! In this project, we dive deep into what makes movies successful, analyzing IMDb ratings, audience engagement, and revenue to create a predictive model that can foresee a film’s potential. We’ve uncovered fascinating trends in how genres evolve over time, providing valuable insights for future productions.\nFinally, we present our proposed remake of the iconic North by Northwest, a thrilling opportunity to bring a timeless classic to life for modern audiences. With its strong foundation and fresh vision, we believe this project holds the key to the next big success in cinema!"
  },
  {
    "objectID": "mp02.html#data-source",
    "href": "mp02.html#data-source",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Data Source",
    "text": "Data Source\nFor this project, we utilize data from the Internet Movie Database (IMDb), a widely recognized source of comprehensive movie information. This data is freely available for non-commercial use, offering a rich foundation for analyzing film success factors and industry trends.\nTo download the IMDb datasets into R and begin your own analysis, you can use the following code:\n\nget_imdb_file &lt;- function(fname){\n    BASE_URL &lt;- \"https://datasets.imdbws.com/\"\n    fname_ext &lt;- paste0(fname, \".tsv.gz\")\n    if(!file.exists(fname_ext)){\n        FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n        download.file(FILE_URL, \n                      destfile = fname_ext)\n    }\n    as.data.frame(readr::read_tsv(fname_ext, lazy=FALSE))\n}\n\nNAME_BASICS &lt;- get_imdb_file(\"name.basics\")\n\nFor readability, we change the name of the following tables:\n\nTITLE_BASICS     &lt;- get_imdb_file(\"title.basics\")\nTITLE_EPISODES   &lt;- get_imdb_file(\"title.episode\")\nTITLE_RATINGS    &lt;- get_imdb_file(\"title.ratings\")\nTITLE_CREW       &lt;- get_imdb_file(\"title.crew\")\nTITLE_PRINCIPALS &lt;- get_imdb_file(\"title.principals\")\n\nDue to the large size of these files, loading them may take some time. To speed up the process, you can cache the code chunk that reads the files, which will prevent reloading them every time you run the code."
  },
  {
    "objectID": "mp02.html#data-sub-sampling",
    "href": "mp02.html#data-sub-sampling",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Data Sub-Sampling",
    "text": "Data Sub-Sampling\nSince the data set contains a vast number of data points, we will narrow it down to ensure smooth analysis. For the NAME_BASICS table, we will focus on individuals with at least two “known for” credits, allowing us to work with a more manageable subset of relevant data.\n\nlibrary(dplyr)\nlibrary(stringr)\n\nNAME_BASICS &lt;- NAME_BASICS |&gt; \n    filter(str_count(knownForTitles, \",\") &gt; 1)\n\nIMDb has a long tail of obscure movies:\n\nlibrary(ggplot2)\nlibrary(scales)\n\nTITLE_RATINGS |&gt;\n    ggplot(aes(x=numVotes)) + \n    geom_histogram(bins=30) +\n    xlab(\"Number of IMDB Ratings\") +\n    ylab(\"Number of Titles\") + \n    ggtitle(\"Majority of IMDB Titles Have Less than 100 Ratings\") + \n    theme_bw() + \n    scale_x_log10(label=scales::comma) + \n    scale_y_continuous(label=scales::comma)\n\n\n\n\n\n\n\n\nAs we can see, the majority of titles have fewer than 100 ratings. Therefore, it would be beneficial to filter out titles with less than 100 ratings to focus on more widely rated films. This becomes even clearer in the following visualization:\n\nTITLE_RATINGS |&gt;\n    pull(numVotes) |&gt;\n    quantile()\n\n     0%     25%     50%     75%    100% \n      5      11      26     100 2954275 \n\n\nBy applying this drop, we significantly reduce the size of our data set:\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n    filter(numVotes &gt;= 100)\n\nNext, we will join the tables to filter out titles with fewer than 100 ratings. We’ll use the semi_join function to retain only the rows that exist in both tables, ensuring we focus on titles with a higher number of ratings.\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_EPISODES_1 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\nTITLE_EPISODES_2 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(parentTconst == tconst))\n\nTITLE_EPISODES &lt;- bind_rows(TITLE_EPISODES_1,\n                            TITLE_EPISODES_2) |&gt;\n    distinct()\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n    semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\n\nrm(TITLE_EPISODES_1)\nrm(TITLE_EPISODES_2)"
  },
  {
    "objectID": "mp02.html#taking-a-look-at-the-data",
    "href": "mp02.html#taking-a-look-at-the-data",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Taking a Look at the Data",
    "text": "Taking a Look at the Data\nWe will first inspect the data to determine if any changes to the data types are necessary. In this dataset, most columns are being read as character types. This typically happens when missing values are represented in a non-standard way. For example, in these files, missing values are represented as \\N. Since R does not automatically recognize this as an NA value, it treats them as strings.\nTo address this, we need to:\n\nUse the mutate function to modify the columns.\nApply the as.numeric function to convert columns to the correct data type.\n\nWe can clean the datasets using the following code:\n\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n    mutate_if(is.character, ~na_if(., \"\\\\N\")) |&gt;\n    mutate(birthYear = as.numeric(birthYear),\n           deathYear = as.numeric(deathYear))\n\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n    mutate_if(is.character, ~na_if(., \"\\\\N\")) |&gt;\n    mutate(\n        startYear = as.numeric(startYear), \n        endYear = as.numeric(endYear),    \n        runtimeMinutes = as.numeric(runtimeMinutes)  \n    )\n\n\nTITLE_EPISODES &lt;- TITLE_EPISODES |&gt;\n    mutate_if(is.character, ~na_if(., \"\\\\N\")) |&gt;\n    mutate(\n        seasonNumber = as.numeric(seasonNumber),\n        episodeNumber = as.numeric(episodeNumber)\n    )\n\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n    mutate_if(is.character, ~na_if(., \"\\\\N\")) |&gt;\n    mutate(\n        averageRating = as.numeric(averageRating), \n        numVotes = as.numeric(numVotes)  \n    )\n\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n    mutate_if(is.character, ~na_if(., \"\\\\N\"))\n\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n    mutate_if(is.character, ~na_if(., \"\\\\N\")) |&gt;\n    mutate(characters = str_replace_all(characters, '\\\\[|\\\\\"|\\\\]', '')) #clean undesired characters\n\nDuring the cleaning process, we transformed string columns into numeric columns and cleaned the character columns in the TITLE_PRINCIPALS table. Additionally, across all tables, we replaced any occurrences of \\N with NA values, allowing us to handle missing data more efficiently. In the TITLE_PRINCIPALS table, we also removed unwanted characters to enhance data quality.\nBy cleaning and preparing the data, we now have a clearer view of the relationships between tables and the variables at our disposal."
  },
  {
    "objectID": "mp02.html#data-analysis",
    "href": "mp02.html#data-analysis",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Data Analysis",
    "text": "Data Analysis\nWith the data now properly cleaned and filtered, we can move forward with our analysis. To begin, we will address the following key questions:\n\nHow many movies are in our dataset?\nHow many TV series?\nHow many TV episodes?\n\nAnswering these questions will help us gain an overview of the data set and set the foundation for deeper insights.\n\n# Install and load the gt package\nif(!require(gt)) {\n  install.packages(\"gt\")\n}\nlibrary(gt)\n\n# Get the count of movies, TV series, and TV episodes\nmovie &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"movie\") |&gt;\n  nrow()\n\ntvSeries &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"tvSeries\") |&gt;\n  nrow()\n\ntvEpisode &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"tvEpisode\") |&gt;\n  nrow()\n\n# Create a summary table and apply gt\nsummary_table &lt;- tibble(\n  Title_Type = c(\"Movies\", \"TV Series\", \"TV Episode\"),\n  Count = c(movie, tvSeries, tvEpisode)\n) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Number of Movies, TV Series, and TV Episodes\"\n  )\nsummary_table\n\n\n\n\n\n\n\nNumber of Movies, TV Series, and TV Episodes\n\n\nTitle_Type\nCount\n\n\n\n\nMovies\n100562\n\n\nTV Series\n20186\n\n\nTV Episode\n103647\n\n\n\n\n\n\n\nWe observed a notable trend in our dataset: the number of movies and TV episodes is quite similar, while TV series have the lowest count among the three categories. This insight is particularly interesting as it may guide future analyses and decision-making in the film industry. Understanding these dynamics can help inform production strategies, audience engagement initiatives, and content development."
  },
  {
    "objectID": "mp02.html#data-exploration-oldest-living-person",
    "href": "mp02.html#data-exploration-oldest-living-person",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Data Exploration: Oldest Living Person",
    "text": "Data Exploration: Oldest Living Person\nTo identify the oldest living person in our dataset, we can focus on the NAME_BASICS table, filtering by individuals who are still alive and sorting by birth year. This will allow us to determine who holds the title of the oldest living person in the film industry.\n\nlibrary(dplyr)\nlibrary(DT)  # Use DT for pagination\n\noldest_person &lt;- NAME_BASICS |&gt;         \n  filter(is.na(deathYear)) |&gt;            # Keep only people with NA in deathYear (still alive)\n  mutate(age = 2024 - birthYear) |&gt;      # Calculate their age in 2024\n  filter(age &gt; 110) |&gt; \n  filter(age &lt; 116)  # Filter for ages below 117 year  # Find the person with the maximum age\n\nsummary_table &lt;- oldest_person |&gt; \n  select(Name = primaryName, \"Year of Birth\" = birthYear, Age = age)\n\ndatatable(summary_table, \n          options = list(pageLength = 5),   # Limit to 5 rows per page\n          caption = 'Oldest Person Alive in this Data Set')\n\n\n\n\n\nInitially, we compiled a list of individuals who were 117 years old based on the dataset. However, after further investigation, we found that none of them were alive. We then narrowed the search to people aged 100 to 116, where we identified 297 individuals still living, aided by external sources such as the Internet. Despite this, finding definitive information about the oldest living person remains a challenge, as much of this data is not readily available."
  },
  {
    "objectID": "mp02.html#identifying-a-perfectly-rated-tv-episode",
    "href": "mp02.html#identifying-a-perfectly-rated-tv-episode",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Identifying a Perfectly Rated TV Episode",
    "text": "Identifying a Perfectly Rated TV Episode\nNext, we aimed to identify a TV episode with a perfect 10/10 IMDb rating and at least 200,000 ratings. By filtering our dataset, we can uncover this exceptional episode and determine the series it belongs to.\n\ngood_ranking &lt;- TITLE_BASICS |&gt;\n  inner_join(TITLE_RATINGS, by = \"tconst\") |&gt;  # Join to bring in titleType from TITLE_BASICS\n  filter(titleType == \"tvEpisode\") |&gt;        # Now you can filter for TV episodes\n  filter(averageRating == 10 & numVotes &gt; 200000) |&gt;  # Filter for 10/10 rating and &gt;200k votes\n  arrange(desc(numVotes))  # Sort in descending order by number of votes\n\n# Create the summary table\nsummary_table &lt;- good_ranking |&gt;\n  select(Name = primaryTitle, rating = averageRating, \"Number of Votes\" = numVotes ) |&gt; \n  gt() |&gt;                       \n  tab_header(\n    title = \"TV Episode with a Rating of 10/10 and More Than 200,000 Votes\"\n  )\n\nsummary_table\n\n\n\n\n\n\n\nTV Episode with a Rating of 10/10 and More Than 200,000 Votes\n\n\nName\nrating\nNumber of Votes\n\n\n\n\nOzymandias\n10\n230332\n\n\n\n\n\n\n\nOur analysis revealed that the Breaking Bad episode “Ozymandias” has achieved a perfect 10/10 rating with over 229,000 votes on IMDb. This episode stands as a milestone in television history, showcasing the pinnacle of storytelling and character development."
  },
  {
    "objectID": "mp02.html#exploring-mark-hamills-most-known-projects",
    "href": "mp02.html#exploring-mark-hamills-most-known-projects",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Exploring Mark Hamill’s Most Known Projects",
    "text": "Exploring Mark Hamill’s Most Known Projects\nWe will now explore the four projects that actor Mark Hamill is most known for on IMDb. Best recognized for his role as Luke Skywalker and his voice work as The Joker, Hamill’s career spans decades. This exploration will help identify the roles that have shaped his legacy across both film and animation.\n\nmark_hamill &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Mark Hamill\") |&gt;\n  select(knownForTitles)\n\ntconsts &lt;- strsplit(mark_hamill$knownForTitles, \",\")[[1]]\n\nmark_hamill_title &lt;- TITLE_BASICS |&gt;\n   filter(tconst %in% tconsts) |&gt;\n   select(\"Title Name\" = primaryTitle, Type = titleType) |&gt;\n   gt() |&gt;\n    tab_header(\n    title = \"The four Projects Mark Hamill Is Best Known For\")\n\n\nmark_hamill_title\n\n\n\n\n\n\n\nThe four Projects Mark Hamill Is Best Known For\n\n\nTitle Name\nType\n\n\n\n\nStar Wars: Episode IV - A New Hope\nmovie\n\n\nStar Wars: Episode V - The Empire Strikes Back\nmovie\n\n\nStar Wars: Episode VI - Return of the Jedi\nmovie\n\n\nStar Wars: Episode VIII - The Last Jedi\nmovie\n\n\n\n\n\n\n\nFrom our analysis, we can see that Mark Hamill’s top four “Most Known For” projects are all from the Star Wars franchise, solidifying his iconic role as Luke Skywalker. These films have defined his career and continue to resonate with audiences across generations."
  },
  {
    "objectID": "mp02.html#next-exploration-highest-rated-tv-series",
    "href": "mp02.html#next-exploration-highest-rated-tv-series",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Next Exploration: Highest Rated TV Series",
    "text": "Next Exploration: Highest Rated TV Series\nBuilding on our analysis, we now turn our attention to identifying the TV series with more than 12 episodes that has the highest average rating. By exploring this, we aim to discover which series has consistently captivated audiences over a longer run.\n\nseries &lt;- TITLE_BASICS |&gt;\n  inner_join(TITLE_EPISODES, by = \"tconst\") |&gt;\n  inner_join(TITLE_RATINGS,by = \"tconst\")\n\nbest_series &lt;- series |&gt;\n  filter(episodeNumber &gt; 12) |&gt;\n  distinct() |&gt;\n  filter(averageRating == max(averageRating, na.rm = TRUE)) |&gt;\n  select(\"Tile Name\" = primaryTitle, \"Average Ratings\" = averageRating) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"The TV Series with the Highest Average Rating and More Than 12 Episodes\"\n  )\n  \nbest_series\n\n\n\n\n\n\n\nThe TV Series with the Highest Average Rating and More Than 12 Episodes\n\n\nTile Name\nAverage Ratings\n\n\n\n\nSeries finale\n10\n\n\nI challenge the Ender Dragon in Minecraft (Ending)\n10\n\n\n37. Bolum\n10\n\n\n38. Bolum\n10\n\n\n39. Bolum\n10\n\n\n40. Bolum\n10\n\n\nGoodbye.\n10\n\n\nIce Hockey: Courtney\n10\n\n\nOzymandias\n10\n\n\n\n\n\n\n\nAs we can see in the results, there are several series with more than 12 episodes that hold a perfect rating of 10. However, there seems to be an issue with the data. The series “Bolum” is listed multiple times, which likely represents individual episodes of the same show, rather than different series. This duplication, indicated by episode numbers (e.g., 37. Bolum, 38. Bolum), points to a mistake in how the data has been recorded."
  },
  {
    "objectID": "mp02.html#investigating-jump-the-shark-in-happy-days",
    "href": "mp02.html#investigating-jump-the-shark-in-happy-days",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Investigating “Jump the Shark” in Happy Days",
    "text": "Investigating “Jump the Shark” in Happy Days\nThe TV series Happy Days (1974-1984) gave us the common idiom “jump the shark.” The phrase refers to a moment in a controversial fifth season episode (aired in 1977) where a lead character literally jumped over a shark on water skis. Over time, this phrase has come to mean the point when a once-great show becomes ridiculous and begins to decline in quality.\nTo investigate this, we now ask: Is it true that episodes from the later seasons of Happy Days have lower average ratings than the early seasons? Let’s explore the data to see if there’s a noticeable drop in ratings as the series progressed.\n\ndata &lt;- TITLE_BASICS |&gt;\n  inner_join(TITLE_EPISODES, join_by(tconst == parentTconst)) |&gt;  # Join episodes with series\n  inner_join(TITLE_RATINGS, by = \"tconst\")  # Join ratings dat\n\nhappy_days &lt;- data |&gt;\n    filter(primaryTitle == \"Happy Days\") |&gt;\n    arrange(seasonNumber) |&gt;\n    select(primaryTitle, \"Season Number\" = seasonNumber, \"Average Rating\" = averageRating, year = endYear) |&gt;\n    distinct() |&gt;\n\n  gt() |&gt;\n  tab_header(\n    title = \"Happy Days Seasons & Ratings per season\"\n  )\n\nhappy_days\n\n\n\n\n\n\n\nHappy Days Seasons & Ratings per season\n\n\nprimaryTitle\nSeason Number\nAverage Rating\nyear\n\n\n\n\nHappy Days\n1\n7.4\n1984\n\n\nHappy Days\n2\n7.4\n1984\n\n\nHappy Days\n3\n7.4\n1984\n\n\nHappy Days\n4\n7.4\n1984\n\n\nHappy Days\n5\n7.4\n1984\n\n\nHappy Days\n6\n7.4\n1984\n\n\nHappy Days\n7\n7.4\n1984\n\n\nHappy Days\n8\n7.4\n1984\n\n\nHappy Days\n9\n7.4\n1984\n\n\nHappy Days\n10\n7.4\n1984\n\n\nHappy Days\n11\n7.4\n1984\n\n\n\n\n\n\n\nThe assumption that the later seasons of Happy Days had lower average ratings is not true. As we can observe, the average rating remained constant at 7.4 across all seasons. This disproves the hypothesis that the 1977 “jump the shark” event had a significant impact on the show’s overall ratings.\nDespite the controversy surrounding that episode, the data shows no evidence of a decline in audience ratings afterward."
  },
  {
    "objectID": "mp02.html#creating-criteria-for-how-successful-a-movie-is",
    "href": "mp02.html#creating-criteria-for-how-successful-a-movie-is",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Creating Criteria for How Successful a Movie Is:",
    "text": "Creating Criteria for How Successful a Movie Is:\nAs we aim to propose a new movie, it is important to understand how successful it might be. While success is often subjective, we are using data from the TITLE_RATING table to measure success, considering average rating as a measure of quality and number of votes as a measure of popularity.\nTraditionally, a movie’s success is also evaluated by the revenue it generates. Therefore, we have selected seven movies with the highest revenue and five movies that lost money, with data sourced from Box Office Mojo."
  },
  {
    "objectID": "mp02.html#movie-revenue-table",
    "href": "mp02.html#movie-revenue-table",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Movie Revenue Table",
    "text": "Movie Revenue Table\n\n\n\n\n\n\n\nSuccessful Movies and Revenue\nUnsuccessful Movies, Money Lost\n\n\n\n\nAvatar ($2.92 billion)\nJohn Carter (-$0.20 billion)\n\n\nAvatar: The Way of Water ($2.32 billion)\nThe Lone Ranger (-$0.19 billion)\n\n\nTitanic ($2.25 billion)\nMars Needs Moms (-$0.15 billion)\n\n\nStar Wars: Episode VII - The Force Awakens ($2.07 billion)\nKing Arthur: Legend of the Sword (-$0.15 billion)\n\n\nSpider-Man: No Way Home ($1.92 billion)\nSinbad: Legend of the Seven Seas (-$0.125 billion)\n\n\nJurassic World ($1.67 billion)\nCutthroat Island (-$0.1 billion)\n\n\nTop Gun: Maverick ($1.49 billion)\nR.I.P.D. (-$0.1 billion)\n\n\n\n\n#first we are filtering the movies we need, selecting the usuful tables, such as originalTitle, averageRating, numVotes\nmovies &lt;- TITLE_BASICS |&gt;\n  inner_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n  filter(titleType == \"movie\")\n  \nfilter_movies &lt;- movies |&gt;\n  filter((str_detect(str_to_lower(primaryTitle), \"avatar\") & startYear == 2009) |\n          (str_detect(str_to_lower(primaryTitle), \"avatar: the way of water\") & startYear == 2022) |\n          (primaryTitle == \"Titanic\" & startYear == 1997) |\n          (str_detect(str_to_lower(primaryTitle), \"star wars\") & startYear == 2015) |\n          (str_detect(str_to_lower(primaryTitle), \"spider-man: no way home\") & startYear == 2021) |\n          (str_detect(str_to_lower(primaryTitle), \"jurassic world\") & startYear == 2015) |\n          (str_detect(str_to_lower(primaryTitle), \"top gun: maverick\") & startYear == 2022) |\n          (str_detect(str_to_lower(primaryTitle), \"john carter\")) |\n          (str_detect(str_to_lower(primaryTitle), \"the lone ranger\") & startYear == 2013) |\n          (str_detect(str_to_lower(primaryTitle), \"mars needs moms\")) |\n          (str_detect(str_to_lower(primaryTitle), \"king arthur\") & startYear == 2017) |\n          (primaryTitle ==  \"Cutthroat Island\") |\n          (primaryTitle ==  \"R.I.P.D.\") |\n          (str_detect(str_to_lower(primaryTitle), \"legend of the seven seas\")))\nfilter_movies &lt;- filter_movies |&gt;\n  arrange(primaryTitle)\n\n# Assign revenue to the filtered dataframe\nfilter_movies$Revenue &lt;- c(\n  2.92,   # Avatar (successful)\n  2.32,   # Avatar: The Way of Water (successful)\n  -0.10,  # Cutthroat Island (loss)\n  -0.20,  # John Carter (loss)\n  1.67,   # Jurassic World (successful)\n  -0.15,  # King Arthur: Legend of the Sword (loss)\n  -0.15,  # Mars Needs Moms (loss)\n  -0.10,  # R.I.P.D. (loss)\n  -0.125, # Sinbad: Legend of the Seven Seas (loss)\n  1.92,   # Spider-Man: No Way Home (successful)\n  2.07,   # Star Wars: Episode VII - The Force Awakens (successful)\n  -0.19,  # The Lone Ranger (loss)\n  2.25,   # Titanic (successful)\n  1.49    # Top Gun: Maverick (successful)\n)\n\nfilter_movies |&gt;\n  arrange(desc(Revenue)) |&gt;  # Sort by revenue\n  select(\"Original Name\" = primaryTitle, \"Average Rating\" = averageRating, \"Votes\" = numVotes, \"Revenue (Billion)\" = Revenue) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"7 most Sucessful and 7 most Unsuccessful Movies based on revenue (on Billion)\"\n   )\n\n\n\n\n\n\n\n7 most Sucessful and 7 most Unsuccessful Movies based on revenue (on Billion)\n\n\nOriginal Name\nAverage Rating\nVotes\nRevenue (Billion)\n\n\n\n\nAvatar\n7.9\n1406045\n2.920\n\n\nAvatar: The Way of Water\n7.5\n513213\n2.320\n\n\nTitanic\n7.9\n1309446\n2.250\n\n\nStar Wars: Episode VII - The Force Awakens\n7.8\n985881\n2.070\n\n\nSpider-Man: No Way Home\n8.2\n911025\n1.920\n\n\nJurassic World\n6.9\n689617\n1.670\n\n\nTop Gun: Maverick\n8.2\n735945\n1.490\n\n\nCutthroat Island\n5.7\n31185\n-0.100\n\n\nR.I.P.D.\n5.6\n146113\n-0.100\n\n\nSinbad: Legend of the Seven Seas\n6.7\n60614\n-0.125\n\n\nKing Arthur: Legend of the Sword\n6.7\n236363\n-0.150\n\n\nMars Needs Moms\n5.4\n24310\n-0.150\n\n\nThe Lone Ranger\n6.4\n246519\n-0.190\n\n\nJohn Carter\n6.6\n289519\n-0.200\n\n\n\n\n\n\n\nAfter adding the revenue column, we will create a model to serve as a criterion for success. The model is based on the following assumptions:\n\nScale: Our success scale will range from 0 to 10, with 10 representing an extremely successful movie.\nRevenue Mapping: For the positive revenue values (successful movies), we will assign ratings between 9.5 and 10, indicating extreme success. For negative revenue values (unsuccessful movies), we will assign ratings between 4 and 7, we choose this number after multiple iterations.\n\nThis range was chosen because, as observed in our table, even if a movie receives a large number of votes, ratings below 7 tend to correlate with more negative feedback, the opposite of what we expect from a successful movie, regardless of popularity.\n\n# Step 1: Define the max and min values for positive and negative revenues\nmax_positive_revenue &lt;- max(filter_movies$Revenue[filter_movies$Revenue &gt; 0])\nmin_positive_revenue &lt;- min(filter_movies$Revenue[filter_movies$Revenue &gt; 0])\n\nmax_negative_revenue &lt;- max(filter_movies$Revenue[filter_movies$Revenue &lt; 0])  # closest to 0\nmin_negative_revenue &lt;- min(filter_movies$Revenue[filter_movies$Revenue &lt; 0])  # most negative\n\n# Step 2: Apply different formulas based on whether revenue is positive or negative\nfilter_movies$Success_Score &lt;- ifelse(\n  filter_movies$Revenue &gt; 0, \n  9.5 + 0.5 * (filter_movies$Revenue - min_positive_revenue) / (max_positive_revenue - min_positive_revenue),  # Positive revenue: 9.5 to 10 scale\n  4 +  3 * (filter_movies$Revenue - min_negative_revenue) / (max_negative_revenue - min_negative_revenue)  # Negative revenue: 6 to 7 scale\n)\n\nfilter_movies |&gt;\n  arrange(desc(Success_Score)) |&gt; # Sort by revenue\n  select(\"Original Name\" = primaryTitle, \"Average Rating\" = averageRating, \"Votes\" = numVotes, \"Success Score\" = Success_Score ) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"7 most Sucessful and 7 most Unsuccessful Movies based on Success Score (on Billion)\"\n   )\n\n\n\n\n\n\n\n7 most Sucessful and 7 most Unsuccessful Movies based on Success Score (on Billion)\n\n\nOriginal Name\nAverage Rating\nVotes\nSuccess Score\n\n\n\n\nAvatar\n7.9\n1406045\n10.000000\n\n\nAvatar: The Way of Water\n7.5\n513213\n9.790210\n\n\nTitanic\n7.9\n1309446\n9.765734\n\n\nStar Wars: Episode VII - The Force Awakens\n7.8\n985881\n9.702797\n\n\nSpider-Man: No Way Home\n8.2\n911025\n9.650350\n\n\nJurassic World\n6.9\n689617\n9.562937\n\n\nTop Gun: Maverick\n8.2\n735945\n9.500000\n\n\nCutthroat Island\n5.7\n31185\n7.000000\n\n\nR.I.P.D.\n5.6\n146113\n7.000000\n\n\nSinbad: Legend of the Seven Seas\n6.7\n60614\n6.250000\n\n\nKing Arthur: Legend of the Sword\n6.7\n236363\n5.500000\n\n\nMars Needs Moms\n5.4\n24310\n5.500000\n\n\nThe Lone Ranger\n6.4\n246519\n4.300000\n\n\nJohn Carter\n6.6\n289519\n4.000000\n\n\n\n\n\n\n\nBased on the assigned success scores, we will apply a linear regression model to determine the parameters B0, B1, and B2. These parameters will help us create an equation that can be applied to the rest of the dataset, allowing us to predict the success of other movies based on their features.\nThe linear regression model will use the following:\n\nB0: The intercept, representing the baseline success score.\nB1: The coefficient for the average rating, indicating the effect of quality on a movie’s success.\nB2: The coefficient for the number of ratings, reflecting how popularity impacts success.\n\n\n# Fit the linear regression model\nmodel &lt;- lm(Success_Score ~ averageRating + numVotes, data = filter_movies)\n\nsummary(model)\n\n\nCall:\nlm(formula = Success_Score ~ averageRating + numVotes, data = filter_movies)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7786 -0.5785  0.1934  1.0393  1.8759 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   2.140e+00  4.838e+00   0.442    0.667\naverageRating 5.830e-01  7.967e-01   0.732    0.480\nnumVotes      2.731e-06  1.643e-06   1.662    0.125\n\nResidual standard error: 1.498 on 11 degrees of freedom\nMultiple R-squared:  0.6284,    Adjusted R-squared:  0.5608 \nF-statistic: 9.299 on 2 and 11 DF,  p-value: 0.004322\n\n\nBased on the regression analysis, the final model to predict the success score is represented by the following equation:\n\\[\n\\text{Success Score} = 2.133 + 0.5842 \\times (\\text{Average Rating}) + 2.730 \\times 10^{-6} \\times (\\text{Votes})\n\\]\n\nThe R-squared value is 0.6283, meaning that about 62.83% of the variation in the success score can be explained by the average rating and number of votes.\nThe Adjusted R-squared is 0.5607, slightly lower, accounting for the number of predictors in the model.\nWith a p-value of 0.0043, the overall model is statistically significant.\n\nBased on our analysis, we have determined that the threshold for a movie to be considered successful is an average rating of 7.9 or higher."
  },
  {
    "objectID": "mp02.html#analyzing-a-prestige-actor-leonardo-dicaprio",
    "href": "mp02.html#analyzing-a-prestige-actor-leonardo-dicaprio",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Analyzing a Prestige Actor: Leonardo DiCaprio",
    "text": "Analyzing a Prestige Actor: Leonardo DiCaprio\nNext, we will examine the projects of a prestigious actor, Leonardo DiCaprio, and confirm that many of his films achieve high scores on our success metric. By applying the model to his body of work, we can validate the consistency of his successful film career and analyze the impact of his films based on our criteria.\n\n# Filter the data for Leonardo DiCaprio and Christopher Nolan\nleonardo_dicaprio &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Leonardo DiCaprio\") |&gt;\n  select(knownForTitles)\n\nnolan &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Christopher Nolan\") |&gt;\n  select(knownForTitles)\n\n# Split the knownForTitles column to get the tconsts\ntconsts_leonardo &lt;- strsplit(leonardo_dicaprio$knownForTitles, \",\")[[1]]\ntconsts_nolan &lt;- strsplit(nolan$knownForTitles, \",\")[[1]]\n\n# Filter the movies based on the tconsts for Leonardo DiCaprio and Christopher Nolan\nleonardo_movies &lt;- movies |&gt;\n  filter(tconst %in% tconsts_leonardo) |&gt;\n  mutate(Person = \"Leonardo DiCaprio\")\n\nnolan_movies &lt;- movies |&gt;\n  filter(tconst %in% tconsts_nolan) |&gt;\n  mutate(Person = \"Christopher Nolan\")\n\nleonardo_nolan_movies &lt;- bind_rows(leonardo_movies, nolan_movies)\n\nleonardo_nolan_movies |&gt;\n  select(\"Original Name\" = primaryTitle, \n         \"Average Rating\" = averageRating, \n         \"Votes\" = numVotes, \n         \"Status\" = SuccessStatus, \n         \"Associated With\" = Person) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Movies Associated with Leonardo DiCaprio and Christopher Nolan\"\n  )\n\n\n\n\n\n\n\nMovies Associated with Leonardo DiCaprio and Christopher Nolan\n\n\nOriginal Name\nAverage Rating\nVotes\nStatus\nAssociated With\n\n\n\n\nTitanic\n7.9\n1309446\nSuccessful\nLeonardo DiCaprio\n\n\nThe Departed\n8.5\n1449409\nSuccessful\nLeonardo DiCaprio\n\n\nThe Wolf of Wall Street\n8.2\n1627553\nSuccessful\nLeonardo DiCaprio\n\n\nInception\n8.8\n2605397\nSuccessful\nLeonardo DiCaprio\n\n\nThe Prestige\n8.5\n1472193\nSuccessful\nChristopher Nolan\n\n\nInterstellar\n8.7\n2179321\nSuccessful\nChristopher Nolan\n\n\nInception\n8.8\n2605397\nSuccessful\nChristopher Nolan\n\n\n\n\n\n\n\nAs we can see, both Leonardo DiCaprio and Christopher Nolan have a significant number of movies that score highly on our success metric. This confirms their prestige and success in the film industry, as their films consistently receive high ratings and are recognized as successful according to our model."
  },
  {
    "objectID": "mp02.html#final-validation-analyzing-bottom-successful-movies",
    "href": "mp02.html#final-validation-analyzing-bottom-successful-movies",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Final Validation: Analyzing Bottom Successful Movies",
    "text": "Final Validation: Analyzing Bottom Successful Movies\nFor our final validation, we will select the bottom 5 successful movies based on our success metric and manually check if they have won any major awards. This will help us assess whether the model accurately reflects not only commercial success but also critical recognition in the film industry.\n\n# Select top 5 movies based on success score\ntop_movies &lt;- movies |&gt;\n  arrange(desc(SuccessNewScore)) |&gt;\n  filter(SuccessStatus == \"Successful\") |&gt;\n  slice_tail(n = 5)\n\nsummarize &lt;-top_movies |&gt;\n  select(\"Original Name\" = primaryTitle, \n         \"Average Rating\" = averageRating, \n         \"Votes\" = numVotes, \n         \"Success Score\" = SuccessNewScore, \n         \"Status\" = SuccessStatus) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Bottom 5 succesful Movies Based on Success Score\"\n  )\nsummarize\n\n\n\n\n\n\n\nBottom 5 succesful Movies Based on Success Score\n\n\nOriginal Name\nAverage Rating\nVotes\nSuccess Score\nStatus\n\n\n\n\nAvatar: The Way of Water\n7.5\n513213\n7.915571\nSuccessful\n\n\nWreck-It Ralph\n7.7\n468223\n7.909589\nSuccessful\n\n\nThe Breakfast Club\n7.8\n446608\n7.909000\nSuccessful\n\n\nIndependence Day\n7.0\n617080\n7.907028\nSuccessful\n\n\nTo Kill a Mockingbird\n8.3\n337554\n7.903382\nSuccessful\n\n\n\n\n\n\n\nFrom the bottom 5 successful movies, we can see that all of them have received good recognition. Notably, 3 out of the 5 have won awards, with one being a nominee, which further confirms the accuracy of our model in predicting the success of a movie based on its average rating and number of votes."
  },
  {
    "objectID": "mp02.html#threshold-for-success",
    "href": "mp02.html#threshold-for-success",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Threshold for Success",
    "text": "Threshold for Success\nWe examined the “Success Score” metric, which combines both IMDb rating and number of votes. This score reflects both the quality and popularity of a movie. Based on our analysis, we determined that the threshold for a movie to be considered successful is an average rating of 7.9 or higher.\nThis threshold was derived from the analysis of the top 7 successful movies and the bottom 7 unsuccessful movies in terms of revenue. The score of 7.9 corresponds to the lowest rating of the most successful movies, specifically Avatar: The Way of Water.\nHowever, this threshold can be adjusted due to the limited number of successful movies. Adjusting the threshold to 7.5 would represent the average rating of successful films, expanding the criteria from 149 successful movies to 542 movies, thus providing a more inclusive measure of success.\n\nlibrary(tidyr)\n\n#changing the threshold to 7.5\nTITLE_RATINGS$SuccessStatus &lt;- ifelse(TITLE_RATINGS$SuccessNewScore &gt; 7.5, \"Successful\", \"Unsuccessful\")\n\nmovies &lt;- TITLE_BASICS |&gt;\n  inner_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n  filter(titleType == \"movie\")\n\nsuccessful_genre &lt;- movies |&gt;\n  filter(SuccessStatus == \"Successful\") |&gt;\n  mutate(decade = floor(startYear / 10) * 10) |&gt;\n  separate_rows(genres, sep = \",\") |&gt;  # Split genres into individual rows\n  group_by(decade, genres) |&gt;\n  summarise(count = n(), .groups = 'drop')  # Summarise with count\n\nsuccessful_genre_per_decade &lt;- successful_genre |&gt;\n  group_by(decade) |&gt;\n  filter(count == max(count)) \n\nggplot(successful_genre_per_decade, aes(x = decade, y = count, fill = genres)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +  \n  labs(title = \"Most Successful Genres by Decade\",\n       x = \"Decade\",\n       y = \"Number of Movies\",\n       fill = \"Genre\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_fill_brewer(palette = \"Set1\") \n\n\n\n\n\n\n\n\nFor each decade, we observe a variety of genres that have achieved success. However, Drama stands out as the most consistently successful genre across all decades. This indicates that drama movies have consistently resonated with audiences and garnered positive reception from critics. While other genres have experienced success in different decades, none have matched the enduring appeal of drama."
  },
  {
    "objectID": "mp02.html#studying-the-evolution-of-genres-over-time",
    "href": "mp02.html#studying-the-evolution-of-genres-over-time",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Studying the Evolution of Genres Over Time",
    "text": "Studying the Evolution of Genres Over Time\nWe aim to explore which genre consistently yields the most “successes” and identify genres that once produced reliable successes but have since fallen out of favor. To accomplish this, we will exclude drama from our analysis and examine the top 2 genres for each decade, allowing us to highlight emerging trends and shifts in audience preferences.\n\nsuccessful_genre_per_decade &lt;- successful_genre |&gt;\n  filter(genres != \"Drama\") |&gt;\n  group_by(decade) |&gt;\n  arrange(decade, desc(count)) |&gt;  # Arrange by decade and descending count\n  slice_head(n = 2)  # Select the top 2 genres per decade\n\nggplot(successful_genre_per_decade, aes(x = decade, y = count, fill = genres)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +  \n  labs(title = \"Most Successful Genres by Decade (Excluding Drama)\",\n       x = \"Decade\",\n       y = \"Number of Movies\",\n       fill = \"Genre\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_fill_brewer(palette = \"Set1\") \n\n\n\n\n\n\n\n\nAfter excluding the drama genre, we observe that Action and Adventure have been the most successful genres over time, with Action showing significant growth and Adventure demonstrating more consistency. Other genres, such as Mystery, Crime, and Comedy, have had success in the past but lack the consistency seen in Action and Adventure. Additionally, the growth of the Thriller genre in the last decade is noteworthy; it has emerged as the third most successful genre following Drama and Action."
  },
  {
    "objectID": "mp02.html#movies-production-vs.-success",
    "href": "mp02.html#movies-production-vs.-success",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Movies Production vs. Success",
    "text": "Movies Production vs. Success\nWe aim to analyze the relationship between the number of movies produced and the number of successful movies. Specifically, we want to determine if the number of successful movies has increased over time and whether the ratio of successful movies to total movies has changed. To achieve this, we will calculate the success ratio by decade and genre, focusing on the top 5 genres: Drama, Action, Adventure, Thriller, and Crime.\n\nlibrary(dplyr)\nlibrary(gt)\n\ntop_genres &lt;- c(\"Drama\", \"Action\", \"Adventure\", \"Thriller\", \"Crime\")\n\nratio_successs &lt;- movies |&gt;\n  mutate(decade = floor(startYear / 10) * 10) |&gt;\n  separate_rows(genres, sep = \",\") |&gt;  # Split genres into individual rows if it's a combined column\n  filter(genres %in% top_genres) |&gt;  # Focus only on top 5 genre\n  group_by(decade, genres) |&gt;\n  summarise(\n    total_movies = n(), \n    successful_movies = sum(SuccessStatus == \"Successful\"), \n    .groups = 'drop'\n  ) |&gt;\n  mutate(successful_ratio = round((successful_movies / total_movies)*100, 2))  # Round to 2 decimals\n\nggplot(ratio_successs, aes(x = decade, y = successful_ratio, color = genres)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Percentage of Successful Movies to Total Movies by Decade and Genre\",\n       x = \"Decade\",\n       y = \"Success Ratio\",\n       color = \"Genre\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\nWith this graph, we can observe the varying ratios of success, indicating the percentage of successful movies within each genre. Overall, there is a clear upward trend in the success of movies across all genres. However, Adventure and Action have shown the most significant growth since the 1970s, while Drama has maintained a consistent performance over time. This highlights the evolving landscape of popular genres and their ability to resonate with audiences across different decades.\n\nggplot(ratio_successs, aes(x = decade, y = total_movies, fill = genres)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Total Number of Movies by Decade and Genre\",\n       x = \"Decade\",\n       y = \"Number of Movies\",\n       fill = \"Genre\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\nFrom these results, we can clearly see that the number of movies created in the Drama genre consistently exceeds that of any other genre by more than double, though the success ratio has increased over time. In contrast, the success ratios for Adventure and Action have shown growth since the 1970s, while the number of movies produced in these genres has remained steady.\nAdditionally, we observe that in the last decade, Thriller has increased both in the number of movies created and its success ratio, indicating a positive trend for this genre. Overall, there is a decline in movie productions, which can be attributed to the rise of streaming services that have transformed the way movies are produced and consumed. This trend is likely to continue as the industry adapts to new technologies and shifting consumer preferences."
  },
  {
    "objectID": "mp02.html#hollywood-project",
    "href": "mp02.html#hollywood-project",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Hollywood Project",
    "text": "Hollywood Project\nThe next step is to create our own movie. To do this, we will first select a genre, two actors, and one director that would be ideal for the film. Here are the criteria we will use:\n\nGenre: Adventure, combined with some Action, as it has proven consistent over time with a high success ratio.\nActors and Director: We will select successful individuals from this genre while incorporating some new actors who have found success in the last decade.\n\n\nmy_genres &lt;- c(\"Action\", \"Adventure\")\n\nmy_movie &lt;-movies |&gt;\n  inner_join(TITLE_PRINCIPALS, by = \"tconst\") |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\")\n\nget_crew &lt;- my_movie |&gt;\n  filter(startYear &gt;= 2008) |&gt;\n  separate_rows(genres, sep = \",\") |&gt;  \n  filter(genres %in% my_genres, SuccessStatus == \"Successful\")\n\ntop_actors &lt;- get_crew |&gt;\n  filter(category == \"actor\" | category == \"actress\") |&gt;\n  group_by(primaryName) |&gt;  \n  summarise(successful_movies = n()) |&gt;  # Count the number of successful movies per actor\n  arrange(desc(successful_movies)) |&gt;  # Sort by most successful movies\n  slice_head(n = 6)  # Select the top 5 actors\n\ntop_directors &lt;- get_crew |&gt;\n  filter(category == \"director\") |&gt;  \n  group_by(primaryName) |&gt;  \n  summarise(successful_movies = n()) |&gt;  \n  arrange(desc(successful_movies))|&gt; \n  slice_head(n = 7)  \n\ntop_actors |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 5 Actors in Successful Action/Adventure Movies After 2010\"\n  )\n\n\n\n\n\n\n\nTop 5 Actors in Successful Action/Adventure Movies After 2010\n\n\nprimaryName\nsuccessful_movies\n\n\n\n\nRobert Downey Jr.\n7\n\n\nChris Evans\n6\n\n\nIrrfan Khan\n5\n\n\nAndrew Garfield\n4\n\n\nChris Pine\n4\n\n\nClark Gregg\n4\n\n\n\n\n\n\ntop_directors |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Most Important Directors in Successful Action/Adventure Movies After 2010\"\n  )\n\n\n\n\n\n\n\nMost Important Directors in Successful Action/Adventure Movies After 2010\n\n\nprimaryName\nsuccessful_movies\n\n\n\n\nDavid Yates\n3\n\n\nZack Snyder\n3\n\n\nChris Sanders\n2\n\n\nChristopher Nolan\n2\n\n\nColin Trevorrow\n2\n\n\nDean DeBlois\n2\n\n\nGuy Ritchie\n2\n\n\n\n\n\n\n\nNow that we have identified the actors with the most success in the Adventure genre, we will select the top 2 actors and the top director based on their success ratios. To accomplish this, we will calculate the ratio of successful movies to total movies for each actor and director.\nFollowing this analysis, we will create a bar plot to visualize both the total number of movies and the number of successful movies for the selected actors and directors. This will help us clearly see who has performed best in this genre and support our decision-making process for the movie project.\n\n#parameters to select the actors and directors with our genre. \nselected_actors &lt;- c(\"Chris Evans\", \"Hugo Weaving\", \"Robert Downey Jr.\", \"Irrfan Khan\", \"Clark Gregg\", \"Henry Cavill\")\nselected_directors &lt;- c(\"David Yates\", \"Zack Snyder\", \"Chris Sanders\", \"Christopher Nolan\", \"Dean DeBlois\", \"Guy Ritchie\", \"J.J. Abrams\")\nmy_genres &lt;- c(\"Action\", \"Adventure\")\n\n\n# Step 2: Filter the data for selected actors and directors\nactor_movies &lt;- my_movie |&gt;\n  separate_rows(genres, sep = \",\") |&gt;  \n  filter(genres %in% my_genres & startYear &gt;= 2008 & primaryName %in% selected_actors)|&gt;\n  group_by(primaryName) |&gt;\n  summarise(total_movies = n())\n\ndirector_movies &lt;- my_movie |&gt;\n  separate_rows(genres, sep = \",\") |&gt;  \n  filter(genres %in% my_genres & startYear &gt;= 2008 & primaryName %in% selected_directors)|&gt;\n  group_by(primaryName) |&gt;\n  summarise(total_movies = n())\n\n# we would calculate the success ratio for each actor. \nactor_data &lt;- actor_movies |&gt;\n  left_join(top_actors, by = \"primaryName\") |&gt;\n  replace_na(list(successful_movies = 0)) |&gt;\n  mutate(success_ratio = successful_movies / total_movies)  # Calculate success ratio\n\n# Reshape data for easier plotting of bars,  this help us to create the plot.\nactor_data_long &lt;- actor_data |&gt;\n  pivot_longer(cols = c(\"total_movies\", \"successful_movies\"), \n               names_to = \"category\", \n               values_to = \"count\")\n\n# Create the bar plot with a line for the success ratio\nggplot() +\n  # Bar plot for total and successful movies\n  geom_bar(data = actor_data_long, aes(x = primaryName, y = count, fill = category), \n           stat = \"identity\", position = \"dodge\") +\n  \n  # Line plot for success ratio (on top of the original unreshaped data)\n  geom_line(data = actor_data, aes(x = primaryName, y = success_ratio * max(actor_data$total_movies), group = 1), \n            color = \"blue\", size = 1, linetype = \"dashed\") +  \n  \n  # Add points for success ratio\n  geom_point(data = actor_data, aes(x = primaryName, y = success_ratio * max(actor_data$total_movies)), \n             color = \"blue\", size = 3) +\n  \n  # Labels and titles\n  labs(title = \"Movies vs Successful Movies (with Success Ratio)\",\n       x = \"Actor\",\n       y = \"Count of Movies\",\n       fill = \"Category\") +\n  \n  # Secondary y-axis for the success ratio\n  scale_y_continuous(sec.axis = sec_axis(~./max(actor_data$total_movies), name = \"Success Ratio\")) +\n  \n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nAfter analyzing the data, we can see that Clark Gregg and Hugo Weaving have a record of success, each with a 100% success ratio in 4 movies after 2008. However, we want to incorporate Chris Evans into our movies because, despite his success ratio being 0.5, he has appeared in 12 movies, which is a good indicator of his popularity and appeal to audiences.\n\n# Prepare the director data (similar process as for actors)\ndirector_data &lt;- director_movies |&gt;\n  left_join(top_directors, by = \"primaryName\") |&gt;\n  replace_na(list(successful_movies = 0)) |&gt;\n  mutate(success_ratio = successful_movies / total_movies)  # Calculate success ratio\n\n# Reshape data for easier plotting of bars\ndirector_data_long &lt;- director_data |&gt;\n  pivot_longer(cols = c(\"total_movies\", \"successful_movies\"), \n               names_to = \"category\", \n               values_to = \"count\")\n\n# Create the bar plot with a line for the success ratio\nggplot() +\n  # Bar plot for total and successful movies\n  geom_bar(data = director_data_long, aes(x = primaryName, y = count, fill = category), \n           stat = \"identity\", position = \"dodge\") +\n  \n  # Line plot for success ratio (on top of the original unreshaped data)\n  geom_line(data = director_data, aes(x = primaryName, y = success_ratio * max(director_data$total_movies), group = 1), \n            color = \"blue\", size = 1, linetype = \"dashed\") +  \n  \n  # Add points for success ratio\n  geom_point(data = director_data, aes(x = primaryName, y = success_ratio * max(director_data$total_movies)), \n             color = \"blue\", size = 3) +\n  \n  # Labels and titles\n  labs(title = \"Movies vs Successful Movies (Directors, After 2010, with Success Ratio)\",\n       x = \"Director\",\n       y = \"Count of Movies\",\n       fill = \"Category\") +\n  \n  # Secondary y-axis for the success ratio\n  scale_y_continuous(sec.axis = sec_axis(~./max(director_data$total_movies), name = \"Success Ratio\")) +\n  \n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\nWarning: Use of `director_data$total_movies` is discouraged.\nℹ Use `total_movies` instead.\nUse of `director_data$total_movies` is discouraged.\nℹ Use `total_movies` instead.\n\n\n\n\n\n\n\n\n\nNow, we will select our director based on the success ratio. In this case, it is an easy pick: we have selected David Yates, who has 3 successful movies out of 5 in the genre we are looking for. Although other directors have more movies, the success ratio is the most important factor in this case."
  },
  {
    "objectID": "mp02.html#movie-creation",
    "href": "mp02.html#movie-creation",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Movie Creation",
    "text": "Movie Creation\nOur movies would be a remake movie, we would look movies that where succesful 25 years ago and we would create a remake of it. We would select 7 movies that have as genre action and adventure, in that way we would be sure that the movie would be successful.\n\nlibrary(knitr)\n\nmy_genres &lt;- c(\"Action\", \"Adventure\")\n\nmy_movie &lt;-movies |&gt;\n  inner_join(TITLE_PRINCIPALS, by = \"tconst\") |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\") |&gt;\n  inner_join(TITLE_CREW, by = \"tconst\")  # Join with crew data\n\n\n\nselection &lt;- my_movie |&gt;\n  separate_rows(genres, sep = \",\") |&gt;\n                \n  filter((startYear &lt; 2000) & (SuccessStatus == \"Successful\") & (genres %in% my_genres)) |&gt;\n  group_by(primaryTitle) |&gt;\n  filter(n_distinct(genres) == 2) |&gt;  # Keep only movies with both Action and Adventure genres\n  arrange(desc(SuccessNewScore)) |&gt;\n  distinct(primaryTitle, .keep_all = TRUE)  |&gt;  #Keep only the first occurrence of each movie, then ungroup\n  ungroup()   \n  \nselection |&gt;\n  select(\"Original Name\" = primaryTitle, \n         \"Average Rating\" = averageRating, \n         \"Votes\" = numVotes,\n         \"Year\" = startYear,\n         \"Success Score\" = SuccessNewScore) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Successful Movies with Action and Adventure Genres Before 2000\"\n  )\n\n\n\n\n\n\n\nSuccessful Movies with Action and Adventure Genres Before 2000\n\n\nOriginal Name\nAverage Rating\nVotes\nYear\nSuccess Score\n\n\n\n\nStar Wars: Episode IV - A New Hope\n8.6\n1475294\n1977\n11.184673\n\n\nStar Wars: Episode V - The Empire Strikes Back\n8.7\n1406428\n1980\n11.055088\n\n\nTerminator 2: Judgment Day\n8.6\n1199025\n1991\n10.430458\n\n\nStar Wars: Episode VI - Return of the Jedi\n8.3\n1140740\n1983\n10.096080\n\n\nRaiders of the Lost Ark\n8.4\n1052079\n1981\n9.912456\n\n\nJurassic Park\n8.2\n1090143\n1993\n9.899530\n\n\nThe Terminator\n8.1\n943192\n1984\n9.439934\n\n\nAliens\n8.4\n788138\n1986\n9.191897\n\n\nIndiana Jones and the Last Crusade\n8.2\n822785\n1989\n9.169643\n\n\nStar Wars: Episode I - The Phantom Menace\n6.5\n874093\n1999\n8.316574\n\n\nPrincess Mononoke\n8.3\n444856\n1997\n8.196317\n\n\nMen in Black\n7.3\n625083\n1997\n8.104137\n\n\nIndiana Jones and the Temple of Doom\n7.5\n542706\n1984\n7.996087\n\n\nThe Fifth Element\n7.6\n516253\n1997\n7.982291\n\n\nPredator\n7.8\n466197\n1987\n7.962478\n\n\nNorth by Northwest\n8.3\n351696\n1959\n7.941990\n\n\nIndependence Day\n7.0\n617080\n1996\n7.907028\n\n\nDie Hard with a Vengeance\n7.6\n413379\n1995\n7.701445\n\n\nMission: Impossible\n7.2\n478390\n1996\n7.645245\n\n\nBatman\n7.5\n412196\n1989\n7.639795\n\n\nThe Mummy\n7.1\n471372\n1999\n7.567666\n\n\nRamayana: The Legend of Prince Rama\n9.2\n15449\n1993\n7.549816\n\n\nThe Iron Giant\n8.1\n234915\n1999\n7.506338"
  },
  {
    "objectID": "mp02.html#movie-selection-north-by-northwest-1959",
    "href": "mp02.html#movie-selection-north-by-northwest-1959",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Movie Selection: North by Northwest (1959)",
    "text": "Movie Selection: North by Northwest (1959)\nBased on the table analysis, we have chosen North by Northwest (1959) for the following key reasons:\n\nThe movie has not been remade: This classic suspense thriller remains untouched, providing a unique opportunity for a fresh adaptation. Modern audiences might appreciate a new take on the original storyline.\nSimplified rights acquisition:\n\nMost of the original actors and director are no longer living, which may simplify the process of securing rights.\nThis could make the adaptation process faster and less complex from a legal standpoint.\n\n\nA remake of this iconic film can resonate well with contemporary audiences, who may find its themes of suspense, intrigue, and mistaken identity just as compelling today as they were in 1959. A new adaptation, with modern cinematic techniques and storytelling, has the potential to breathe new life into this timeless thriller and mistery.\n\n# Extract genres for \"North by Northwest\"\ngenres &lt;- TITLE_BASICS |&gt;\n  filter(primaryTitle == \"North by Northwest\") |&gt;\n  select(genres)\n\n# Select \"North by Northwest\" for the remake and display relevant info\nnorth &lt;- my_movie |&gt;\n  filter(primaryTitle == \"North by Northwest\") |&gt;\n  select(directors,\n         writers,\n         primaryName) |&gt;\n  distinct()  # Ensure no duplicate rows are included\n\n# Display the selected movie info in a nicely formatted table\nnorth |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Selected Movie for Remake: North by Northwest (1959)\"\n  )\n\n\n\n\n\n\n\nSelected Movie for Remake: North by Northwest (1959)\n\n\ndirectors\nwriters\nprimaryName\n\n\n\n\nnm0000033\nnm0499626,nm0222985\nCary Grant\n\n\nnm0000033\nnm0499626,nm0222985\nEva Marie Saint\n\n\nnm0000033\nnm0499626,nm0222985\nJames Mason\n\n\nnm0000033\nnm0499626,nm0222985\nJessie Royce Landis\n\n\nnm0000033\nnm0499626,nm0222985\nLeo G. Carroll\n\n\nnm0000033\nnm0499626,nm0222985\nJosephine Hutchinson\n\n\nnm0000033\nnm0499626,nm0222985\nPhilip Ober\n\n\nnm0000033\nnm0499626,nm0222985\nMartin Landau\n\n\nnm0000033\nnm0499626,nm0222985\nAdam Williams\n\n\nnm0000033\nnm0499626,nm0222985\nEdward Platt\n\n\nnm0000033\nnm0499626,nm0222985\nAlfred Hitchcock\n\n\nnm0000033\nnm0499626,nm0222985\nErnest Lehman\n\n\nnm0000033\nnm0499626,nm0222985\nBernard Herrmann\n\n\nnm0000033\nnm0499626,nm0222985\nRobert Burks\n\n\nnm0000033\nnm0499626,nm0222985\nGeorge Tomasini\n\n\nnm0000033\nnm0499626,nm0222985\nRobert F. Boyle\n\n\n\n\n\n\n\nBased on the Crew group for this movie, the only one alive is Eva Marie Saint which today has 100 years, which would be a good idea to include her in the movie, to give a sense of continuity with the original movie."
  },
  {
    "objectID": "mp02.html#proposal-for-remake-of-north-by-northwest",
    "href": "mp02.html#proposal-for-remake-of-north-by-northwest",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Proposal for Remake of North by Northwest",
    "text": "Proposal for Remake of North by Northwest\nWe propose a modern remake of Alfred Hitchcock’s 1959 classic North by Northwest, a thrilling action-adventure about mistaken identity and espionage. This project capitalizes on the growing popularity of the action, adventure, and thriller genres, all of which have seen strong growth over the past decade.\nChris Evans (success ratio 0.5, 12 films) will play the lead, Roger Thornhill. His global popularity from blockbuster franchises like Captain America guarantees mass appeal, particularly among younger audiences. Hugo Weaving (100% success ratio, 4 films) will play the sophisticated villain, bringing depth and menace, as he has done in The Matrix and The Lord of the Rings. Clark Gregg (100% success ratio, 4 films) will take on a pivotal supporting role, adding gravitas as a shadowy government figure.\nDavid Yates, with a 60% success ratio, is our chosen director. Known for delivering hits like Harry Potter and Fantastic Beasts, his ability to handle large-scale action-adventure films with complex narratives ensures this project will be a success.\nThe consistent rise of the action-adventure genre, paired with the recent resurgence of thrillers, positions North by Northwest perfectly for a remake. Our analysis shows that these genres have seen significant growth, and tapping into this trend with a strong cast and proven director will maximize our movie’s potential.\nWe project the film to gross $600 to $800 million globally, appealing to both classic film fans and modern action audiences."
  },
  {
    "objectID": "mp02.html#conclusion",
    "href": "mp02.html#conclusion",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Conclusion",
    "text": "Conclusion\nThis project has provided valuable insights into the film industry, focusing on the success of movies based on IMDb ratings, number of votes, and revenue. We have developed a model to predict the success of a movie based on these factors, and we have validated the model through various spot checks and analyses. We have also explored the evolution of genres over time, identifying trends and patterns that can inform future movie production. also, the creation of our own movie has been a great experience, and we are confident that our proposed remake of North by Northwest has the potential to be a success ."
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "",
    "text": "Date: November 12th, 2024\nAuthor: Juan Camilo Martinez\n\n\n\nIn the United States, the President isn’t chosen directly by popular vote. Instead, the winner is determined through the Electoral College, a system in which each state’s influence is tied to its congressional representation. With the 2024 Presidential election drawing near, it’s timely to ask: Would a different allocation method for electoral votes result in election outcomes that better represent the national popular vote?\nThis project dives into this question, analyzing historical congressional and presidential election data from MIT’s Election Data Science Lab and UCLA Congressional Boundary Files. By exploring electoral vote distributions under different allocation rules, we aim to see how alternate methods might change election results.\n\n\n\n\n\n\nThe U.S. Constitution sets the foundation of the Electoral College, granting each state a number of electors equal to its congressional delegation: the number of House representatives plus two senators. Most states follow a winner-take-all rule, where the state’s popular vote winner claims all electoral votes. However, states like Nebraska and Maine use a district-wide allocation method, awarding votes by individual district results, with two additional votes for the statewide popular winner.\nIn this analysis, we’ll test four allocation strategies to see their potential impact: 1. State-Wide Winner-Take-All 2. District-Wide Winner-Take-All with At-Large Votes 3. State-Wide Proportional 4. National Proportional\n\n\n\n\n\n\n\nWe start by loading the necessary R packages for data manipulation and visualization.\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(stringr)\nlibrary(statebins)\nlibrary(scales)\n**Data Sources and Import\nThe project uses two primary datasets:\nU.S. House Election Data (1976–2022): Details vote counts from congressional races across the 50 states. U.S. Presidential Election Data (1976–2020): Provides state-level vote counts for presidential elections.\nDATA_HOUSE &lt;- read_csv(\"1976-2022-house.csv\") |&gt; filter(!is.na(party))\nDATA_PRESIDENT &lt;- read_csv(\"1976-2020-president.csv\") |&gt;  filter(!is.na(candidate) & !is.na(party_detailed))\nAdditionally, we download congressional boundary files for spatial analysis, covering elections from 1976 to 2012 from UCLA and from 2013 to 2023 from the U.S. Census Bureau.\nus_shapefiles &lt;- function(start = 95, end = 112) {\n  BASE_URL &lt;- \"https://cdmaps.polisci.ucla.edu/shp/districts\"\n  \ncongress_shapefiles_census &lt;- function(start_year = 2013, end_year = 2023) {\n  BASE_URL &lt;- \"https://www2.census.gov/geo/tiger/TIGER\""
  },
  {
    "objectID": "mp03.html#introduction",
    "href": "mp03.html#introduction",
    "title": "# Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Introduction",
    "text": "Introduction\nIn the United States, the President isn’t chosen directly by popular vote. Instead, the winner is determined through the Electoral College, a system in which each state’s influence is tied to its congressional representation. With the 2024 Presidential election drawing near, it’s timely to ask: Would a different allocation method for electoral votes result in election outcomes that better represent the national popular vote?\nThis project dives into this question, analyzing historical congressional and presidential election data from MIT’s Election Data Science Lab and UCLA Congressional Boundary Files. By exploring electoral vote distributions under different allocation rules, we aim to see how alternate methods might change election results."
  },
  {
    "objectID": "mp03.html#background",
    "href": "mp03.html#background",
    "title": "# Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Background",
    "text": "Background\n\nHow the Electoral College Works\nThe U.S. Constitution sets the foundation of the Electoral College, granting each state a number of electors equal to its congressional delegation: the number of House representatives plus two senators. Most states follow a winner-take-all rule, where the state’s popular vote winner claims all electoral votes. However, states like Nebraska and Maine use a district-wide allocation method, awarding votes by individual district results, with two additional votes for the statewide popular winner.\nIn this analysis, we’ll test four allocation strategies to see their potential impact: 1. State-Wide Winner-Take-All 2. District-Wide Winner-Take-All with At-Large Votes 3. State-Wide Proportional 4. National Proportional"
  },
  {
    "objectID": "mp03.html#data-collection",
    "href": "mp03.html#data-collection",
    "title": "# Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Data Collection",
    "text": "Data Collection\n\nSetting Up the Libraries and Data\nWe start by loading the necessary R packages for data manipulation and visualization.\n\n\nShow the code\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(stringr)\nlibrary(statebins)\nlibrary(scales)\nlibrary(usmap)\nlibrary(gt)\nlibrary(DT)\nlibrary(gganimate)\nlibrary(maps)\nlibrary(gifski)"
  },
  {
    "objectID": "mp03.html#data-sources-and-import",
    "href": "mp03.html#data-sources-and-import",
    "title": "# Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Data Sources and Import",
    "text": "Data Sources and Import\nThe project uses two primary datasets:\nU.S. House Election Data (1976–2022): Details vote counts from congressional races across the 50 states. U.S. Presidential Election Data (1976–2020): Provides state-level vote counts for presidential elections.\n\n\nShow the code\nDATA_HOUSE &lt;- read_csv(\"1976-2022-house.csv\") |&gt; filter(!is.na(party))\nDATA_PRESIDENT &lt;- read_csv(\"1976-2020-president.csv\") |&gt;  filter(!is.na(candidate) & !is.na(party_detailed))\n\n\nAdditionally, we download congressional boundary files for spatial analysis, covering elections from 1976 to 2012 from UCLA and from 2013 to 2023 from the U.S. Census Bureau.\nFirst, we download the UCLA congressional boundary files.\n\n\nShow the code\ncongress_shapefiles_ucla &lt;- function(start = 95, end = 112) {\n  BASE_URL &lt;- \"https://cdmaps.polisci.ucla.edu/shp/districts\"\n  \n  # Create directory if it doesn't exist\n  target_dir &lt;- \"data/congress_shapefiles\"\n  if (!dir.exists(target_dir)) {\n    dir.create(target_dir, recursive = TRUE)\n  }\n  \n  for (congress in start:end) {\n    congress_str &lt;- sprintf(\"%03d\", congress)\n    file_url &lt;- paste0(BASE_URL, congress_str, \".zip\")\n    dest_file &lt;- file.path(target_dir, paste0(\"congress_\", congress_str, \"_shapefile.zip\"))\n    \n    # Avoid re-downloading\n    if (!file.exists(dest_file)) {\n      tryCatch({\n        download.file(file_url, destfile = dest_file, mode = \"wb\")\n        # Check if the file is downloaded successfully\n        if (file.size(dest_file) &gt; 0) {\n          message(\"Successfully downloaded shapefile for Congress \", congress_str)\n        } else {\n          file.remove(dest_file)\n          message(\"Download failed for Congress \", congress_str, \". File was empty and deleted.\")\n        }\n      }, error = function(e) {\n        message(\"Error downloading for Congress \", congress_str, \": \", e)\n      })\n    } else {\n      message(\"File for Congress \", congress_str, \" already exists. Skipping download.\")\n    }\n  }\n}\n\n#function\ncongress_shapefiles_ucla(93,112)\n\n\nNow, we download the U.S. Census Bureau congressional boundary files.\n\n\nShow the code\n# Define function to download, unzip, and read a shapefile\ndownload_and_read_shapefile &lt;- function(year, congress_num) {\n  # Set base URL and target directory\n  base_url &lt;- sprintf(\"https://www2.census.gov/geo/tiger/TIGER%d/CD/\", year)\n  target_dir &lt;- \"data/congress_shapefiles\"\n  \n  # Create directory if it doesn't exist\n  if (!dir.exists(target_dir)) dir.create(target_dir, recursive = TRUE)\n  \n  # Construct filename and paths\n  file_name &lt;- sprintf(\"tl_%d_us_cd%d\", year, congress_num)\n  zip_file &lt;- file.path(target_dir, paste0(file_name, \".zip\"))\n  unzip_dir &lt;- file.path(target_dir, file_name)\n  shapefile_path &lt;- file.path(unzip_dir, paste0(file_name, \".shp\"))\n  \n  # Download the file if it doesn't exist\n  if (!file.exists(zip_file)) {\n    file_url &lt;- paste0(base_url, file_name, \".zip\")\n    tryCatch({\n      download.file(file_url, destfile = zip_file, mode = \"wb\")\n      if (file.size(zip_file) &gt; 0) {\n        message(\"Downloaded: \", file_name, \" for year \", year)\n      } else {\n        file.remove(zip_file)\n        message(\"Download failed for \", file_name, \". Empty file removed.\")\n      }\n    }, error = function(e) {\n      message(\"Error downloading \", file_name, \": \", e$message)\n    })\n  }\n  \n  # Unzip and read the shapefile if it hasn’t been unzipped already\n  if (file.exists(zip_file) && !file.exists(shapefile_path)) {\n    unzip(zipfile = zip_file, exdir = unzip_dir)\n  }\n  \n  # Load the shapefile if it exists\n  if (file.exists(shapefile_path)) {\n    return(read_sf(shapefile_path))\n  } else {\n    message(\"Shapefile not found for \", file_name, \" in year \", year)\n    return(NULL)\n  }\n}\n# Iterate over years and download shapefiles based on Congress sessions\nbase_year &lt;- 2022\nfor (i in 0:10) {\n  year &lt;- base_year - i\n  \n  # Determine Congress number based on the year\n  congress &lt;- if (year &gt;= 2018) 116\n              else if (year &gt;= 2016) 115\n              else if (year &gt;= 2014) 114\n              else if (year == 2013) 113\n              else if (year == 2012) 112\n              else NA\n  \n  if (!is.na(congress)) {\n    district_name &lt;- sprintf(\"tl_%d_us_cd%d\", year, congress)\n    \n    # Download and read shapefile\n    district_data &lt;- download_and_read_shapefile(year, congress)\n    \n    # Assign the data to a unique variable in the global environment\n    if (!is.null(district_data)) {\n      assign(district_name, district_data, envir = .GlobalEnv)\n    }\n  } else {\n    message(\"Congress data not available for year \", year)\n  }\n}\n\n\n\nInitial Exploration of Vote Count Data\nAnalyze Seat Gains and Losses in the U.S. House (1976-2022) This code calculates which states gained or lost the most seats in the House of Representatives between 1976 and 2022.\n\n\nShow the code\n# Filter House data for relevant years and calculate seat counts\nhouse_seats_over_time &lt;- DATA_HOUSE |&gt;\n  filter(year %in% c(1976, 2022)) |&gt;\n  distinct(year, state, district) |&gt;\n  group_by(year, state) |&gt;\n  summarise(total_seats = n(), .groups = \"drop\")\n\n# Calculate seat changes from 1976 to 2022\nseat_changes &lt;- house_seats_over_time |&gt;\n  pivot_wider(names_from = year, values_from = total_seats, names_prefix = \"year_\") |&gt;\n  mutate(seat_change = year_2022 - year_1976) |&gt;\n  arrange(desc(seat_change))\n\n# Identify top 5 states with highest seat gains and losses\ntop_gained_states &lt;- seat_changes |&gt; slice_max(seat_change, n = 5)\ntop_lost_states &lt;- seat_changes |&gt; slice_min(seat_change, n = 5)\n\n# Plot top gained seats\nplot_seat_gains &lt;- ggplot(top_gained_states, aes(x = reorder(state, seat_change), y = seat_change, fill = seat_change)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Top 5 States with Seat Gains (1976-2022)\",\n       x = \"State\",\n       y = \"Number of Seats Gained\") +\n  scale_fill_gradient(low = \"lightblue\", high = \"darkblue\") +\n  theme_minimal()\n\n# Plot top lost seats\nplot_seat_losses &lt;- ggplot(top_lost_states, aes(x = reorder(state, seat_change), y = seat_change, fill = -seat_change)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Top 5 States with Seat Losses (1976-2022)\",\n       x = \"State\",\n       y = \"Number of Seats Lost\") +\n  scale_fill_gradient(low = \"lightcoral\", high = \"darkred\") +\n  theme_minimal()\n\n# Display both plots\nlist(plot_seat_gains, plot_seat_losses)\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\nAs we can see in this graph, the states that gained the most seats are Texas, Florida, California, Arizona, and Georgia. On the other hand, the states that lost the most seats are New York, Ohio, Pennsylvania, Illinois, Ohio, and Michigan. The following table explains the population growth between 1976 to 2022 shifts are the main reason for these changes.\n\n\n\nLocation\nGrowth Percentage\n\n\n\n\nArizona\n192.0%\n\n\nFlorida\n161.9%\n\n\nTexas\n145.9%\n\n\nGeorgia\n125.0%\n\n\nCalifornia\n77.3%\n\n\nUSA\n52.8%\n\n\nIllinois\n12.5%\n\n\nPennsylvania\n10.2%\n\n\nNew York\n10.1%\n\n\nOhio\n9.3%\n\n\nMichigan\n8.1%\n\n\n\nWe see clearly that the Usa is in the middle of the list, and the states that lost the most seats are in the bottom of the list.\n\n\nShow the code\n# Filter for New York State and relevant House races\nny_house_data &lt;- DATA_HOUSE |&gt;\n  filter(state == \"NEW YORK\", office == \"US HOUSE\")\n\n# Calculate total votes (with fusion) and major party votes (without fusion)\nfusion_analysis &lt;- ny_house_data |&gt;\n  mutate(is_major_party = party %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  group_by(year, district, candidate) |&gt;\n  summarise(\n    total_votes_all_lines = sum(candidatevotes, na.rm = TRUE),\n    major_party_votes = sum(candidatevotes[is_major_party], na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n# Determine winners by both fusion and non-fusion scenarios\nwinners_fusion &lt;- fusion_analysis |&gt;\n  group_by(year, district) |&gt;\n  filter(total_votes_all_lines == max(total_votes_all_lines)) |&gt;\n  select(year, district, candidate, total_votes_all_lines) |&gt;\n  rename(fusion_winner = candidate, fusion_votes = total_votes_all_lines)|&gt;\n  ungroup()\n\nwinners_nonfusion &lt;- fusion_analysis |&gt;\n  group_by(year, district) |&gt;\n  filter(major_party_votes == max(major_party_votes)) |&gt;\n  select(year, district, candidate, major_party_votes) |&gt;\n  rename(nonfusion_winner = candidate, nonfusion_votes = major_party_votes) |&gt;\n  ungroup()\n\n# Find elections where fusion changed the winner\nelection_outcomes &lt;- winners_fusion |&gt;\n  inner_join(winners_nonfusion, by = c(\"year\", \"district\")) |&gt;\n  filter(fusion_winner != nonfusion_winner) |&gt;\n  arrange(desc(year))\n  \n\ndatatable(\n  election_outcomes,\n  options = list(\n    pageLength = 10,         # Rows per page\n    autoWidth = TRUE,        # Adjust columns automatically\n    dom = 'tip',             # Only show table, info, and pagination\n    lengthMenu = c(5, 10, 15, 20)  # Options for rows per page\n  ),\n  rownames = FALSE           # Hide row names\n)\n\n\n\n\n\n\nAs we can see, fusion voting can be a deciding factor in elections. The table demonstrates that fusion voting increases the likelihood of being elected by enabling candidates to gather votes from multiple party lines. This consolidates support in a way that would not be possible without fusion, giving fusion candidates a strategic advantage.\n\n\nAnalyzing Presidential vs. Congressional Vote Patterns Across Parties\nThis analysis explores whether presidential candidates tend to run ahead of or behind their congressional counterparts in the same state. Specifically, we’re investigating if Democratic and Republican presidential candidates receive more votes in a given state than all congressional candidates from their party in that same state.\nThe following code will compare these vote counts, identifying instances where presidential candidates either outperformed (“ahead”) or underperformed (“behind”) their co-partisans. This will also help us understand if this trend varies over time, across states, or between parties, providing insights into the relative popularity of presidential candidates within each party.\n\n\nShow the code\n# Aggregate Democratic and Republican presidential votes by state and year\npresidential_votes &lt;- DATA_PRESIDENT |&gt;\n  filter(office == \"US PRESIDENT\", party_simplified %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  group_by(year, state, party_simplified) |&gt;\n  summarise(total_pres_votes = sum(candidatevotes, na.rm = TRUE), .groups = \"drop\")\n\n# Aggregate Democratic and Republican congressional votes by state and year\ncongressional_votes &lt;- DATA_HOUSE |&gt;\n  filter(office == \"US HOUSE\", party %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  group_by(year, state, party) |&gt;\n  summarise(total_congress_votes = sum(candidatevotes, na.rm = TRUE), .groups = \"drop\") |&gt;\n  rename(party_simplified = party)\n\n# Merge the presidential and congressional vote data\nvote_comparison &lt;- presidential_votes |&gt;\n  inner_join(congressional_votes, by = c(\"year\", \"state\", \"party_simplified\")) |&gt;\n  mutate(\n    vote_difference = total_pres_votes - total_congress_votes,\n    ran_ahead = ifelse(vote_difference &gt; 0, \"Ahead\", \"Behind\")\n  )\n\n# Additional Visualization: Calculate the average vote difference per year across all states\navg_vote_diff &lt;- vote_comparison |&gt;\n  group_by(year, party_simplified) |&gt;\n  summarise(avg_vote_difference = mean(vote_difference, na.rm = TRUE), .groups = \"drop\")\n\n# Line chart for average vote difference over time\nggplot(avg_vote_diff, aes(x = year, y = avg_vote_difference, color = party_simplified)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Average Vote Difference Between Presidential and Congressional Candidates\",\n    subtitle = \"Across All States, by Party (1976 - 2020)\",\n    x = \"Year\",\n    y = \"Average Vote Difference (Presidential - Congressional)\",\n    color = \"Party\"\n  ) +\n  scale_color_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\")) +\n\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nAs we can see in the image, the average vote difference between presidential and congressional candidates has fluctuated over the years. On this, we can see a major fluctuation in the Republican party, what indicate that the presidential candidate has a major influence on votes than the congressional candidates. For Democrats, the difference is not that big, but still, the presidential candidate has a major influence on votes than the congressional candidates.\n\n\nShow the code\n# Summary of total instances where presidential candidates ran ahead or behind, grouped by party\nparty_summary_stats &lt;- vote_comparison |&gt;\n  group_by(party_simplified) |&gt;\n  summarise(\n    total_ahead = sum(ran_ahead == \"Ahead\"),\n    total_behind = sum(ran_ahead == \"Behind\"),\n    Difference = total_ahead - total_behind,  # Calculate the variance between ahead and behind\n    .groups = \"drop\"\n  )\n\n# Display the summary table\nparty_summary_stats |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Summary of Presidential Candidates Running Ahead or Behind Congressional Candidates\",\n    subtitle = \"Total Instances by Party (1976-2020)\"\n  ) |&gt;\n  cols_label(\n    party_simplified = \"Party\",\n    total_ahead = \"Total States (Ahead)\",\n    total_behind = \"Total States (Behind)\",\n    Difference = \"Difference (Ahead - Behind)\"\n  )\n\n\n\n\n\n\n\n\nSummary of Presidential Candidates Running Ahead or Behind Congressional Candidates\n\n\nTotal Instances by Party (1976-2020)\n\n\nParty\nTotal States (Ahead)\nTotal States (Behind)\nDifference (Ahead - Behind)\n\n\n\n\nDEMOCRAT\n318\n274\n44\n\n\nREPUBLICAN\n390\n207\n183\n\n\n\n\n\n\n\nFinally, we can observe the difference between the number of states where the presidential candidate ran ahead or behind their congressional counterparts. The table shows that both Democratic and Republican presidential candidates have run ahead more than behind, however, Republicans have a higher difference between the two categories. This indicates that Republican presidential candidates tend to outperform their congressional counterparts more frequently than Democratic candidates.\n\n\nImporting and Plotting Shapefile Data\nThe shapefiles we downloaded are provided in zip archives containing several files, but we only need the .shp file from each archive. In this section, we’ll walk through extracting the .shp file, loading it into R, and creating a plot from the data. The main tool we’ll use is the sf package, specifically the read_sf() function, which allows us to read shapefiles directly into R. Below, I’ll demonstrate how this process works.\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(sf)\n\nif(!file.exists(\"nyc_borough_boundaries.zip\")){\n    download.file(\"https://data.cityofnewyork.us/api/geospatial/tqmj-j8zm?method=export&format=Shapefile\", \n              destfile=\"nyc_borough_boundaries.zip\")\n}\n\n##-\ntd &lt;- tempdir(); \nzip_contents &lt;- unzip(\"nyc_borough_boundaries.zip\", \n                      exdir = td)\n    \nfname_shp &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\nnyc_sf &lt;- read_sf(fname_shp)\n\n\nWith the following code, we can plot the shapefile data to visualize the boundaries of New York City boroughs.\n\n\nShow the code\nread_shp_from_zip &lt;- function(zip_file) {\n  files_in_zip &lt;- unzip(zip_file, list = TRUE)\n  \n  shp_file &lt;- files_in_zip$Name[grepl(\"\\\\.shp$\", files_in_zip$Name)]\n  \n  if (length(shp_file) == 1) {\n    unzip(zip_file, files = shp_file, exdir = tempdir())\n    \n    shp_path &lt;- file.path(tempdir(), shp_file)\n    \n    shape_data &lt;- read_sf(shp_path)\n    \n    return(shape_data)\n  } else {\n    message(\"No .shp file found in the provided zip archive.\")\n    return(NULL)\n  }\n}\n\n\nNow we would plot the shapefile data to visualize the boundaries of New York City boroughs.\n\n\nShow the code\nggplot(nyc_sf, \n       aes(geometry=geometry)) + \n    geom_sf()\n\n\n\n\n\n\n\n\n\nAlso we can plot the shapefile data with a fill color to visualize the boundaries of New York City boroughs.\n\n\nShow the code\nggplot(nyc_sf, \n       aes(geometry=geometry, \n           fill = shape_area)) + \n    geom_sf()\n\n\n\n\n\n\n\n\n\n\n\nElectoral College Results in 2000\nIn this section, we will create a choropleth map to visualize the results of the 2000 U.S. Presidential Election. The map will display the winning party in each state, highlighting the electoral college winner between George W. Bush and Al Gore.\n\n\nShow the code\n# Step 1: Prepare Election Data\nelection_2000 &lt;- DATA_PRESIDENT |&gt;\n  filter(year == 2000, office == \"US PRESIDENT\") |&gt;\n  group_by(state) |&gt;\n  slice_max(order_by = candidatevotes, n = 1) |&gt;\n  ungroup()\n\n# Step 2: Assign Winning Party\nelection_2000 &lt;- election_2000 |&gt;\n  mutate(\n    party_winner = ifelse(party_simplified == \"REPUBLICAN\", \"Republican\", \"Democrat\"),\n    state = tolower(state)\n  )\n\n# Step 3: Define Electoral College Votes (EC)\nec_votes &lt;- data.frame(\n  state_po = c(\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"DC\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\", \"IN\", \"IA\",\n               \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \"NM\",\n               \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\",\n               \"WV\", \"WI\", \"WY\"),\n  EC = c(9, 3, 8, 6, 54, 8, 8, 3, 3, 25, 13, 4, 4, 22, 12, 7, 6, 8, 9, 4, 10, 12, 18, 10, 7, 11, 3, 5, 4, 4, 15, 5,\n         33, 14, 3, 21, 8, 7, 23, 4, 8, 3, 11, 32, 5, 3, 13, 11, 5, 11, 3)\n)\n\n# Step 4: Merge EC into election data\nelection_2000 &lt;- election_2000 |&gt;\n  left_join(ec_votes, by = \"state_po\")\n\n\n# Step 5: Prepare Map Data\nus_map &lt;- map_data(\"state\") |&gt;\n  mutate(region = tolower(region))\n\n# Step 6: Merge election data with map data\nmap_data_with_results &lt;- us_map |&gt;\n  left_join(election_2000, by = c(\"region\" = \"state\"))\n\n# Step 7: Calculate State Centers for Labels\nstate_centers &lt;- map_data_with_results |&gt;\n  group_by(region) |&gt;\n  summarize(\n    long_center = mean(range(long, na.rm = TRUE)),\n    lat_center = mean(range(lat, na.rm = TRUE)),\n    state_po = first(state_po),\n    EC = first(EC)\n  ) |&gt;\n  ungroup()\n\n# Step 8: Define Offset Labels for Northeastern States\nnortheast_labels &lt;- data.frame(\n  state_po = c(\"NH\", \"VT\", \"MA\", \"RI\", \"CT\", \"NJ\", \"DE\", \"MD\", \"DC\"),\n  long_offset = c(5, 4, 7, 6, 5, 6, 5, 6, 4),\n  lat_offset = c(2, 1, -1, 0, -2, -1, -1, -1, -2)\n)\n\n# Join offset labels with state centers to add adjusted coordinates\nstate_centers &lt;- state_centers |&gt;\n  left_join(northeast_labels, by = \"state_po\") |&gt;\n  mutate(\n    long_center_adjusted = ifelse(!is.na(long_offset), long_center + long_offset, long_center),\n    lat_center_adjusted = ifelse(!is.na(lat_offset), lat_center + lat_offset, lat_center)\n  )\n\n# Prepare connector line data for northeastern states\nnortheast_connectors &lt;- state_centers |&gt;\n  filter(!is.na(long_offset) & !is.na(lat_offset))\n\n# Step 9: Plot the Map with Insets and Connectors\nggplot(map_data_with_results) +\n  # Mainland US\n  geom_polygon(aes(x = long, y = lat, group = group, fill = party_winner), color = \"white\") +\n  # Alaska inset\n  geom_polygon(data = subset(map_data_with_results, region == \"alaska\"),\n               aes(x = long - 35, y = lat + 10, group = group, fill = party_winner), color = \"white\") +\n  # Hawaii inset\n  geom_polygon(data = subset(map_data_with_results, region == \"hawaii\"),\n               aes(x = long + 50, y = lat - 5, group = group, fill = party_winner), color = \"white\") +\n  # Party color scale\n  scale_fill_manual(\n    values = c(\"Democrat\" = \"blue\", \"Republican\" = \"red\"),\n    name = \"2000 Election Winner\",\n    labels = c(\"Democrat\" = \"Gore\", \"Republican\" = \"Bush\"),\n    na.value = \"grey\"\n  ) +\n  # Labels for each state with offset adjustments for northeastern states\n  geom_text(\n    data = state_centers,\n    aes(x = long_center_adjusted, y = lat_center_adjusted, label = paste(state_po, EC)),\n    color = \"black\",\n    size = 3,\n    fontface = \"bold\"\n  ) +\n  # Connector lines for northeastern states\n  geom_segment(data = northeast_connectors, \n               aes(x = long_center, y = lat_center, xend = long_center_adjusted, yend = lat_center_adjusted), \n               color = \"black\", linetype = \"solid\") +\n  # Map labels and theme\n  labs(\n    title = \"2000 Presidential Election Results by State\",\n    subtitle = \"Bush vs. Gore\",\n    caption = \"Data Source: MIT Election Data Science Lab\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(hjust = 0.5, size = 16),\n    plot.subtitle = element_text(hjust = 0.5, size = 12)\n  )\n\n\n\n\n\n\n\n\n\nNow we can see a clear picture of how the 2000 U.S. Presidential Election played out across the country. The map shows the winning party in each state, highlighting the electoral college winner between George W. Bush and Al Gore. This visualization provides a snapshot of the election results and the distribution of electoral votes across the United States.\n\n\nAnimated Map of U.S. Presidential Election Results (1976-2020)\nAfter this we want to visualize the results of all the U.S. Presidential Elections from 1976 to 2020. The following code will create a faceted map showing the winning party in each state for each election year, along with the corresponding electoral college votes.\n\n\nShow the code\n# Step 1: Prepare Election Data (for multiple years)\nelection_data &lt;- DATA_PRESIDENT |&gt;\n  filter(office == \"US PRESIDENT\") |&gt;\n  group_by(year, state) |&gt;\n  slice_max(order_by = candidatevotes, n = 1) |&gt;\n  ungroup() |&gt;\n  mutate(\n    party_winner = ifelse(party_simplified == \"REPUBLICAN\", \"Republican\", \"Democrat\"),\n    state = tolower(state)  # Ensure compatibility with map data\n  )\n\n# Step 2: Prepare Map Data\nus_map &lt;- map_data(\"state\") |&gt;\n  mutate(region = tolower(region))\n\n# Step 3: Merge Election Data with Map Data\nmap_data_with_results &lt;- us_map |&gt;\n  left_join(election_data, by = c(\"region\" = \"state\"))\n\n# Step 4: Plot the Animated Map with Explicit Colors\nanimated_map &lt;- ggplot(map_data_with_results) +\n  geom_polygon(aes(x = long, y = lat, group = group, fill = party_winner), color = \"white\") +\n  scale_fill_manual(\n    values = c(\"Democrat\" = \"#0000FF\", \"Republican\" = \"#FF0000\"),  # Explicit blue and red\n    name = \"Election Winner\",\n    na.value = \"grey\"  # Handle unexpected NA values as grey\n  ) +\n  labs(\n    title = \"Presidential Election Results by State Over Time\",\n    subtitle = 'Year: {frame_time}',\n    caption = \"Data Source: MIT Election Data Science Lab\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(hjust = 0.5, size = 16),\n    plot.subtitle = element_text(hjust = 0.5, size = 12)\n  ) +\n  transition_time(year) +   # Add animation over time\n  ease_aes('linear')        # Smooth animation\n\n# Render the animation as before\n#animate(animated_map,nframes = 100,fps = 10,width = 800,height = 600, renderer = gifski_renderer(\"election_animation_high_res.gif\"))\n\nknitr::include_graphics(\"election_animation_high_res.gif\")"
  },
  {
    "objectID": "mp03.html#comparing-the-effects-of-ecv-allocation-rules",
    "href": "mp03.html#comparing-the-effects-of-ecv-allocation-rules",
    "title": "# Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Comparing the Effects of ECV Allocation Rules",
    "text": "Comparing the Effects of ECV Allocation Rules\nNow, we are finishing the exploration of the data, we can compare the effects of different Electoral College Vote (ECV).Go through the historical voting data and assign each state’s ECVs according to various strategies:\nState-Wide Winner-Take-All District-Wide Winner-Take-All + State-Wide “At Large” Votes State-Wide Proportional National Proportional\nFirst, we would join the Electoral College votes to the presidential data previously assigned.\n\n\nShow the code\n# Join the Electoral College votes to the presidential data\npres_data_filtered &lt;- DATA_PRESIDENT |&gt;\n  filter(party_simplified %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  left_join(ec_votes, by = \"state_po\")"
  },
  {
    "objectID": "mp03.html#state-wide-winner-take-all",
    "href": "mp03.html#state-wide-winner-take-all",
    "title": "# Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "State-Wide Winner-Take-All",
    "text": "State-Wide Winner-Take-All\n\n\nShow the code\n# Group by year and state to get the total votes for each candidate within each state\nstatewide_results &lt;- pres_data_filtered |&gt;\n  group_by(year, state, party_simplified) |&gt;\n  summarize(candidatevotes = sum(candidatevotes), EC = first(EC), .groups = \"drop\")\n\n# Determine the winner in each state and assign all ECVs to that candidate\nstatewide_winners &lt;- statewide_results |&gt;\n  group_by(year, state) |&gt;\n  filter(candidatevotes == max(candidatevotes)) |&gt;\n  ungroup()\n\n# Sum ECVs for each party by year\nstatewide_winners_ecv &lt;- statewide_winners |&gt;\n  group_by(year, party_simplified) |&gt;\n  summarize(ecv_total = sum(EC), .groups = \"drop\")\n\n# Plot ECVs over time for State-Wide Winner-Take-All\nggplot(statewide_winners_ecv, aes(x = year, y = ecv_total, color = party_simplified)) +\n  geom_line(size = 1) +  # Slightly thicker line for better visibility\n  geom_point(size = 2) +  # Small points to highlight each year\n  labs(\n    title = \"Electoral College Votes Over Time (State-Wide Winner-Take-All)\",\n    x = \"Year\",\n    y = \"Total Electoral College Votes (ECV)\",\n    color = \"Party\"\n  ) +\n  scale_color_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\")) +  # Custom party colors\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\n\nThis fist plot shows the ECVs over time for the State-Wide Winner-Take-All allocation method. We can see how the ECVs are distributed between the Democratic and Republican parties in each election year fluctuating over time. Each time a dot is above the other, it means that the party with the higher line won the election. The trends indicate that the differences between parties have decreased over time."
  },
  {
    "objectID": "mp03.html#district-wide-winner-take-all-state-wide-at-large-votes",
    "href": "mp03.html#district-wide-winner-take-all-state-wide-at-large-votes",
    "title": "# Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "District-Wide Winner-Take-All + State-Wide “At Large” Votes",
    "text": "District-Wide Winner-Take-All + State-Wide “At Large” Votes\n\n\nShow the code\n# Define the list of presidential election years\npresidential_years &lt;- seq(1976, 2020, by = 4)\n\n# Filter DATA_HOUSE and DATA_PRESIDENT to include only presidential election years\ndistrict_data &lt;- DATA_HOUSE |&gt;\n  filter(year %in% presidential_years)\n\npresident_data &lt;- DATA_PRESIDENT |&gt;\n  filter(year %in% presidential_years)\n\n# Step 1: Determine district-level winners for Democrats and Republicans only, and assign 1 ECV per district\ndistrict_winners &lt;- district_data |&gt;\n  filter(party %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  group_by(year, state, state_po, district, party) |&gt;\n  summarize(candidatevotes = sum(candidatevotes), .groups = \"drop\") |&gt;\n  group_by(year, state, state_po, district) |&gt;\n  filter(candidatevotes == max(candidatevotes)) |&gt;\n  ungroup() |&gt;\n  mutate(ECV = 1)  # Each district gets 1 ECV\n\n# Step 2: Calculate statewide winners for \"at-large\" ECVs for Democrats and Republicans only\nstatewide_winners &lt;- president_data |&gt;\n  filter(party_simplified %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  group_by(year, state, state_po, party_simplified) |&gt;\n  summarize(candidatevotes = sum(candidatevotes), .groups = \"drop\") |&gt;\n  group_by(year, state, state_po) |&gt;\n  filter(candidatevotes == max(candidatevotes)) |&gt;\n  ungroup() |&gt;\n  mutate(ECV = 2)  # Statewide winner gets 2 at-large ECVs\n\n# Step 3: Combine district and state-wide results, focusing only on Democrat and Republican results\ndistrict_plus_at_large &lt;- bind_rows(\n  district_winners |&gt; rename(party_simplified = party),\n  statewide_winners\n)\n\n# Step 4: Summarize ECV allocation by party and year\necv_allocation &lt;- district_plus_at_large |&gt;\n  group_by(year, party_simplified) |&gt;\n  summarize(total_ecv = sum(ECV), .groups = \"drop\")\n\n# Step 5: Verify total ECVs across all states per year to see how they differ from 538\ntotal_ecv_by_year &lt;- ecv_allocation |&gt;\n  group_by(year) |&gt;\n  summarize(yearly_total_ecv = sum(total_ecv))\n\n# Check for any year where the total ECVs do not equal 538\ntotal_ecv_check &lt;- total_ecv_by_year |&gt;\n  filter(yearly_total_ecv != 538)\n\ngt(total_ecv_check)\n\n\n\n\n\n\n\n\nyear\nyearly_total_ecv\n\n\n\n\n1976\n537\n\n\n1980\n537\n\n\n1984\n537\n\n\n1988\n537\n\n\n1992\n529\n\n\n1996\n537\n\n\n2000\n537\n\n\n2004\n537\n\n\n2008\n537\n\n\n2012\n537\n\n\n2016\n537\n\n\n\n\n\n\n\nThis this we prove that the total ECVs for each year are around 538, which is the total number of Electoral College votes. This confirms that the allocation method is working correctly and that the ECVs are being distributed as expected.\n\n\nShow the code\nggplot(ecv_allocation, aes(x = year, y = total_ecv, color = party_simplified)) +\n  geom_line(size = 1) +  # Slightly thicker line for better visibility\n  geom_point(size = 2) +  # Small points to emphasize each data point\n  labs(\n    title = \"Electoral College Votes Over Time (District-Wide + At-Large Allocation)\",\n    x = \"Year\",\n    y = \"Electoral College Votes (ECV)\",\n    color = \"Party\"\n  ) +\n  scale_color_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\")) +  # Custom party colors\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\n\nThis plot shows the ECVs over time for the District-Wide Winner-Take-All + State-Wide “At Large” Votes allocation method. We can see how the ECVs are distributed between the Democratic and Republican parties in each election year, fluctuating over time.\n#State-Wide Proportional\n\n\nShow the code\n# Step 1: Calculate vote share for Democrats and Republicans in each state\nstatewide_results &lt;- pres_data_filtered |&gt;\n  group_by(year, state, state_po, party_simplified, EC) |&gt;\n  summarize(candidatevotes = sum(candidatevotes), .groups = \"drop\") |&gt;\n  group_by(year, state, state_po) |&gt;\n  mutate(vote_share = candidatevotes / sum(candidatevotes)) |&gt;\n  ungroup()\n\n# Step 2: Allocate ECVs proportionally based on vote share\nstatewide_results &lt;- statewide_results |&gt;\n  mutate(proportional_ecv = round(vote_share * EC))  # Round to ensure integer ECVs\n\n# Step 3: Adjust ECVs to ensure they match the state's total (handle rounding errors)\n# For each state-year, adjust ECVs if the sum does not match the expected total\nstatewide_adjusted &lt;- statewide_results |&gt;\n  group_by(year, state, state_po) |&gt;\n  mutate(\n    ecv_adjustment = EC - sum(proportional_ecv),  # Calculate difference due to rounding\n    proportional_ecv = proportional_ecv + if_else(\n      row_number() == 1 & ecv_adjustment != 0, ecv_adjustment, 0\n    )  # Adjust the first party’s ECV by the remainder\n  ) |&gt;\n  ungroup()\n\n# Step 4: Summarize ECV allocation by party and year\necv_state_proportion &lt;- statewide_adjusted |&gt;\n  group_by(year, party_simplified) |&gt;\n  summarize(total_ecv = sum(proportional_ecv), .groups = \"drop\")\n\n# Step 5: Plot ECV vs Time for Democrats and Republicans (State-Wide Proportional)\nggplot(ecv_state_proportion, aes(x = year, y = total_ecv, color = party_simplified)) +\n  geom_line(size = 1) +  # Slightly thicker line for clarity\n  geom_point(size = 2) +  # Small points to emphasize each data point\n  labs(\n    title = \"Electoral College Votes Over Time (State-Wide Proportional)\",\n    x = \"Year\",\n    y = \"Electoral College Votes (ECV)\",\n    color = \"Party\"\n  ) +\n  scale_color_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\")) +  # Custom party colors\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\n\nWe can see a difference between this and the others methods, in this one we can see a clear advantage for the Democratic party, this is because the proportional allocation of ECVs is based on the vote share in each state. This method gives a more accurate representation of the popular vote in each state, which can lead to a more balanced distribution of ECVs between the two major parties."
  },
  {
    "objectID": "mp03.html#national-proportional",
    "href": "mp03.html#national-proportional",
    "title": "# Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "National Proportional",
    "text": "National Proportional\n\n\nShow the code\n# Step 1: Calculate the total national vote share for Democrats and Republicans\nnational_results &lt;- pres_data_filtered |&gt;\n  group_by(year, party_simplified) |&gt;\n  summarize(total_votes = sum(candidatevotes), .groups = \"drop\") |&gt;\n  group_by(year) |&gt;\n  mutate(national_vote_share = total_votes / sum(total_votes)) |&gt;\n  ungroup()\n\n# Step 2: Allocate ECVs based on national vote share\nnational_results &lt;- national_results |&gt;\n  mutate(proportional_ecv = round(national_vote_share * 538))  # Round to ensure integer ECVs\n\n# Step 3: Adjust ECVs to ensure they sum to 538 (handle rounding discrepancies)\n# Adjust any remainder due to rounding so that the sum equals 538\nnational_adjusted &lt;- national_results |&gt;\n  group_by(year) |&gt;\n  mutate(\n    ecv_adjustment = 538 - sum(proportional_ecv),  # Calculate difference due to rounding\n    proportional_ecv = proportional_ecv + if_else(\n      row_number() == 1 & ecv_adjustment != 0, ecv_adjustment, 0\n    )  # Adjust the first party's ECV by the remainder\n  ) |&gt;\n  ungroup()\n\n# Step 4: Plot ECV vs Time for Democrats and Republicans (National Proportional)\nggplot(national_adjusted, aes(x = year, y = proportional_ecv, color = party_simplified)) +\n  geom_line(size = 1) +  # Slightly thicker line for better visibility\n  geom_point(size = 2) +  # Add small points at each year for clarity\n  labs(\n    title = \"ECV Over Time by National Proportional Allocation\",\n    x = \"Year\",\n    y = \"Electoral College Votes (ECV)\",\n    color = \"Party\"\n  ) +\n  scale_color_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\")) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\n\nNow we would compared all this methods to see the differences between them.\n\n\nShow the code\n# Total number of elections\ntotal_elections &lt;- 12\n\n# Calculate the percentage of Republican wins for each method\n\n# State-Wide Winner-Take-All\nstatewide_republican_wins &lt;- statewide_winners_ecv |&gt;\n  group_by(year) |&gt;\n  summarize(winning_party = party_simplified[which.max(ecv_total)]) |&gt;\n  ungroup() |&gt;\n  count(winning_party) |&gt;\n  filter(winning_party == \"REPUBLICAN\") |&gt;\n  pull(n)\nstatewide_republican_percentage &lt;- round((statewide_republican_wins / total_elections) * 100,2)\nstatewide_democratic_percentage &lt;- 100 - statewide_republican_percentage\n\n# District-Wide Winner-Take-All + At-Large\ndistrict_republican_wins &lt;- ecv_allocation |&gt;\n  group_by(year) |&gt;\n  summarize(winning_party = party_simplified[which.max(total_ecv)]) |&gt;\n  ungroup() |&gt;\n  count(winning_party) |&gt;\n  filter(winning_party == \"REPUBLICAN\") |&gt;\n  pull(n)\ndistrict_republican_percentage &lt;- round((district_republican_wins / total_elections) * 100,2)\ndistrict_democratic_percentage &lt;- 100 - district_republican_percentage\n\n# State-Wide Proportional\nstate_proportional_republican_wins &lt;- ecv_state_proportion |&gt;\n  group_by(year) |&gt;\n  summarize(winning_party = party_simplified[which.max(total_ecv)]) |&gt;\n  ungroup() |&gt;\n  count(winning_party) |&gt;\n  filter(winning_party == \"REPUBLICAN\") |&gt;\n  pull(n)\nstate_proportional_republican_percentage &lt;- round((state_proportional_republican_wins / total_elections) * 100,2)\nstate_proportional_democratic_percentage &lt;- 100 - state_proportional_republican_percentage\n\n# National Proportional\nnational_proportional_republican_wins &lt;- national_adjusted |&gt;\n  group_by(year) |&gt;\n  summarize(winning_party = party_simplified[which.max(proportional_ecv)]) |&gt;\n  ungroup() |&gt;\n  count(winning_party) |&gt;\n  filter(winning_party == \"REPUBLICAN\") |&gt;\n  pull(n)\nnational_proportional_republican_percentage &lt;- round((national_proportional_republican_wins / total_elections) * 100, 2)\nnational_proportional_democratic_percentage &lt;- 100 - national_proportional_republican_percentage\n\n# Create a data frame to summarize the results\nwin_percentage_table &lt;- data.frame(\n  Party = c(\"REPUBLICAN\", \"DEMOCRAT\"),\n  `State-Wide Winner-Take-All` = c(statewide_republican_percentage, statewide_democratic_percentage),\n  `District-Wide + At-Large` = c(district_republican_percentage, district_democratic_percentage),\n  `State-Wide Proportional` = c(state_proportional_republican_percentage, state_proportional_democratic_percentage),\n  `National Proportional` = c(national_proportional_republican_percentage, national_proportional_democratic_percentage)\n)\n\ngt(win_percentage_table) \n\n\n\n\n\n\n\n\nParty\nState.Wide.Winner.Take.All\nDistrict.Wide...At.Large\nState.Wide.Proportional\nNational.Proportional\n\n\n\n\nREPUBLICAN\n50\n50\n33.33\n33.33\n\n\nDEMOCRAT\n50\n50\n66.67\n66.67"
  },
  {
    "objectID": "mp03.html#analysis-of-each-scheme",
    "href": "mp03.html#analysis-of-each-scheme",
    "title": "# Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Analysis of Each Scheme",
    "text": "Analysis of Each Scheme\nState-Wide Winner-Take-All: This widely used method awards all ECVs to the state-wide popular vote winner, often ignoring narrower vote margins. Result: Produced a balanced outcome, with each party winning 50% of elections (1976–2020), but often distorts national popular vote representation.\nDistrict-Wide Winner-Take-All + At-Large: ECVs are awarded by district, with two at-large votes for the state winner. Result: Also produced a 50/50 split, providing a slightly more detailed reflection of local preferences, but still favoring swing districts.\nState-Wide Proportional: ECVs are split based on each candidate’s vote share within a state, offering a fairer reflection of popular support. Result: Democrats won 66.7% of elections, indicating a tilt toward populous urban centers.\nNational Proportional: Allocates ECVs based on the national popular vote percentage. Result: Similar to State-Wide Proportional, with Democrats winning 66.7% of elections, showing a national popular vote bias."
  },
  {
    "objectID": "mp03.html#conclusion-the-fairest-scheme",
    "href": "mp03.html#conclusion-the-fairest-scheme",
    "title": "# Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Conclusion: The Fairest Scheme",
    "text": "Conclusion: The Fairest Scheme\nState-Wide Proportional is arguably the fairest, as it represents the actual vote distribution within each state. However, it significantly shifts outcomes, favoring Democrats due to higher urban population density. For instance, in a year like 2000, where Republicans narrowly won in the actual ECV count, a proportional method could have swung the outcome.\nOverall, State-Wide Proportional most accurately reflects voter preferences but tilts outcomes toward the party with broad, urban support."
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "",
    "text": "Selecting a retirement plan is one of the most critical financial decisions for new CUNY faculty, as it has permanent implications for their long-term financial security. Once chosen, the plan cannot be changed, making it essential to consider the potential risks and rewards. This project evaluates the Teachers Retirement System (TRS) and the Optional Retirement Plan (ORP) using historical economic data and Monte Carlo simulations. By modeling contributions, investment returns, and inflation-adjusted benefits, we aim to provide a personalized, data-driven recommendation tailored to individual financial goals and risk tolerance."
  },
  {
    "objectID": "mp04.html#introduction",
    "href": "mp04.html#introduction",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "",
    "text": "Selecting a retirement plan is one of the most critical financial decisions for new CUNY faculty, as it has permanent implications for their long-term financial security. Once chosen, the plan cannot be changed, making it essential to consider the potential risks and rewards. This project evaluates the Teachers Retirement System (TRS) and the Optional Retirement Plan (ORP) using historical economic data and Monte Carlo simulations. By modeling contributions, investment returns, and inflation-adjusted benefits, we aim to provide a personalized, data-driven recommendation tailored to individual financial goals and risk tolerance."
  },
  {
    "objectID": "mp04.html#objectives",
    "href": "mp04.html#objectives",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Objectives",
    "text": "Objectives\nThe primary goal of this project is to equip new CUNY faculty with an evidence-based recommendation for selecting the optimal retirement plan (TRS or ORP). This involves analyzing historical economic and financial data to assess how each plan performs under different economic conditions. We will utilize the following datasets:\n\nRate of Inflation: Inflation erodes the purchasing power of retirement benefits. For TRS, it determines the annual adjustment in benefits, while for ORP, it impacts the real value of savings and withdrawals.\nRate of Wage Growth: Wage growth directly influences contributions to both plans. For TRS, it affects the final average salary (FAS), which determines retirement payouts. For ORP, higher wages lead to larger investment contributions.\nUS Equity Market Returns: Equity market performance is crucial for ORP participants, as their investments grow with market returns. US equities represent a significant portion of most retirement portfolios.\nInternational Equity Market Returns: Diversification through international equities reduces risk and enhances returns for ORP participants. It provides a broader perspective on investment performance beyond the US market.\nBond Returns: Bonds are a critical component of retirement portfolios, especially for ORP participants as they approach retirement. Bonds provide stability and reduce volatility in investment returns.\nShort-Term Debt Returns: Short-term debt, such as Treasury yields, represents a safe investment option with lower risk. It is particularly relevant for retirees seeking liquidity and capital preservation.\n\nBy combining insights from these datasets, we will simulate potential retirement outcomes for each plan, identify risks such as running out of funds, and quantify the financial benefits of each option. This comprehensive approach ensures that our recommendation aligns with the diverse financial priorities of faculty members.\n\nLibraries\nThe following are the libraries we would use throughtout the project.\n\n\nShow the code\nlibrary(httr2)    # For API requests\nlibrary(dplyr)    # For data manipulation\nlibrary(ggplot2)  # For visualization\nlibrary(zoo)      # For handling time-series data and filling missing values"
  },
  {
    "objectID": "mp04.html#data-sources",
    "href": "mp04.html#data-sources",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Data Sources",
    "text": "Data Sources\n\nLibraries\nThe following are the libraries we would use throughtout the project.\n\n\nShow the code\nlibrary(httr2)    # For API requests\nlibrary(dplyr)    # For data manipulation\nlibrary(ggplot2)  # For visualization\nlibrary(zoo)      # For handling time-series data and filling missing values"
  },
  {
    "objectID": "mp04.html#data-collection",
    "href": "mp04.html#data-collection",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Data Collection",
    "text": "Data Collection\nFor this project, we will use data from two economic and financial data sources:\n\nAlphaVantage: A commercial stock market data provider.\nFRED: The Federal Reserve Economic Data repository maintained by the Federal Reserve Bank of St. Louis.\n\nFRED is free to access, but AlphaVantage is a commercial service requiring a subscription. For this mini-project, the free tier of AlphaVantage will suffice.\n\nLinks to Resources\n\nAlphaVantage\nFRED (Federal Reserve Economic Data)"
  },
  {
    "objectID": "mp04.html#data-collection-1",
    "href": "mp04.html#data-collection-1",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "DATA COLLECTION",
    "text": "DATA COLLECTION\nWe would proceed to collect the data from the sources mentioned above. and we would do it on a monthly basis for the last 15 years.\nTod do so, we would use their Apis:\n\n\nShow the code\n# Define the base URL for the FRED API\nfred_url &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\n\n# Define the base URL for the AlphaVantage API\nalphavantage_url &lt;- \"https://www.alphavantage.co/query\"\n\n# Read FRED API key from a file\nfred_key &lt;- readLines(\"fred_key.txt\") # Ensure this file contains only your FRED API key\n\n# Read AlphaVantage API key from a file\nalphavantage_key &lt;- readLines(\"alphavantage_key.txt\") # Ensure this file contains only your AlphaVantage API key\n\n# Montly basis for the last 15 years\nTime_Montly =as.Date(\"2009-01-01\")\n\n\n\nFetching Rate of Inflation (CPI for NYC Metro Area) from FRED\n\n\n\nShow the code\n# Fetch Inflation Data from FRED (CPI for NYC Metro Area)\ninflation_req &lt;- request(fred_url) |&gt;\n  req_url_query(\n    series_id = \"CUUR0400SA0\",  # CPI for All Urban Consumers: NYC Metro Area\n    api_key = fred_key,\n    file_type = \"json\"\n  )\n\ninflation_resp &lt;- inflation_req |&gt; req_perform()\ninflation_data &lt;- resp_body_json(inflation_resp)$observations\n\n# Convert to DataFrame\ninflation_df &lt;- data.frame(\n  date = as.Date(sapply(inflation_data, function(x) x$date)),\n  inflation_rate = as.numeric(sapply(inflation_data, function(x) x$value))\n)\n\n\nWarning in data.frame(date = as.Date(sapply(inflation_data, function(x)\nx$date)), : NAs introduced by coercion\n\n\nShow the code\n# Filter the data for the last 15 years\ninflation_df &lt;- inflation_df |&gt; filter(date &gt;= Time_Montly)\n\n# Count the rows and check for missing data\ncat(\"Total Rows in Inflation Data: \", nrow(inflation_df), \"\\n\")\n\n\nTotal Rows in Inflation Data:  190 \n\n\nShow the code\ncat(\"Rows with Missing Inflation Data: \", sum(is.na(inflation_df$inflation_rate)), \"\\n\")\n\n\nRows with Missing Inflation Data:  0 \n\n\nShow the code\n# Preview the data\ntail(inflation_df)\n\n\n          date inflation_rate\n185 2024-05-01        334.292\n186 2024-06-01        333.662\n187 2024-07-01        333.174\n188 2024-08-01        333.442\n189 2024-09-01        334.265\n190 2024-10-01        334.558\n\n\n\nFetching Rate of Wage Growth from FRED, We’ll use the AHETPI series to measure wage growth.\n\n\n\nShow the code\n# Fetch Wage Growth Data from FRED\nwage_growth_req &lt;- request(fred_url) |&gt;\n  req_url_query(\n    series_id = \"AHETPI\", # Average Hourly Earnings of Production and Nonsupervisory Employees\n    api_key = fred_key,\n    file_type = \"json\"\n  )\n\nwage_growth_resp &lt;- wage_growth_req |&gt; req_perform()\nwage_growth_data &lt;- resp_body_json(wage_growth_resp)$observations\n\n# Convert to a DataFrame\nwage_growth_df &lt;- data.frame(\n  date = as.Date(sapply(wage_growth_data, function(x) x$date)), # Parse date\n  wage_growth = as.numeric(sapply(wage_growth_data, function(x) x$value)) # Convert to numeric\n)\n\n# Filter data for the last 15–20 years\nwage_growth_df &lt;- wage_growth_df |&gt; filter(date &gt;= Time_Montly)\n\n# Check the data\ntotal_rows &lt;- nrow(wage_growth_df)\nmissing_rows &lt;- sum(is.na(wage_growth_df$wage_growth))\n\n# Output results\ncat(\"Total Rows in Wage Growth Data: \", total_rows, \"\\n\")\n\n\nTotal Rows in Wage Growth Data:  190 \n\n\nShow the code\ncat(\"Rows with Missing Wage Growth Data: \", missing_rows, \"\\n\")\n\n\nRows with Missing Wage Growth Data:  0 \n\n\nShow the code\n# Preview the first few rows of the dataset\ntail(wage_growth_df)\n\n\n          date wage_growth\n185 2024-05-01       29.95\n186 2024-06-01       30.07\n187 2024-07-01       30.16\n188 2024-08-01       30.26\n189 2024-09-01       30.36\n190 2024-10-01       30.48\n\n\n\nFetching US Equity Market Returns from AlphaVantage\n\n\n\nShow the code\n# Fetch US Equity Market Data from AlphaVantage\nus_equity_req &lt;- request(alphavantage_url) |&gt;\n  req_url_query(\n    `function` = \"TIME_SERIES_MONTHLY_ADJUSTED\",\n    symbol = \"SPY\", # S&P 500 ETF\n    apikey = alphavantage_key\n  )\nus_equity_resp &lt;- us_equity_req |&gt; req_perform()\nus_equity_data &lt;- resp_body_json(us_equity_resp)$`Monthly Adjusted Time Series`\n\n# Convert to DataFrame\nus_equity_df &lt;- data.frame(\n  date = as.Date(names(us_equity_data)), # Extract dates\n  us_equity_close = as.numeric(sapply(us_equity_data, function(x) x$`5. adjusted close`)) # Extract adjusted close prices\n)\n\n# Standardize dates to the first of each month\nus_equity_df$date &lt;- as.Date(format(us_equity_df$date, \"%Y-%m-01\"))\n\n# Filter for the past 15 years\nus_equity_df &lt;- us_equity_df |&gt; filter(date &gt;= Time_Montly)\n\n# Check Data Quality\ncat(\"Total Rows in US Equity Data: \", nrow(us_equity_df), \"\\n\")\n\n\nTotal Rows in US Equity Data:  192 \n\n\nShow the code\ncat(\"Rows with Missing US Equity Data: \", sum(is.na(us_equity_df$us_equity_close)), \"\\n\")\n\n\nRows with Missing US Equity Data:  0 \n\n\nShow the code\n# Preview Data\ntail(us_equity_df)\n\n\n          date us_equity_close\n187 2009-06-01         69.2692\n188 2009-05-01         69.3159\n189 2009-04-01         65.4879\n190 2009-03-01         59.5698\n191 2009-02-01         54.9799\n192 2009-01-01         61.5986\n\n\n\nFetching International Equity Market Returns from AlphaVantage\n\n\n\nShow the code\n# Fetch International Equity Market Data from AlphaVantage\nintl_req &lt;- request(alphavantage_url) |&gt;\n  req_url_query(\n    `function` = \"TIME_SERIES_MONTHLY_ADJUSTED\",\n    symbol = \"VEA\", # Vanguard FTSE Developed Markets ETF\n    apikey = alphavantage_key\n  )\n\nintl_resp &lt;- intl_req |&gt; req_perform()\nintl_data &lt;- resp_body_json(intl_resp)$`Monthly Adjusted Time Series`\n\n# Convert the fetched data into a data frame\nintl_equity_df &lt;- data.frame(\n  date = as.Date(names(intl_data)),\n  intl_equity_close = as.numeric(sapply(intl_data, function(x) x$`5. adjusted close`))\n)\n\n# Standardize dates to the first of each month\nintl_equity_df$date &lt;- as.Date(format(intl_equity_df$date, \"%Y-%m-01\"))\n\n# Filter for the past 15 years\nintl_equity_df &lt;- intl_equity_df |&gt; filter(date &gt;= Time_Montly)\n\n# Print information about the dataset\ncat(\"Total Rows in International Equity Data: \", nrow(intl_equity_df), \"\\n\")\n\n\nTotal Rows in International Equity Data:  192 \n\n\nShow the code\ncat(\"Rows with Missing International Equity Data: \", sum(is.na(intl_equity_df$intl_equity_close)), \"\\n\")\n\n\nRows with Missing International Equity Data:  0 \n\n\nShow the code\nhead(intl_equity_df)\n\n\n        date intl_equity_close\n1 2024-12-01           50.5600\n2 2024-11-01           50.3000\n3 2024-10-01           50.1000\n4 2024-09-01           52.8100\n5 2024-08-01           52.2448\n6 2024-07-01           50.7689\n\n\n\nFetching Bond Returns from AlphaVantage\n\n\n\nShow the code\n# Fetch Bond Yield Data (Daily)\nbond_req &lt;- request(fred_url) |&gt;\n  req_url_query(\n    series_id = \"DGS10\", # 10-Year Treasury Yield\n    api_key = fred_key,\n    file_type = \"json\"\n  )\n\nbond_resp &lt;- bond_req |&gt; req_perform()\nbond_data &lt;- resp_body_json(bond_resp)$observations\n\n# Convert to DataFrame\nbond_df &lt;- data.frame(\n  date = as.Date(sapply(bond_data, function(x) x$date)),\n  bond_yield = as.numeric(sapply(bond_data, function(x) x$value))\n)\n\n\nWarning in data.frame(date = as.Date(sapply(bond_data, function(x) x$date)), :\nNAs introduced by coercion\n\n\nShow the code\n# Summary of Fetched Data\ncat(\"Total Rows in Bond Yield Data: \", nrow(bond_df), \"\\n\")\n\n\nTotal Rows in Bond Yield Data:  16415 \n\n\nShow the code\ncat(\"Rows with Missing Bond Yield Data: \", sum(is.na(bond_df$bond_yield)), \"\\n\")\n\n\nRows with Missing Bond Yield Data:  700 \n\n\nShow the code\n# Downsample to Monthly Averages\nbond_df &lt;- bond_df |&gt;\n  mutate(month = format(date, \"%Y-%m\")) |&gt;# Extract year-month for grouping\n  group_by(month) |&gt;\n  summarize(\n    bond_yield = mean(bond_yield, na.rm = TRUE), # Average yield for the month\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(date = as.Date(paste0(month, \"-01\"))) |&gt;# Convert back to Date\n  select(date, bond_yield)\n\n# Filter for the last 15 years\nbond_df &lt;- bond_df |&gt; filter(date &gt;= Time_Montly)\n\n# Summary after Downsampling\ncat(\"Total Rows in Bond Yield Data (Monthly): \", nrow(bond_df), \"\\n\")\n\n\nTotal Rows in Bond Yield Data (Monthly):  192 \n\n\nShow the code\ncat(\"Rows with Missing Bond Yield Data: \", sum(is.na(bond_df$bond_yield)), \"\\n\")\n\n\nRows with Missing Bond Yield Data:  0 \n\n\nShow the code\n# Preview the Data\nprint(tail(bond_df))\n\n\n# A tibble: 6 × 2\n  date       bond_yield\n  &lt;date&gt;          &lt;dbl&gt;\n1 2024-07-01       4.25\n2 2024-08-01       3.87\n3 2024-09-01       3.72\n4 2024-10-01       4.10\n5 2024-11-01       4.36\n6 2024-12-01       4.19\n\n\n\nFetching Short-Term Debt Returns from AlphaVantage: for this dataset, we need to downsample the daily data to monthly averages to align with the other datasets.\n\n\n\nShow the code\n# Fetch Short-Term Debt Data (2-Year Treasury Yield) - Attempt Monthly Query\nshort_term_req &lt;- request(fred_url) |&gt;\n  req_url_query(\n    series_id = \"DGS2\", # 2-Year Treasury Yield\n    frequency = \"m\", # Request monthly data directly\n    api_key = fred_key,\n    file_type = \"json\"\n  )\n\nshort_term_resp &lt;- short_term_req |&gt; req_perform()\nshort_term_data &lt;- resp_body_json(short_term_resp)$observations\n\n# Convert JSON to DataFrame\nshort_term_df &lt;- data.frame(\n  date = as.Date(sapply(short_term_data, function(x) x$date), format = \"%Y-%m-%d\"),\n  short_term_yield = as.numeric(sapply(short_term_data, function(x) x$value))\n)\n\n\nWarning in data.frame(date = as.Date(sapply(short_term_data, function(x)\nx$date), : NAs introduced by coercion\n\n\nShow the code\n# Filter for the Last 15 Years\nshort_term_df &lt;- short_term_df |&gt; filter(date &gt;= Time_Montly)\n\n# Check Total Rows and Missing Data\ncat(\"Total Rows in Short-Term Yield Data (Monthly): \", nrow(short_term_df), \"\\n\")\n\n\nTotal Rows in Short-Term Yield Data (Monthly):  192 \n\n\nShow the code\ncat(\"Rows with Missing Short-Term Yield Data: \", sum(is.na(short_term_df$short_term_yield)), \"\\n\")\n\n\nRows with Missing Short-Term Yield Data:  1 \n\n\nShow the code\n# Preview the Data\ntail(short_term_df)\n\n\n          date short_term_yield\n187 2024-07-01             4.50\n188 2024-08-01             3.97\n189 2024-09-01             3.62\n190 2024-10-01             3.97\n191 2024-11-01             4.26\n192 2024-12-01               NA"
  },
  {
    "objectID": "mp04.html#data-preprocessing-and-cleaning",
    "href": "mp04.html#data-preprocessing-and-cleaning",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Data Preprocessing and Cleaning",
    "text": "Data Preprocessing and Cleaning\nNow that we have collected the data, we can proceed to the next step, which is data preprocessing. We we gonna unify the data and deal with missing values by using bootstrapping.\n\n\nShow the code\n# Load necessary libraries for analysis and visualization\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Define a function to cap outliers\ncap_outlier &lt;- function(x, lower_bound, upper_bound) {\n  x[x &lt; lower_bound] &lt;- lower_bound\n  x[x &gt; upper_bound] &lt;- upper_bound\n  return(x)\n}\n\n# Standardize dates to the first of each month for all datasets\ndatasets &lt;- list(inflation_df, wage_growth_df, us_equity_df, intl_equity_df, bond_df, short_term_df)\ndatasets &lt;- lapply(datasets, function(df) {\n  df$date &lt;- as.Date(format(df$date, \"%Y-%m-01\"))\n  return(df)\n})\n\n# Unpack datasets back\ninflation_df &lt;- datasets[[1]]\nwage_growth_df &lt;- datasets[[2]]\nus_equity_df &lt;- datasets[[3]]\nintl_equity_df &lt;- datasets[[4]]\nbond_df &lt;- datasets[[5]]\nshort_term_df &lt;- datasets[[6]]\n\n# Create master date sequence\nstart_date &lt;- as.Date(\"2009-01-01\")\nend_date &lt;- as.Date(format(Sys.Date(), \"2024-10-01\"))\ndate_seq &lt;- data.frame(date = seq(from = start_date, to = end_date, by = \"month\"))\ncat(\"Expected number of rows (months) over 15 years: \", nrow(date_seq), \"\\n\")\n\n\nExpected number of rows (months) over 15 years:  190 \n\n\nShow the code\n# Perform left joins for all datasets\ncombined_data &lt;- date_seq %&gt;%\n  left_join(inflation_df, by = \"date\") %&gt;%\n  left_join(wage_growth_df, by = \"date\") %&gt;%\n  left_join(us_equity_df, by = \"date\") %&gt;%\n  left_join(intl_equity_df, by = \"date\") %&gt;%\n  left_join(bond_df, by = \"date\") %&gt;%\n  left_join(short_term_df, by = \"date\")\n\n# Check for missing data\ncat(\"Total Rows in Combined Dataset: \", nrow(combined_data), \"\\n\")\n\n\nTotal Rows in Combined Dataset:  190 \n\n\nShow the code\nmissing_summary &lt;- combined_data %&gt;%\n  summarize(across(everything(), ~ sum(is.na(.))))\ncat(\"Missing Data Summary:\\n\")\n\n\nMissing Data Summary:\n\n\nShow the code\nprint(missing_summary)\n\n\n  date inflation_rate wage_growth us_equity_close intl_equity_close bond_yield\n1    0              0           0               0                 0          0\n  short_term_yield\n1                0\n\n\nShow the code\n# Apply bootstrapping for missing values\ncat(\"Applying bootstrap for missing data...\\n\")\n\n\nApplying bootstrap for missing data...\n\n\nShow the code\ncombined_data &lt;- combined_data %&gt;%\n  mutate(\n    inflation_rate = replace(inflation_rate,\n                             is.na(inflation_rate),\n                             sample(na.omit(inflation_rate), size = sum(is.na(inflation_rate)), replace = TRUE)),\n    wage_growth = replace(wage_growth,\n                          is.na(wage_growth),\n                          sample(na.omit(wage_growth), size = sum(is.na(wage_growth)), replace = TRUE)),\n    us_equity_close = replace(us_equity_close,\n                              is.na(us_equity_close),\n                              sample(na.omit(us_equity_close), size = sum(is.na(us_equity_close)), replace = TRUE)),\n    intl_equity_close = replace(intl_equity_close,\n                                is.na(intl_equity_close),\n                                sample(na.omit(intl_equity_close), size = sum(is.na(intl_equity_close)), replace = TRUE)),\n    bond_yield = replace(bond_yield,\n                         is.na(bond_yield),\n                         sample(na.omit(bond_yield), size = sum(is.na(bond_yield)), replace = TRUE)),\n    short_term_yield = replace(short_term_yield,\n                               is.na(short_term_yield),\n                               sample(na.omit(short_term_yield), size = sum(is.na(short_term_yield)), replace = TRUE))\n  )\n\n# Handle outliers by capping them\ncombined_data &lt;- combined_data %&gt;%\n  mutate(\n    inflation_rate = cap_outlier(inflation_rate, \n                                 lower_bound = quantile(inflation_rate, 0.01, na.rm = TRUE), \n                                 upper_bound = quantile(inflation_rate, 0.99, na.rm = TRUE)),\n    wage_growth = cap_outlier(wage_growth, \n                              lower_bound = quantile(wage_growth, 0.01, na.rm = TRUE), \n                              upper_bound = quantile(wage_growth, 0.99, na.rm = TRUE)),\n    us_equity_close = cap_outlier(us_equity_close, \n                                  lower_bound = quantile(us_equity_close, 0.01, na.rm = TRUE), \n                                  upper_bound = quantile(us_equity_close, 0.99, na.rm = TRUE)),\n    intl_equity_close = cap_outlier(intl_equity_close, \n                                    lower_bound = quantile(intl_equity_close, 0.01, na.rm = TRUE), \n                                    upper_bound = quantile(intl_equity_close, 0.99, na.rm = TRUE)),\n    bond_yield = cap_outlier(bond_yield, \n                             lower_bound = quantile(bond_yield, 0.01, na.rm = TRUE), \n                             upper_bound = quantile(bond_yield, 0.99, na.rm = TRUE)),\n    short_term_yield = cap_outlier(short_term_yield, \n                                   lower_bound = quantile(short_term_yield, 0.01, na.rm = TRUE), \n                                   upper_bound = quantile(short_term_yield, 0.99, na.rm = TRUE))\n  )\n\n# Validate the final dataset\ncat(\"After handling missing values and outliers:\\n\")\n\n\nAfter handling missing values and outliers:\n\n\nShow the code\nmissing_summary_after &lt;- combined_data %&gt;%\n  summarize(across(everything(), ~ sum(is.na(.))))\nprint(missing_summary_after)\n\n\n  date inflation_rate wage_growth us_equity_close intl_equity_close bond_yield\n1    0              0           0               0                 0          0\n  short_term_yield\n1                0\n\n\nShow the code\n# You can now proceed with additional analysis or visualization\ntail(combined_data)\n\n\n          date inflation_rate wage_growth us_equity_close intl_equity_close\n185 2024-05-01        334.268      29.950        524.0620          50.10910\n186 2024-06-01        333.662      30.070        542.5534          49.28300\n187 2024-07-01        333.174      30.160        549.1232          50.76890\n188 2024-08-01        333.442      30.260        561.9538          50.93125\n189 2024-09-01        334.265      30.271        562.6893          50.93125\n190 2024-10-01        334.268      30.271        562.6893          50.10000\n    bond_yield short_term_yield\n185   4.482273             4.86\n186   4.305263             4.74\n187   4.248636             4.50\n188   3.870909             3.97\n189   3.723500             3.62\n190   4.095455             3.97\n\n\n\n\nShow the code\ntail(combined_data)\n\n\n          date inflation_rate wage_growth us_equity_close intl_equity_close\n185 2024-05-01        334.268      29.950        524.0620          50.10910\n186 2024-06-01        333.662      30.070        542.5534          49.28300\n187 2024-07-01        333.174      30.160        549.1232          50.76890\n188 2024-08-01        333.442      30.260        561.9538          50.93125\n189 2024-09-01        334.265      30.271        562.6893          50.93125\n190 2024-10-01        334.268      30.271        562.6893          50.10000\n    bond_yield short_term_yield\n185   4.482273             4.86\n186   4.305263             4.74\n187   4.248636             4.50\n188   3.870909             3.97\n189   3.723500             3.62\n190   4.095455             3.97"
  },
  {
    "objectID": "mp04.html#initial-analysis",
    "href": "mp04.html#initial-analysis",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Initial Analysis",
    "text": "Initial Analysis\nNow that we have a clean dataset, we can perform some initial analysis to understand the trends and relationships between the variables.\nWe will visualize the data using line plots and we going to display them in pairs to compare the trends between the variables. so it would be inflation rate and wage growth, US equity and International equity, and Bond Yield and Short-Term Yield.\n\n\nShow the code\n# Create plot for Inflation Rate and Wage Growth with corrected labels and separate y-axes\np1 &lt;- ggplot(combined_data, aes(x = date)) +\n  # Inflation Rate\n  geom_line(aes(y = inflation_rate), color = \"blue\") +\n  scale_y_continuous(\n    name = \"Inflation Rate Value (NYC)\",  # Corrected label for inflation rate\n    sec.axis = sec_axis(~ . * 0, name = \" \")  # Create empty secondary axis for separation\n  ) +\n  # Wage Growth (scaled for visibility)\n  geom_line(aes(y = wage_growth * 10), color = \"green\") +\n  scale_y_continuous(\n    name = \"Inflation Rate Value (NYC)\",  # Corrected label for wage growth\n    sec.axis = sec_axis(~ . / 10, name = \"Wage Growth (Hourly Earnings in $)\")  # Corrected secondary label\n  ) +\n  labs(title = \"Inflation Rate and Wage Growth Over Time\", x = \"Date\") +\n  theme_minimal() +\n  theme(\n    axis.title.y.left = element_text(color = \"blue\", size = 12),\n    axis.title.y.right = element_text(color = \"green\", size = 12)\n  )\n\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\nShow the code\n# Create plot for US Equity and International Equity\np2 &lt;- ggplot(combined_data, aes(x = date)) +\n  # US Equity Return\n  geom_line(aes(y = us_equity_close), color = \"red\") +  \n  scale_y_continuous(\n    name = \"US Equity Return (Adjusted Close)\"  \n  ) +\n  # International Equity Return (scaled for visibility)\n  geom_line(aes(y = intl_equity_close * 10), color = \"purple\") +  \n  scale_y_continuous(\n    name = \"US Equity Return (Adjusted Close)\",\n    sec.axis = sec_axis(~ . / 10, name = \"Intl Equity Return\")  \n  ) +\n  labs(title = \"US and International Equity Returns Over Time\", x = \"Date\") +\n  theme_minimal() +\n  theme(\n    axis.title.y.left = element_text(color = \"red\", size = 12),\n    axis.title.y.right = element_text(color = \"purple\", size = 12)\n  )\n\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\nShow the code\n# Create plot for Bond Yield and Short-Term Yield \np3 &lt;- ggplot(combined_data, aes(x = date)) +\n  # Bond Yield\n  geom_line(aes(y = bond_yield * 2), color = \"orange\") +  \n  scale_y_continuous(\n    name = \"Bond Yield (%)\", \n    sec.axis = sec_axis(~ . / 2, name = \"Bond Yield (%)\")  \n  ) +\n  # Short-Term Yield\n  geom_line(aes(y = short_term_yield * 2), color = \"brown\") +  \n  scale_y_continuous(\n    name = \"Short-Term Yield (%)\", \n    sec.axis = sec_axis(~ . / 2, name = \"Short-Term Yield (%)\")  \n  ) +\n  labs(title = \"Bond Yield and Short-Term Yield Over Time\", x = \"Date\") +\n  theme_minimal() +\n  theme(\n    axis.title.y.left = element_text(color = \"orange\", size = 12),\n    axis.title.y.right = element_text(color = \"brown\", size = 12)\n  )\n\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\nShow the code\n# Display all three plots\np1\n\n\n\n\n\n\n\n\n\nShow the code\np2\n\n\n\n\n\n\n\n\n\nShow the code\np3\n\n\n\n\n\n\n\n\n\n\nCorrelation matrix\nWe will calculate the correlation matrix to understand the relationships between the variables. This will help us identify any significant correlations that can inform our analysis and decision-making process.\n\n\nShow the code\n# Load the necessary library\nlibrary(ggcorrplot)\n\n# Select only numeric columns\nnumeric_data &lt;- combined_data %&gt;%\n  select(where(is.numeric))\n\n# Compute the correlation matrix\ncor_matrix &lt;- cor(numeric_data, use = \"complete.obs\")\n\n# Visualize the correlation matrix using ggcorrplot\nggcorrplot(cor_matrix, \n           type = \"lower\",          # Only show the lower triangle\n           lab = TRUE,              # Show correlation coefficient labels\n           lab_size = 3,            # Adjust label size\n           method = \"circle\",       # Use circle shapes for correlation values\n           colors = c(\"red\", \"white\", \"blue\"), # Color scale\n           title = \"Correlation Matrix\", # Plot title\n           ggtheme = theme_minimal(), # Minimalistic theme\n           tl.cex = 12,             # Title label size\n           tl.col = \"black\"         # Title text color\n)"
  },
  {
    "objectID": "mp04.html#key-findings",
    "href": "mp04.html#key-findings",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Key Findings:",
    "text": "Key Findings:\n\n1. Inflation Rate and Wage Growth:\n\nCorrelation: 0.98 (Very strong)\nInterpretation: When inflation increases, wage growth tends to follow suit. This suggests that as the cost of living rises, your wages are likely to increase as well, which helps you keep up with inflation.\n\n\n\n2. Stock Market Returns:\n\nUS and International Markets:\n\nCorrelation: 0.97 (Very strong)\nInterpretation: US and international stock markets have a highly positive correlation, meaning they often move in the same direction. This makes diversification between US and international markets a good strategy for reducing risk while maintaining growth potential.\n\n\n\nUS Equity Returns and Bond Yields:\n\nCorrelation: 0.68 (Moderate)\nInterpretation: The moderate correlation suggests that when US stocks perform well, bond yields may not follow the same trend. Investors need to balance their portfolios between stocks and bonds, taking into account their risk tolerance and time horizon.\n\n\n\n\n3. Inflation and Other Variables:\n\nInflation and US Stock Returns:\n\nCorrelation: 0.92 (Strong)\nInterpretation: Inflation and US stock returns are highly correlated. As inflation rises, US stocks tend to perform well, making stocks a potential hedge against inflation.\n\n\n\nInflation and International Stock Returns:\n\nCorrelation: 0.97 (Very strong)\nInterpretation: Like US stocks, international stocks also tend to rise with inflation, making global diversification a viable strategy for mitigating inflation risk.\n\n\n\nInflation and Bond Yields:\n\nCorrelation: 0.72 (Moderate)\nInterpretation: Inflation and bond yields are moderately correlated. Rising inflation generally leads to higher bond yields, which can help protect fixed-income investments from losing purchasing power.\n\n\n\n\n4. Bond Yield and Short-Term Yield:\n\nCorrelation: 0.75 (Strong)\nInterpretation: Bond yields and short-term yields tend to move in the same direction, which suggests that both types of bonds respond similarly to changes in interest rates."
  },
  {
    "objectID": "mp04.html#implications-for-retirement-planning",
    "href": "mp04.html#implications-for-retirement-planning",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Implications for Retirement Planning:",
    "text": "Implications for Retirement Planning:\n\nWage Growth and Inflation: The close correlation between wage growth and inflation means that as prices rise, your income should likely increase, helping you maintain purchasing power in retirement.\nEquity Market Performance: The strong correlation between US and international equity returns allows for diversification between these markets without sacrificing performance.\nBonds and Short-Term Yields: Understanding the relationship between bonds and short-term debt yields is crucial for managing fixed-income investments in a rising interest rate environment."
  },
  {
    "objectID": "mp04.html#historical-comparison-of-trs-and-orp",
    "href": "mp04.html#historical-comparison-of-trs-and-orp",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Historical Comparison of TRS and ORP",
    "text": "Historical Comparison of TRS and ORP\n\nTRS vs. ORP: Retirement Comparison\nIn this section, we compare the Teachers Retirement System (TRS) and the Optional Retirement Plan (ORP) for the first month of retirement, assuming:\n\nCareer Timeline: Employee joined CUNY in January 2009 and retired in November 2024.\nStarting Salary: $50,000 in 2009, adjusted annually for historical wage growth (using actual wage growth data).\nFixed Plan Parameters: Contribution rates and retirement benefits remain constant during the employee’s career.\n\n\n\nPlan Overviews\n\nTRS (Teachers Retirement System)\n\nDefined-Benefit Plan:\n\nRetirement income is based on the Final Average Salary (FAS) and years of service.\n\nRetirement Benefit Formula:\n\n( = () ), capped at 30 years of service.\n\nInflation Adjustment:\n\nAnnual increase of 50% of CPI, capped between 1% and 3% annually.\n\n\n\n\nORP (Optional Retirement Plan)\n\nDefined-Contribution Plan:\n\nEmployee and employer contribute to an individual retirement account, invested in a Fidelity Freedom Fund.\n\nContributions:\n\nEmployee: Salary-based contribution (e.g., 3% for salaries ≤$45,000; 3.5% for $45,001–$55,000).\nEmployer: 8% for the first 7 years, then 10% for subsequent years.\n\nInvestment Returns:\n\nUS equities: 7%, International equities: 6%, Bonds: 4%.\n\nRetirement Strategy: Withdraw 4% of accumulated funds annually at retirement.\n\n\n\n\n\nStep-by-Step Analysis\n\n1. Starting Salary\n\nThe employee’s starting salary is $50,000 in January 2009.\nThe salary increases annually based on historical wage growth data.\n\n\n\n2. TRS Benefit Calculation\n\nFinal Average Salary (FAS):\n\nCalculated as the average of the last three years of salary, adjusted for wage growth.\n\nYears of Service:\n\nThe employee works for 15 years (from 2009 to 2024), and their pension benefit is calculated based on 15 years of service.\n\nMonthly Pension:\n\nThe pension benefit is calculated as ( () ) (since the employee works 15 years).\n\nInflation Adjustment:\n\nThe pension benefit will be inflation-adjusted annually based on the CPI. The adjustment will be between 1% and 3% per year, applied each September after retirement.\n\n\n\n\n3. ORP Benefit Calculation\n\nMonthly Contributions:\n\nThe employee and employer contributions are calculated based on the salary brackets (e.g., 3% for salaries ≤$45,000).\nContributions are made monthly throughout the employee’s 15-year career.\n\nInvestment Growth:\n\nThe employee’s contributions grow using the average market returns over time:\n\nUS equities: 7%, International equities: 6%, Bonds: 4%.\n\n\nRetirement Withdrawal:\n\nThe employee can begin withdrawing funds from their ORP account upon retirement.\nA general rule of thumb is to withdraw 4% annually from the accumulated balance at retirement.\n\n\n\nThe following code snippets will calculate the retirement benefits for both TRS and ORP based on the assumptions outlined above. We will compare the monthly benefits and accumulated funds for each plan to provide a comprehensive analysis of the retirement options.\n\n\nShow the code\n# Assuming the dataset 'combined_data' already exists\n# Calculate percentage growth for inflation_rate\ncombined_data$inflation_rate_growth &lt;- c(NA, diff(combined_data$inflation_rate) / head(combined_data$inflation_rate, -1))\n\n# Calculate percentage growth for wage_growth\ncombined_data$wage_growth_growth &lt;- c(NA, diff(combined_data$wage_growth) / head(combined_data$wage_growth, -1))\n\n# Calculate percentage growth for us_equity_close\ncombined_data$us_equity_growth &lt;- c(NA, diff(combined_data$us_equity_close) / head(combined_data$us_equity_close, -1))\n\n# Calculate percentage growth for intl_equity_close\ncombined_data$intl_equity_growth &lt;- c(NA, diff(combined_data$intl_equity_close) / head(combined_data$intl_equity_close, -1))\n\n# For bond_yield and short_term_yield, just divide by 100 to convert to decimals\ncombined_data$bond_yield &lt;- combined_data$bond_yield / 100\ncombined_data$short_term_yield &lt;- combined_data$short_term_yield / 100\n\n# Create a new dataset with only the percentage growth columns\npercentages &lt;- combined_data[, c('date', 'inflation_rate_growth', 'wage_growth_growth', \n                                 'us_equity_growth', 'intl_equity_growth', \n                                 'bond_yield', 'short_term_yield')]\n\n# Drop the first month (first row)\npercentages &lt;- percentages[-1, ]\n\n# Show the results\nprint(percentages)\n\n\n          date inflation_rate_growth wage_growth_growth us_equity_growth\n2   2009-02-01          0.000000e+00       0.0000000000    -3.622939e-03\n3   2009-03-01          1.326105e-04       0.0003568841     0.000000e+00\n4   2009-04-01          2.544201e-03       0.0010810811     6.700512e-02\n5   2009-05-01          3.015006e-03       0.0005399568     5.845355e-02\n6   2009-06-01          5.938682e-03       0.0016189962    -6.737271e-04\n7   2009-07-01         -1.732882e-03       0.0016163793     7.460603e-02\n8   2009-08-01          1.822456e-03       0.0043033889     3.693857e-02\n9   2009-09-01          1.864620e-03       0.0016068559     3.545818e-02\n10  2009-10-01          6.945264e-04       0.0021390374    -1.922586e-02\n11  2009-11-01         -3.261555e-03       0.0032017076     6.160715e-02\n12  2009-12-01         -1.916005e-03       0.0021276596     1.907201e-02\n13  2010-01-01          3.109796e-03       0.0021231423    -3.634293e-02\n14  2010-02-01          8.636795e-04       0.0015889831     3.119505e-02\n15  2010-03-01          2.861308e-03       0.0000000000     6.090499e-02\n16  2010-04-01          1.779819e-03       0.0026441036     1.547038e-02\n17  2010-05-01          9.719623e-04       0.0026371308    -7.945481e-02\n18  2010-06-01         -1.219419e-03       0.0010520779    -5.174291e-02\n19  2010-07-01          8.320258e-04       0.0005254861     6.830035e-02\n20  2010-08-01          8.674790e-04       0.0036764706    -4.498046e-02\n21  2010-09-01         -6.274743e-04       0.0000000000     8.955377e-02\n22  2010-10-01          1.463520e-03       0.0047095761     3.820246e-02\n23  2010-11-01         -1.668862e-04       0.0005208333     0.000000e+00\n24  2010-12-01          1.849588e-03       0.0005205622     6.684401e-02\n25  2011-01-01          4.809056e-03       0.0057232050     2.329955e-02\n26  2011-02-01          5.745040e-03      -0.0010346611     3.473746e-02\n27  2011-03-01          9.477300e-03       0.0000000000     1.074389e-04\n28  2011-04-01          5.645354e-03       0.0025893320     2.896088e-02\n29  2011-05-01          2.980201e-03       0.0025826446    -1.121447e-02\n30  2011-06-01         -1.929843e-03       0.0000000000    -1.688645e-02\n31  2011-07-01         -1.183821e-03       0.0030911901    -2.000474e-02\n32  2011-08-01          1.830513e-03       0.0000000000    -5.497566e-02\n33  2011-09-01          4.053071e-03       0.0010272214    -6.945000e-02\n34  2011-10-01          2.094725e-04       0.0035915854     1.091476e-01\n35  2011-11-01         -1.849953e-03       0.0000000000    -4.063205e-03\n36  2011-12-01         -2.858754e-03       0.0005112474     1.043932e-02\n37  2012-01-01          3.783146e-03       0.0010219724     4.637443e-02\n38  2012-02-01          4.432702e-03       0.0000000000     4.340591e-02\n39  2012-03-01          8.887150e-03       0.0025523226     3.215672e-02\n40  2012-04-01          2.249622e-03       0.0020366599    -6.675855e-03\n41  2012-05-01          2.115574e-03      -0.0005081301    -6.005606e-02\n42  2012-06-01         -1.510386e-03       0.0020335536     4.052887e-02\n43  2012-07-01         -3.472267e-03       0.0015220700     1.182942e-02\n44  2012-08-01          4.778066e-03       0.0000000000     2.505305e-02\n45  2012-09-01          4.643757e-03       0.0025329281     2.535671e-02\n46  2012-10-01          3.772166e-03       0.0000000000    -1.819820e-02\n47  2012-11-01         -7.490445e-03       0.0030318343     5.659034e-03\n48  2012-12-01         -5.047040e-03       0.0030226700     8.998603e-03\n49  2013-01-01          3.146158e-03       0.0015067805     5.119033e-02\n50  2013-02-01          7.887987e-03       0.0025075226     1.275907e-02\n51  2013-03-01          3.904602e-03       0.0005002501     3.797501e-02\n52  2013-04-01         -9.765998e-05       0.0020000000     1.921221e-02\n53  2013-05-01          2.085032e-03       0.0004990020     2.361012e-02\n54  2013-06-01          1.050941e-03       0.0034912718    -1.336030e-02\n55  2013-07-01          4.825867e-04       0.0009940358     5.167636e-02\n56  2013-08-01          1.057794e-03       0.0019860973    -2.999192e-02\n57  2013-09-01          2.345820e-03       0.0024777007     3.168081e-02\n58  2013-10-01         -6.156545e-04       0.0009886307     4.630679e-02\n59  2013-11-01         -3.573840e-03       0.0029629630     2.963831e-02\n60  2013-12-01         -2.413689e-04       0.0019694732     2.589575e-02\n61  2014-01-01          2.587930e-03       0.0019656020    -3.524821e-02\n62  2014-02-01          3.831741e-03       0.0058852379     4.551566e-02\n63  2014-03-01          6.220172e-03      -0.0004875670     8.310573e-03\n64  2014-04-01          2.994663e-03       0.0004878049     6.951211e-03\n65  2014-05-01          6.430144e-03       0.0014627011     2.320638e-02\n66  2014-06-01          1.102134e-03       0.0024342746     2.063342e-02\n67  2014-07-01          9.684789e-04       0.0009713453    -1.343749e-02\n68  2014-08-01         -7.856109e-04       0.0024260068     3.946383e-02\n69  2014-09-01          1.075892e-03       0.0004840271    -1.379126e-02\n70  2014-10-01         -1.116071e-03       0.0014513788     2.355067e-02\n71  2014-11-01         -5.917649e-03       0.0033816425     2.747214e-02\n72  2014-12-01         -4.683207e-03      -0.0028887819    -2.560075e-03\n73  2015-01-01         -3.249754e-03       0.0043457267    -2.962936e-02\n74  2015-02-01          6.000386e-03       0.0019230769     5.620436e-02\n75  2015-03-01          8.100172e-03       0.0033589251    -1.574480e-02\n76  2015-04-01          2.532169e-03       0.0004782401     9.834278e-03\n77  2015-05-01          7.944631e-03       0.0028680688     1.285576e-02\n78  2015-06-01          4.299279e-04       0.0009532888    -2.009986e-02\n79  2015-07-01          2.897697e-03       0.0014285714     2.214741e-02\n80  2015-08-01         -1.236533e-03       0.0033285782    -6.134457e-02\n81  2015-09-01         -1.961289e-03      -0.0004739336    -2.498985e-02\n82  2015-10-01          3.439001e-04       0.0047415837     8.528627e-02\n83  2015-11-01         -2.422843e-03       0.0009438414     3.654735e-03\n84  2015-12-01         -1.292313e-03       0.0009429514    -1.717930e-02\n85  2016-01-01          4.789799e-03       0.0037682525    -4.978633e-02\n86  2016-02-01          9.035159e-04       0.0009385265    -8.261942e-04\n87  2016-03-01          2.381332e-03       0.0028129395     6.724268e-02\n88  2016-04-01          4.828772e-03       0.0032725573     3.941464e-03\n89  2016-05-01          5.134049e-03       0.0004659832     1.701131e-02\n90  2016-06-01          1.504912e-03       0.0023288309     3.497450e-03\n91  2016-07-01          5.921975e-04       0.0027881041     3.647104e-02\n92  2016-08-01          4.952189e-04       0.0009267841     1.197755e-03\n93  2016-09-01          2.961794e-03       0.0013888889     7.784834e-05\n94  2016-10-01          2.660151e-03       0.0036985668    -1.733712e-02\n95  2016-11-01         -1.796740e-03       0.0000000000     3.683851e-02\n96  2016-12-01          2.726019e-04       0.0023030861     2.028320e-02\n97  2017-01-01          5.202071e-03       0.0022977941     1.789483e-02\n98  2017-02-01          5.733332e-03       0.0013755158     3.929117e-02\n99  2017-03-01          2.763110e-03       0.0018315018     1.258233e-03\n100 2017-04-01          3.388035e-03       0.0027422303     9.926250e-03\n101 2017-05-01          2.261570e-03       0.0009115770     1.411303e-02\n102 2017-06-01          3.498703e-04       0.0027322404     6.373931e-03\n103 2017-07-01          9.392107e-04       0.0027247956     2.055445e-02\n104 2017-08-01          2.253561e-03       0.0013586957     2.917835e-03\n105 2017-09-01          4.786863e-03       0.0031659882     2.014068e-02\n106 2017-10-01          2.803075e-03       0.0000000000     2.356420e-02\n107 2017-11-01         -3.771047e-04       0.0018034265     3.056581e-02\n108 2017-12-01          8.595008e-04       0.0036003600     1.208662e-02\n109 2018-01-01          5.016573e-03       0.0035874439     5.635903e-02\n110 2018-02-01          5.211918e-03       0.0017873101    -3.636019e-02\n111 2018-03-01          3.877132e-03       0.0035682426    -2.741576e-02\n112 2018-04-01          3.996260e-03       0.0017777778     5.168510e-03\n113 2018-05-01          4.590955e-03       0.0022182786     2.430905e-02\n114 2018-06-01          1.869017e-03       0.0030987162     5.754926e-03\n115 2018-07-01          9.062230e-04       0.0017652251     3.704661e-02\n116 2018-08-01          1.606237e-03       0.0035242291     3.192012e-02\n117 2018-09-01          2.685376e-03       0.0030728709     5.948180e-03\n118 2018-10-01          4.111578e-03       0.0021881838    -6.910418e-02\n119 2018-11-01         -2.017318e-03       0.0043668122     1.854904e-02\n120 2018-12-01         -1.690143e-03       0.0047826087    -8.793628e-02\n121 2019-01-01          1.564804e-03       0.0012981393     8.006564e-02\n122 2019-02-01          2.224950e-03       0.0030250648     3.241594e-02\n123 2019-03-01          4.338599e-03       0.0043084877     1.807872e-02\n124 2019-04-01          8.048771e-03       0.0008580009     4.085267e-02\n125 2019-05-01          5.038550e-03       0.0021431633    -6.377119e-02\n126 2019-06-01          2.842587e-04       0.0038494440     6.959247e-02\n127 2019-07-01          2.657248e-04       0.0034086067     1.511945e-02\n128 2019-08-01          8.670659e-04       0.0029723992    -1.674358e-02\n129 2019-09-01          3.089241e-03       0.0025402202     1.947915e-02\n130 2019-10-01          5.225981e-03       0.0029560811     2.210466e-02\n131 2019-11-01         -1.447771e-03       0.0037894737     3.619830e-02\n132 2019-12-01         -1.991740e-03      -0.0004194631     2.903357e-02\n133 2020-01-01          2.773457e-03       0.0033571129    -4.040608e-04\n134 2020-02-01          3.921856e-03       0.0041823505    -7.916574e-02\n135 2020-03-01         -1.519613e-03       0.0058309038    -1.246427e-01\n136 2020-04-01         -3.948977e-03       0.0409937888     1.269836e-01\n137 2020-05-01          5.459615e-04      -0.0059665871     4.764532e-02\n138 2020-06-01          4.002754e-03      -0.0080032013     1.776075e-02\n139 2020-07-01          5.259798e-03      -0.0040338846     5.889230e-02\n140 2020-08-01          3.069700e-03       0.0052652896     6.979666e-02\n141 2020-09-01         -7.596503e-05      -0.0004029009    -3.739838e-02\n142 2020-10-01          1.642416e-03       0.0016122531    -2.493329e-02\n143 2020-11-01         -3.611725e-06       0.0032193159     1.087768e-01\n144 2020-12-01         -1.018510e-03       0.0092258323     3.706605e-02\n145 2021-01-01          2.331946e-03       0.0003974563    -1.019065e-02\n146 2021-02-01          5.280661e-03       0.0031783870     2.780552e-02\n147 2021-03-01          6.899843e-03       0.0035643564     4.540528e-02\n148 2021-04-01          1.026993e-02       0.0067087609     5.291030e-02\n149 2021-05-01          8.063293e-03       0.0066640533     6.566029e-03\n150 2021-06-01          8.642619e-03       0.0050623053     2.247274e-02\n151 2021-07-01          5.550487e-03       0.0058117009     2.441247e-02\n152 2021-08-01          1.828450e-03       0.0061633282     2.975980e-02\n153 2021-09-01          2.272782e-03       0.0068912711    -4.657523e-02\n154 2021-10-01          8.053516e-03       0.0060836502     7.016348e-02\n155 2021-11-01          5.415870e-03       0.0041572184    -8.034782e-03\n156 2021-12-01          3.783230e-03       0.0071509221     4.628837e-02\n157 2022-01-01          8.790890e-03       0.0052316891    -5.274129e-02\n158 2022-02-01          8.212116e-03       0.0026022305    -2.951712e-02\n159 2022-03-01          1.302971e-02       0.0051909529     3.755569e-02\n160 2022-04-01          6.762116e-03       0.0059018812    -8.776903e-02\n161 2022-05-01          8.139478e-03       0.0044004400     2.257255e-03\n162 2022-06-01          1.243682e-02       0.0051113545    -8.246930e-02\n163 2022-07-01          1.451374e-03       0.0047221213     9.208753e-02\n164 2022-08-01          1.974830e-04       0.0032537961    -4.080203e-02\n165 2022-09-01          3.442533e-03       0.0039639640    -9.241634e-02\n166 2022-10-01          6.997912e-03       0.0050251256     8.127562e-02\n167 2022-11-01         -4.349210e-03       0.0046428571     5.559131e-02\n168 2022-12-01         -4.178286e-03       0.0035549236    -5.757565e-02\n169 2023-01-01          9.148154e-03       0.0028338647     6.288713e-02\n170 2023-02-01          5.206676e-03       0.0038855528    -2.514253e-02\n171 2023-03-01          4.966628e-03       0.0056298381     3.712489e-02\n172 2023-04-01          4.589745e-03       0.0034989503     1.597508e-02\n173 2023-05-01          4.152868e-03       0.0038354254     4.616008e-03\n174 2023-06-01          2.852948e-03       0.0038207711     6.481415e-02\n175 2023-07-01          1.285260e-03       0.0044982699     3.273340e-02\n176 2023-08-01          4.214058e-03       0.0020668274    -1.625211e-02\n177 2023-09-01          3.853063e-03       0.0030938467    -4.739330e-02\n178 2023-10-01          6.626136e-04       0.0037697053    -2.170851e-02\n179 2023-11-01         -4.299559e-03       0.0044383749     9.134376e-02\n180 2023-12-01         -1.124735e-03       0.0030591434     4.566261e-02\n181 2024-01-01          6.507492e-03       0.0044052863     1.592633e-02\n182 2024-02-01          3.920098e-03       0.0020242915     5.218702e-02\n183 2024-03-01          8.693170e-03       0.0030303030     3.272381e-02\n184 2024-04-01          5.562880e-03       0.0013427325    -4.031963e-02\n185 2024-05-01          6.525071e-04       0.0040227958     5.057962e-02\n186 2024-06-01         -1.812827e-03       0.0040066778     3.528476e-02\n187 2024-07-01         -1.462558e-03       0.0029930163     1.210904e-02\n188 2024-08-01          8.043845e-04       0.0033156499     2.336561e-02\n189 2024-09-01          2.468195e-03       0.0003635162     1.308794e-03\n190 2024-10-01          8.885166e-06       0.0000000000     0.000000e+00\n    intl_equity_growth  bond_yield short_term_yield\n2        -0.0027563603 0.028700000         0.009800\n3         0.0000000000 0.028195455         0.009300\n4         0.0932066341 0.029271429         0.009300\n5         0.1305881555 0.032930000         0.009300\n6        -0.0182189106 0.037218182         0.011800\n7         0.1057415577 0.035622727         0.010200\n8         0.0449657820 0.035871429         0.011200\n9         0.0393908649 0.034019048         0.009600\n10       -0.0276972044 0.033876190         0.009500\n11        0.0410806387 0.034026316         0.008000\n12        0.0078699853 0.035900000         0.008700\n13       -0.0543831169 0.037331579         0.009300\n14        0.0074184190 0.036910526         0.008600\n15        0.0629563694 0.037273913         0.009600\n16       -0.0277329966 0.038468182         0.010600\n17       -0.1152707091 0.034200000         0.008300\n18       -0.0181323144 0.032040909         0.007200\n19        0.1166246918 0.030114286         0.006200\n20       -0.0358355373 0.026986364         0.005200\n21        0.0965703704 0.026476190         0.004800\n22        0.0405545600 0.025400000         0.003800\n23       -0.0489958695 0.027630000         0.004500\n24        0.0843783045 0.032909091         0.006200\n25        0.0215750859 0.033940000         0.006100\n26        0.0365564878 0.035763158         0.007700\n27       -0.0250226275 0.034143478         0.007000\n28        0.0619142209 0.034550000         0.007300\n29       -0.0270048444 0.031685714         0.005600\n30       -0.0121940215 0.030022727         0.004100\n31       -0.0238947297 0.030030000         0.004100\n32       -0.0858253138 0.023030435         0.002300\n33       -0.1132985850 0.019752381         0.002100\n34        0.0975759127 0.021520000         0.002800\n35       -0.0208646814 0.020135000         0.002500\n36       -0.0212485695 0.019780952         0.002600\n37        0.0561526420 0.019665000         0.002400\n38        0.0488427579 0.019675000         0.002800\n39        0.0032742955 0.021727273         0.003400\n40       -0.0238028780 0.020528571         0.002900\n41       -0.1116800394 0.018031818         0.002900\n42        0.0698078637 0.016223810         0.002900\n43        0.0025319737 0.015266667         0.002500\n44        0.0328602297 0.016782609         0.002700\n45        0.0263521397 0.017231579         0.002600\n46        0.0106447848 0.017461905         0.002800\n47        0.0270815377 0.016540000         0.002700\n48        0.0423541062 0.017190000         0.002600\n49        0.0383184793 0.019147619         0.002700\n50       -0.0114795473 0.019842105         0.002700\n51        0.0118795494 0.019575000         0.002600\n52        0.0521540280 0.017590909         0.002300\n53       -0.0297390785 0.019281818         0.002500\n54       -0.0276929853 0.023000000         0.003300\n55        0.0519652940 0.025822727         0.003400\n56       -0.0160206680 0.027372727         0.003600\n57        0.0784990531 0.028095000         0.004000\n58        0.0320887039 0.026159091         0.003400\n59        0.0066086237 0.027184211         0.003000\n60        0.0189550163 0.029019048         0.003400\n61       -0.0520636116 0.028580952         0.003900\n62        0.0594788283 0.027094737         0.003300\n63       -0.0036551343 0.027233333         0.004000\n64        0.0157506315 0.027052381         0.004200\n65        0.0176519350 0.025590476         0.003900\n66        0.0104266160 0.025985714         0.004500\n67       -0.0237141529 0.025422727         0.005100\n68        0.0026456568 0.024200000         0.004700\n69       -0.0415534921 0.025342857         0.005700\n70       -0.0035247457 0.023040909         0.004500\n71        0.0000000000 0.023255556         0.005300\n72       -0.0377257679 0.022072727         0.006400\n73        0.0071268674 0.018815000         0.005500\n74        0.0615977808 0.019752632         0.006200\n75       -0.0121582445 0.020427273         0.006400\n76        0.0386638673 0.019350000         0.005400\n77       -0.0004837278 0.021975000         0.006100\n78       -0.0290986129 0.023636364         0.006900\n79        0.0146304910 0.023245455         0.006700\n80       -0.0723351369 0.021671429         0.007000\n81       -0.0408413064 0.021728571         0.007100\n82        0.0673387377 0.020700000         0.006400\n83       -0.0076224996 0.022631579         0.008800\n84       -0.0213757534 0.022427273         0.009800\n85       -0.0552804058 0.020852632         0.009000\n86       -0.0308463805 0.017795000         0.007300\n87        0.0719591601 0.018890909         0.008800\n88        0.0231321906 0.018052381         0.007700\n89       -0.0029965257 0.018061905         0.008200\n90       -0.0204044960 0.016440909         0.007300\n91        0.0415691014 0.015040000         0.006700\n92        0.0040749233 0.015565217         0.007400\n93        0.0164811732 0.016304762         0.007700\n94       -0.0240603895 0.017645000         0.008400\n95       -0.0150639481 0.021420000         0.009800\n96        0.0245096659 0.024919048         0.012000\n97        0.0366734810 0.024325000         0.012100\n98        0.0105591441 0.024189474         0.012000\n99        0.0305724546 0.024817391         0.013100\n100       0.0221393407 0.022973684         0.012400\n101       0.0341039995 0.023036364         0.013000\n102       0.0063826969 0.021868182         0.013400\n103       0.0288002441 0.023180000         0.013700\n104       0.0002355370 0.022091304         0.013400\n105       0.0251529176 0.022020000         0.013800\n106       0.0177382397 0.023600000         0.015500\n107       0.0086016891 0.023533333         0.017000\n108       0.0164986049 0.024025000         0.018400\n109       0.0474803094 0.025838095         0.020300\n110      -0.0510752130 0.028589474         0.021800\n111      -0.0039756430 0.028423810         0.022800\n112       0.0126553437 0.028690476         0.023800\n113      -0.0142809509 0.029777273         0.025100\n114      -0.0166577286 0.029123810         0.025300\n115       0.0228438463 0.028890476         0.026100\n116      -0.0173217108 0.028891304         0.026400\n117       0.0071175962 0.030042105         0.027700\n118      -0.0859734382 0.031522727         0.028600\n119       0.0050569626 0.031170000         0.028600\n120      -0.0566730024 0.028326316         0.026800\n121       0.0746636485 0.027138095         0.025400\n122       0.0233239092 0.026763158         0.025000\n123       0.0060213196 0.025709524         0.024100\n124       0.0283818130 0.025323810         0.023400\n125      -0.0521062469 0.023950000         0.022100\n126       0.0585859900 0.020740000         0.018100\n127      -0.0203803513 0.020590909         0.018400\n128      -0.0188430751 0.016263636         0.015700\n129       0.0315617900 0.016995000         0.016500\n130       0.0321311383 0.017068182         0.015500\n131       0.0134451076 0.018121053         0.016100\n132       0.0356535085 0.018628571         0.016100\n133      -0.0299593626 0.017576190         0.015200\n134      -0.0765107291 0.015042105         0.013300\n135      -0.1514575449 0.008700000         0.004500\n136       0.0701857597 0.006576190         0.002200\n137       0.0557741293 0.006740000         0.001700\n138       0.0349218074 0.007286364         0.001900\n139       0.0262962378 0.006567810         0.001500\n140       0.0517465457 0.006567810         0.001400\n141      -0.0178743700 0.006795238         0.001300\n142      -0.0354529501 0.007871429         0.001500\n143       0.1429678896 0.008700000         0.001700\n144       0.0563980248 0.009336364         0.001400\n145      -0.0072023066 0.010810526         0.001300\n146       0.0243242405 0.012578947         0.001300\n147       0.0276861430 0.016108696         0.001500\n148       0.0305424325 0.016350000         0.001600\n149       0.0357635444 0.016210000         0.001600\n150      -0.0093298361 0.015190909         0.002000\n151       0.0050464990 0.013185714         0.002200\n152       0.0131330842 0.012831818         0.002200\n153      -0.0337850783 0.013747619         0.002400\n154       0.0322840719 0.015825000         0.003900\n155      -0.0464303355 0.015595000         0.005100\n156       0.0433695041 0.014650000         0.006800\n157      -0.0385837764 0.017640000         0.009800\n158      -0.0264811807 0.019342105         0.014400\n159       0.0067595749 0.021278261         0.019100\n160      -0.0678736598 0.027475000         0.025400\n161       0.0165294247 0.028980952         0.026200\n162      -0.0918798951 0.031433333         0.030000\n163       0.0529403453 0.028960000         0.030400\n164      -0.0581936292 0.028978261         0.032500\n165      -0.0986197197 0.035190476         0.038600\n166       0.0607805484 0.039835000         0.043800\n167       0.1254881074 0.038910000         0.045000\n168      -0.0218123685 0.036161905         0.042900\n169       0.0903029483 0.035315000         0.042100\n170      -0.0347470642 0.037468421         0.045300\n171       0.0264208375 0.036630435         0.043000\n172       0.0263440623 0.034600000         0.040200\n173      -0.0373159637 0.035736364         0.041300\n174       0.0444176712 0.037480952         0.046400\n175       0.0313978485 0.038995000         0.048300\n176      -0.0394702131 0.041678261         0.049000\n177      -0.0378259123 0.043790000         0.049132\n178      -0.0338518445 0.045068429         0.049132\n179       0.0880685425 0.045028571         0.048800\n180       0.0558316971 0.040200000         0.044600\n181      -0.0108564202 0.040580952         0.043200\n182       0.0274389392 0.042075000         0.045400\n183       0.0365792704 0.042085000         0.045900\n184      -0.0340839853 0.045068429         0.048700\n185       0.0466346676 0.044822727         0.048600\n186      -0.0164860275 0.043052632         0.047400\n187       0.0301503561 0.042486364         0.045000\n188       0.0031978042 0.038709091         0.039700\n189       0.0000000000 0.037235000         0.036200\n190      -0.0163210017 0.040954545         0.039700\n\n\n\n\nShow the code\n# Assuming the percentages data exists and starting salary is $50,000\nstarting_salary &lt;- 50000\nn_months &lt;- nrow(percentages)  # Total number of months\n\n# Create a vector to store the salary for each month\nsalary &lt;- numeric(n_months)\nsalary[1] &lt;- starting_salary\n\n# Calculate salary growth over the months based on wage_growth_growth\nfor (i in 2:n_months) {\n  salary[i] &lt;- salary[i - 1] * (1 + percentages$wage_growth_growth[i - 1])\n}\n\n# Employee contribution percentages based on salary brackets\nemployee_contribution &lt;- numeric(n_months)\nfor (i in 1:n_months) {\n  if (salary[i] &lt;= 45000) {\n    employee_contribution[i] &lt;- 0.03\n  } else if (salary[i] &lt;= 55000) {\n    employee_contribution[i] &lt;- 0.035\n  } else if (salary[i] &lt;= 75000) {\n    employee_contribution[i] &lt;- 0.045\n  } else if (salary[i] &lt;= 100000) {\n    employee_contribution[i] &lt;- 0.0575\n  } else {\n    employee_contribution[i] &lt;- 0.06\n  }\n}\n\n# Employer contributions: 8% for the first 7 years, 10% thereafter\nemployer_contribution &lt;- numeric(n_months)\nyears_of_service &lt;- floor((1:n_months) / 12) + 1  # Convert months to years\n\nfor (i in 1:n_months) {\n  if (years_of_service[i] &lt;= 7) {\n    employer_contribution[i] &lt;- 0.08\n  } else {\n    employer_contribution[i] &lt;- 0.10\n  }\n}\n\n# Calculate monthly contributions for ORP (employee + employer)\nmonthly_contribution_orp &lt;- salary * (employee_contribution + employer_contribution)\n\n# Calculate compounded ORP balance\norp_balance &lt;- numeric(n_months)\norp_balance[1] &lt;- 0  # Assume no balance at the start\n\nfor (i in 2:n_months) {\n  # Growth is applied according to equity, international equity, and bond growth\n  equity_growth &lt;- percentages$us_equity_growth[i - 1] * 0.47 + percentages$intl_equity_growth[i - 1] * 0.32 + percentages$bond_yield[i - 1] * 0.21\n  orp_balance[i] &lt;- orp_balance[i - 1] * (1 + equity_growth) + monthly_contribution_orp[i]\n}\n\n# Final ORP balance at retirement (end of the last month)\nfinal_orp_balance &lt;- orp_balance[n_months]\n\n# Calculate 4% annual withdrawal rate for the first month of retirement\norp_monthly_withdrawal &lt;- final_orp_balance * 0.04 / 12\n\n# Calculate TRS pension benefit: Final Average Salary (FAS) based on last 3 years\nfas &lt;- mean(salary[(n_months - 35):n_months])  # Last 36 months (3 years)\nyears_served &lt;- n_months / 12  # Years of service\ntrs_benefit &lt;- fas * 0.025 * years_served  # Assuming 2.5% per year\n\n# Monthly TRS benefit\ntrs_monthly_benefit &lt;- trs_benefit / 12\n\n# Output results\ncat(\"TRS Monthly Benefit at Retirement (First Month): $\", round(trs_monthly_benefit, 2), \"\\n\")\n\n\nTRS Monthly Benefit at Retirement (First Month): $ 2531.95 \n\n\nShow the code\ncat(\"ORP Monthly Withdrawal (First Month): $\", round(orp_monthly_withdrawal, 2), \"\\n\")\n\n\nORP Monthly Withdrawal (First Month): $ 17529.87"
  },
  {
    "objectID": "mp04.html#fixed-rate-analysis",
    "href": "mp04.html#fixed-rate-analysis",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Fixed-Rate Analysis",
    "text": "Fixed-Rate Analysis\nTo modify the simulation and project an employee’s pension benefit (TRS) or withdrawal amount (ORP) from retirement until death, we need to consider several key factors:\nEstimated Death Age: We’ll assume a reasonable life expectancy for the employee. Cost-of-Living Adjustments (COLA) for TRS: We’ll apply annual inflation adjustments to the TRS benefit, which increase by 50% of the CPI each year, capped at 3%. ORP Market Returns: We’ll assume long-run average returns for the ORP portfolio, based on historical equity, bond, and international equity returns. Projecting Retirement Income: We will simulate the monthly income over time, considering the employee’s longevity and both TRS and ORP scenarios.\n\n\nShow the code\n# Parameters\ndeath_age &lt;- 85  # Assumed age of death\nstart_age &lt;- 50  # Starting age of the employee\ncurrent_age &lt;- start_age + 1  # Employee starts at 51 after the first year\n\n# Time to simulate (from retirement to death)\nyears_of_retirement &lt;- death_age - (start_age + (n_months / 12))  # Years from retirement to death\ntotal_months_in_retirement &lt;- years_of_retirement * 12  # Total months in retirement\n\n# Initialize variables\nmonthly_income_trs &lt;- numeric(total_months_in_retirement)\nmonthly_income_orp &lt;- numeric(total_months_in_retirement)\norp_balance_retirement &lt;- final_orp_balance  # Starting ORP balance at retirement\ntrs_benefit_retirement &lt;- trs_monthly_benefit  # Starting TRS benefit at retirement\n\n# TRS: Apply COLA (cost of living adjustment) each year, capped at 3%\nfor (i in 1:total_months_in_retirement) {\n  if (i %% 12 == 0) {  # At the start of each year\n    inflation_rate &lt;- percentages$inflation_rate_growth[i %% n_months]  # Using inflation data from percentages\n    inflation_adjustment &lt;- min(0.03, inflation_rate * 0.5)  # Apply COLA, capped at 3%\n    trs_benefit_retirement &lt;- trs_benefit_retirement * (1 + inflation_adjustment)\n  }\n  monthly_income_trs[i] &lt;- trs_benefit_retirement  # Monthly TRS income\n}\n\n# ORP: Apply 4% annual withdrawal rate and simulate market returns\nfor (i in 1:total_months_in_retirement) {\n  # Apply monthly growth from historical returns (average returns assumed for simplicity)\n  average_market_growth &lt;- mean(c(percentages$us_equity_growth, percentages$intl_equity_growth, percentages$bond_yield), na.rm = TRUE)\n  orp_balance_retirement &lt;- orp_balance_retirement * (1 + average_market_growth / 12)  # Monthly compounded growth\n  orp_monthly_withdrawal &lt;- orp_balance_retirement * 0.04 / 12  # 4% annual withdrawal rate\n  orp_balance_retirement &lt;- orp_balance_retirement - orp_monthly_withdrawal  # Subtract withdrawal\n  \n  monthly_income_orp[i] &lt;- orp_monthly_withdrawal  # Monthly ORP income\n  \n  # Stop the ORP withdrawals if the balance runs out\n  if (orp_balance_retirement &lt;= 0) {\n    monthly_income_orp[i] &lt;- 0\n    break\n  }\n}\n\n# Calculate the maximum and minimum income gap between TRS and ORP\nincome_gap &lt;- monthly_income_trs - monthly_income_orp\nmax_gap &lt;- max(income_gap, na.rm = TRUE)\nmin_gap &lt;- min(income_gap, na.rm = TRUE)\n\n# Calculate average monthly income for both TRS and ORP\navg_income_trs &lt;- mean(monthly_income_trs, na.rm = TRUE)\navg_income_orp &lt;- mean(monthly_income_orp, na.rm = TRUE)\n\n# Determine if ORP funds run out before death or if there are funds left for heirs\nif (orp_balance_retirement &lt;= 0) {\n  run_out_of_funds &lt;- TRUE\n  remaining_funds &lt;- 0\n} else {\n  run_out_of_funds &lt;- FALSE\n  remaining_funds &lt;- orp_balance_retirement\n}\n\n# Output results\ncat(\"Average Monthly Income - TRS: $\", round(avg_income_trs, 2), \"\\n\")\n\n\nAverage Monthly Income - TRS: $ 2573.32 \n\n\nShow the code\ncat(\"Average Monthly Income - ORP: $\", round(avg_income_orp, 2), \"\\n\")\n\n\nAverage Monthly Income - ORP: $ 13956.3 \n\n\nShow the code\ncat(\"Maximum Monthly Income Gap (TRS vs ORP): $\", round(max_gap, 2), \"\\n\")\n\n\nMaximum Monthly Income Gap (TRS vs ORP): $ -8261.52 \n\n\nShow the code\ncat(\"Minimum Monthly Income Gap (TRS vs ORP): $\", round(min_gap, 2), \"\\n\")\n\n\nMinimum Monthly Income Gap (TRS vs ORP): $ -15020.06 \n\n\nShow the code\ncat(\"TRS Monthly Benefit at Retirement (First Month): $\", round(trs_monthly_benefit, 2), \"\\n\")\n\n\nTRS Monthly Benefit at Retirement (First Month): $ 2531.95 \n\n\nShow the code\ncat(\"ORP Monthly Withdrawal at Retirement (First Month): $\", round(orp_monthly_withdrawal, 2), \"\\n\")\n\n\nORP Monthly Withdrawal at Retirement (First Month): $ 10886.85 \n\n\nShow the code\nif (run_out_of_funds) {\n  cat(\"ORP funds run out before death. There are no funds left for heirs.\\n\")\n} else {\n  cat(\"ORP funds last until death. Remaining funds for heirs: $\", round(remaining_funds, 2), \"\\n\")\n}\n\n\nORP funds last until death. Remaining funds for heirs: $ 3255167"
  }
]