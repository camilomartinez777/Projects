[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Camilo Martinez",
    "section": "",
    "text": "Business Analytics Master’s Student | Data Enthusiast | Finance Passionate\nI am driven by a passion for learning and a deep curiosity about data. I believe numbers offer the clearest path to understanding reality, which is why I advocate for learning through data-driven insights. Originally from Colombia, I now live in New York, where I am pursuing a Master’s in Business Analytics. My focus lies in leveraging analytics to solve complex problems, particularly within the finance industry. I am excited to continue learning and growing in this field, and I am always looking for new opportunities to apply my skills and knowledge."
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "",
    "text": "Welcome to an exciting exploration of the film industry! In this project, we dive deep into what makes movies successful, analyzing IMDb ratings, audience engagement, and revenue to create a predictive model that can foresee a film’s potential. We’ve uncovered fascinating trends in how genres evolve over time, providing valuable insights for future productions.\nFinally, we present our proposed remake of the iconic North by Northwest, a thrilling opportunity to bring a timeless classic to life for modern audiences. With its strong foundation and fresh vision, we believe this project holds the key to the next big success in cinema!"
  },
  {
    "objectID": "mp02.html#introduction",
    "href": "mp02.html#introduction",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "",
    "text": "Welcome to an exciting exploration of the film industry! In this project, we dive deep into what makes movies successful, analyzing IMDb ratings, audience engagement, and revenue to create a predictive model that can foresee a film’s potential. We’ve uncovered fascinating trends in how genres evolve over time, providing valuable insights for future productions.\nFinally, we present our proposed remake of the iconic North by Northwest, a thrilling opportunity to bring a timeless classic to life for modern audiences. With its strong foundation and fresh vision, we believe this project holds the key to the next big success in cinema!"
  },
  {
    "objectID": "mp02.html#data-source",
    "href": "mp02.html#data-source",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Data Source",
    "text": "Data Source\nFor this project, we utilize data from the Internet Movie Database (IMDb), a widely recognized source of comprehensive movie information. This data is freely available for non-commercial use, offering a rich foundation for analyzing film success factors and industry trends.\nTo download the IMDb datasets into R and begin your own analysis, you can use the following code:\n\nget_imdb_file &lt;- function(fname){\n    BASE_URL &lt;- \"https://datasets.imdbws.com/\"\n    fname_ext &lt;- paste0(fname, \".tsv.gz\")\n    if(!file.exists(fname_ext)){\n        FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n        download.file(FILE_URL, \n                      destfile = fname_ext)\n    }\n    as.data.frame(readr::read_tsv(fname_ext, lazy=FALSE))\n}\n\nNAME_BASICS &lt;- get_imdb_file(\"name.basics\")\n\nFor readability, we change the name of the following tables:\n\nTITLE_BASICS     &lt;- get_imdb_file(\"title.basics\")\nTITLE_EPISODES   &lt;- get_imdb_file(\"title.episode\")\nTITLE_RATINGS    &lt;- get_imdb_file(\"title.ratings\")\nTITLE_CREW       &lt;- get_imdb_file(\"title.crew\")\nTITLE_PRINCIPALS &lt;- get_imdb_file(\"title.principals\")\n\nDue to the large size of these files, loading them may take some time. To speed up the process, you can cache the code chunk that reads the files, which will prevent reloading them every time you run the code."
  },
  {
    "objectID": "mp02.html#data-sub-sampling",
    "href": "mp02.html#data-sub-sampling",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Data Sub-Sampling",
    "text": "Data Sub-Sampling\nSince the data set contains a vast number of data points, we will narrow it down to ensure smooth analysis. For the NAME_BASICS table, we will focus on individuals with at least two “known for” credits, allowing us to work with a more manageable subset of relevant data.\n\nlibrary(dplyr)\nlibrary(stringr)\n\nNAME_BASICS &lt;- NAME_BASICS |&gt; \n    filter(str_count(knownForTitles, \",\") &gt; 1)\n\nIMDb has a long tail of obscure movies:\n\nlibrary(ggplot2)\nlibrary(scales)\n\nTITLE_RATINGS |&gt;\n    ggplot(aes(x=numVotes)) + \n    geom_histogram(bins=30) +\n    xlab(\"Number of IMDB Ratings\") +\n    ylab(\"Number of Titles\") + \n    ggtitle(\"Majority of IMDB Titles Have Less than 100 Ratings\") + \n    theme_bw() + \n    scale_x_log10(label=scales::comma) + \n    scale_y_continuous(label=scales::comma)\n\n\n\n\n\n\n\n\nAs we can see, the majority of titles have fewer than 100 ratings. Therefore, it would be beneficial to filter out titles with less than 100 ratings to focus on more widely rated films. This becomes even clearer in the following visualization:\n\nTITLE_RATINGS |&gt;\n    pull(numVotes) |&gt;\n    quantile()\n\n     0%     25%     50%     75%    100% \n      5      11      26     100 2954275 \n\n\nBy applying this drop, we significantly reduce the size of our data set:\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n    filter(numVotes &gt;= 100)\n\nNext, we will join the tables to filter out titles with fewer than 100 ratings. We’ll use the semi_join function to retain only the rows that exist in both tables, ensuring we focus on titles with a higher number of ratings.\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_EPISODES_1 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\nTITLE_EPISODES_2 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(parentTconst == tconst))\n\nTITLE_EPISODES &lt;- bind_rows(TITLE_EPISODES_1,\n                            TITLE_EPISODES_2) |&gt;\n    distinct()\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n    semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\n\nrm(TITLE_EPISODES_1)\nrm(TITLE_EPISODES_2)"
  },
  {
    "objectID": "mp02.html#taking-a-look-at-the-data",
    "href": "mp02.html#taking-a-look-at-the-data",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Taking a Look at the Data",
    "text": "Taking a Look at the Data\nWe will first inspect the data to determine if any changes to the data types are necessary. In this dataset, most columns are being read as character types. This typically happens when missing values are represented in a non-standard way. For example, in these files, missing values are represented as \\N. Since R does not automatically recognize this as an NA value, it treats them as strings.\nTo address this, we need to:\n\nUse the mutate function to modify the columns.\nApply the as.numeric function to convert columns to the correct data type.\n\nWe can clean the datasets using the following code:\n\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n    mutate_if(is.character, ~na_if(., \"\\\\N\")) |&gt;\n    mutate(birthYear = as.numeric(birthYear),\n           deathYear = as.numeric(deathYear))\n\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n    mutate_if(is.character, ~na_if(., \"\\\\N\")) |&gt;\n    mutate(\n        startYear = as.numeric(startYear), \n        endYear = as.numeric(endYear),    \n        runtimeMinutes = as.numeric(runtimeMinutes)  \n    )\n\n\nTITLE_EPISODES &lt;- TITLE_EPISODES |&gt;\n    mutate_if(is.character, ~na_if(., \"\\\\N\")) |&gt;\n    mutate(\n        seasonNumber = as.numeric(seasonNumber),\n        episodeNumber = as.numeric(episodeNumber)\n    )\n\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n    mutate_if(is.character, ~na_if(., \"\\\\N\")) |&gt;\n    mutate(\n        averageRating = as.numeric(averageRating), \n        numVotes = as.numeric(numVotes)  \n    )\n\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n    mutate_if(is.character, ~na_if(., \"\\\\N\"))\n\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n    mutate_if(is.character, ~na_if(., \"\\\\N\")) |&gt;\n    mutate(characters = str_replace_all(characters, '\\\\[|\\\\\"|\\\\]', '')) #clean undesired characters\n\nDuring the cleaning process, we transformed string columns into numeric columns and cleaned the character columns in the TITLE_PRINCIPALS table. Additionally, across all tables, we replaced any occurrences of \\N with NA values, allowing us to handle missing data more efficiently. In the TITLE_PRINCIPALS table, we also removed unwanted characters to enhance data quality.\nBy cleaning and preparing the data, we now have a clearer view of the relationships between tables and the variables at our disposal."
  },
  {
    "objectID": "mp02.html#data-analysis",
    "href": "mp02.html#data-analysis",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Data Analysis",
    "text": "Data Analysis\nWith the data now properly cleaned and filtered, we can move forward with our analysis. To begin, we will address the following key questions:\n\nHow many movies are in our dataset?\nHow many TV series?\nHow many TV episodes?\n\nAnswering these questions will help us gain an overview of the data set and set the foundation for deeper insights.\n\n# Install and load the gt package\nif(!require(gt)) {\n  install.packages(\"gt\")\n}\nlibrary(gt)\n\n# Get the count of movies, TV series, and TV episodes\nmovie &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"movie\") |&gt;\n  nrow()\n\ntvSeries &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"tvSeries\") |&gt;\n  nrow()\n\ntvEpisode &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"tvEpisode\") |&gt;\n  nrow()\n\n# Create a summary table and apply gt\nsummary_table &lt;- tibble(\n  Title_Type = c(\"Movies\", \"TV Series\", \"TV Episode\"),\n  Count = c(movie, tvSeries, tvEpisode)\n) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Number of Movies, TV Series, and TV Episodes\"\n  )\nsummary_table\n\n\n\n\n\n\n\nNumber of Movies, TV Series, and TV Episodes\n\n\nTitle_Type\nCount\n\n\n\n\nMovies\n100562\n\n\nTV Series\n20186\n\n\nTV Episode\n103647\n\n\n\n\n\n\n\nWe observed a notable trend in our dataset: the number of movies and TV episodes is quite similar, while TV series have the lowest count among the three categories. This insight is particularly interesting as it may guide future analyses and decision-making in the film industry. Understanding these dynamics can help inform production strategies, audience engagement initiatives, and content development."
  },
  {
    "objectID": "mp02.html#data-exploration-oldest-living-person",
    "href": "mp02.html#data-exploration-oldest-living-person",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Data Exploration: Oldest Living Person",
    "text": "Data Exploration: Oldest Living Person\nTo identify the oldest living person in our dataset, we can focus on the NAME_BASICS table, filtering by individuals who are still alive and sorting by birth year. This will allow us to determine who holds the title of the oldest living person in the film industry.\n\nlibrary(dplyr)\nlibrary(DT)  # Use DT for pagination\n\noldest_person &lt;- NAME_BASICS |&gt;         \n  filter(is.na(deathYear)) |&gt;            # Keep only people with NA in deathYear (still alive)\n  mutate(age = 2024 - birthYear) |&gt;      # Calculate their age in 2024\n  filter(age &gt; 110) |&gt; \n  filter(age &lt; 116)  # Filter for ages below 117 year  # Find the person with the maximum age\n\nsummary_table &lt;- oldest_person |&gt; \n  select(Name = primaryName, \"Year of Birth\" = birthYear, Age = age)\n\ndatatable(summary_table, \n          options = list(pageLength = 5),   # Limit to 5 rows per page\n          caption = 'Oldest Person Alive in this Data Set')\n\n\n\n\n\nInitially, we compiled a list of individuals who were 117 years old based on the dataset. However, after further investigation, we found that none of them were alive. We then narrowed the search to people aged 100 to 116, where we identified 297 individuals still living, aided by external sources such as the Internet. Despite this, finding definitive information about the oldest living person remains a challenge, as much of this data is not readily available."
  },
  {
    "objectID": "mp02.html#identifying-a-perfectly-rated-tv-episode",
    "href": "mp02.html#identifying-a-perfectly-rated-tv-episode",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Identifying a Perfectly Rated TV Episode",
    "text": "Identifying a Perfectly Rated TV Episode\nNext, we aimed to identify a TV episode with a perfect 10/10 IMDb rating and at least 200,000 ratings. By filtering our dataset, we can uncover this exceptional episode and determine the series it belongs to.\n\ngood_ranking &lt;- TITLE_BASICS |&gt;\n  inner_join(TITLE_RATINGS, by = \"tconst\") |&gt;  # Join to bring in titleType from TITLE_BASICS\n  filter(titleType == \"tvEpisode\") |&gt;        # Now you can filter for TV episodes\n  filter(averageRating == 10 & numVotes &gt; 200000) |&gt;  # Filter for 10/10 rating and &gt;200k votes\n  arrange(desc(numVotes))  # Sort in descending order by number of votes\n\n# Create the summary table\nsummary_table &lt;- good_ranking |&gt;\n  select(Name = primaryTitle, rating = averageRating, \"Number of Votes\" = numVotes ) |&gt; \n  gt() |&gt;                       \n  tab_header(\n    title = \"TV Episode with a Rating of 10/10 and More Than 200,000 Votes\"\n  )\n\nsummary_table\n\n\n\n\n\n\n\nTV Episode with a Rating of 10/10 and More Than 200,000 Votes\n\n\nName\nrating\nNumber of Votes\n\n\n\n\nOzymandias\n10\n230332\n\n\n\n\n\n\n\nOur analysis revealed that the Breaking Bad episode “Ozymandias” has achieved a perfect 10/10 rating with over 229,000 votes on IMDb. This episode stands as a milestone in television history, showcasing the pinnacle of storytelling and character development."
  },
  {
    "objectID": "mp02.html#exploring-mark-hamills-most-known-projects",
    "href": "mp02.html#exploring-mark-hamills-most-known-projects",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Exploring Mark Hamill’s Most Known Projects",
    "text": "Exploring Mark Hamill’s Most Known Projects\nWe will now explore the four projects that actor Mark Hamill is most known for on IMDb. Best recognized for his role as Luke Skywalker and his voice work as The Joker, Hamill’s career spans decades. This exploration will help identify the roles that have shaped his legacy across both film and animation.\n\nmark_hamill &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Mark Hamill\") |&gt;\n  select(knownForTitles)\n\ntconsts &lt;- strsplit(mark_hamill$knownForTitles, \",\")[[1]]\n\nmark_hamill_title &lt;- TITLE_BASICS |&gt;\n   filter(tconst %in% tconsts) |&gt;\n   select(\"Title Name\" = primaryTitle, Type = titleType) |&gt;\n   gt() |&gt;\n    tab_header(\n    title = \"The four Projects Mark Hamill Is Best Known For\")\n\n\nmark_hamill_title\n\n\n\n\n\n\n\nThe four Projects Mark Hamill Is Best Known For\n\n\nTitle Name\nType\n\n\n\n\nStar Wars: Episode IV - A New Hope\nmovie\n\n\nStar Wars: Episode V - The Empire Strikes Back\nmovie\n\n\nStar Wars: Episode VI - Return of the Jedi\nmovie\n\n\nStar Wars: Episode VIII - The Last Jedi\nmovie\n\n\n\n\n\n\n\nFrom our analysis, we can see that Mark Hamill’s top four “Most Known For” projects are all from the Star Wars franchise, solidifying his iconic role as Luke Skywalker. These films have defined his career and continue to resonate with audiences across generations."
  },
  {
    "objectID": "mp02.html#next-exploration-highest-rated-tv-series",
    "href": "mp02.html#next-exploration-highest-rated-tv-series",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Next Exploration: Highest Rated TV Series",
    "text": "Next Exploration: Highest Rated TV Series\nBuilding on our analysis, we now turn our attention to identifying the TV series with more than 12 episodes that has the highest average rating. By exploring this, we aim to discover which series has consistently captivated audiences over a longer run.\n\nseries &lt;- TITLE_BASICS |&gt;\n  inner_join(TITLE_EPISODES, by = \"tconst\") |&gt;\n  inner_join(TITLE_RATINGS,by = \"tconst\")\n\nbest_series &lt;- series |&gt;\n  filter(episodeNumber &gt; 12) |&gt;\n  distinct() |&gt;\n  filter(averageRating == max(averageRating, na.rm = TRUE)) |&gt;\n  select(\"Tile Name\" = primaryTitle, \"Average Ratings\" = averageRating) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"The TV Series with the Highest Average Rating and More Than 12 Episodes\"\n  )\n  \nbest_series\n\n\n\n\n\n\n\nThe TV Series with the Highest Average Rating and More Than 12 Episodes\n\n\nTile Name\nAverage Ratings\n\n\n\n\nSeries finale\n10\n\n\nI challenge the Ender Dragon in Minecraft (Ending)\n10\n\n\n37. Bolum\n10\n\n\n38. Bolum\n10\n\n\n39. Bolum\n10\n\n\n40. Bolum\n10\n\n\nGoodbye.\n10\n\n\nIce Hockey: Courtney\n10\n\n\nOzymandias\n10\n\n\n\n\n\n\n\nAs we can see in the results, there are several series with more than 12 episodes that hold a perfect rating of 10. However, there seems to be an issue with the data. The series “Bolum” is listed multiple times, which likely represents individual episodes of the same show, rather than different series. This duplication, indicated by episode numbers (e.g., 37. Bolum, 38. Bolum), points to a mistake in how the data has been recorded."
  },
  {
    "objectID": "mp02.html#investigating-jump-the-shark-in-happy-days",
    "href": "mp02.html#investigating-jump-the-shark-in-happy-days",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Investigating “Jump the Shark” in Happy Days",
    "text": "Investigating “Jump the Shark” in Happy Days\nThe TV series Happy Days (1974-1984) gave us the common idiom “jump the shark.” The phrase refers to a moment in a controversial fifth season episode (aired in 1977) where a lead character literally jumped over a shark on water skis. Over time, this phrase has come to mean the point when a once-great show becomes ridiculous and begins to decline in quality.\nTo investigate this, we now ask: Is it true that episodes from the later seasons of Happy Days have lower average ratings than the early seasons? Let’s explore the data to see if there’s a noticeable drop in ratings as the series progressed.\n\ndata &lt;- TITLE_BASICS |&gt;\n  inner_join(TITLE_EPISODES, join_by(tconst == parentTconst)) |&gt;  # Join episodes with series\n  inner_join(TITLE_RATINGS, by = \"tconst\")  # Join ratings dat\n\nhappy_days &lt;- data |&gt;\n    filter(primaryTitle == \"Happy Days\") |&gt;\n    arrange(seasonNumber) |&gt;\n    select(primaryTitle, \"Season Number\" = seasonNumber, \"Average Rating\" = averageRating, year = endYear) |&gt;\n    distinct() |&gt;\n\n  gt() |&gt;\n  tab_header(\n    title = \"Happy Days Seasons & Ratings per season\"\n  )\n\nhappy_days\n\n\n\n\n\n\n\nHappy Days Seasons & Ratings per season\n\n\nprimaryTitle\nSeason Number\nAverage Rating\nyear\n\n\n\n\nHappy Days\n1\n7.4\n1984\n\n\nHappy Days\n2\n7.4\n1984\n\n\nHappy Days\n3\n7.4\n1984\n\n\nHappy Days\n4\n7.4\n1984\n\n\nHappy Days\n5\n7.4\n1984\n\n\nHappy Days\n6\n7.4\n1984\n\n\nHappy Days\n7\n7.4\n1984\n\n\nHappy Days\n8\n7.4\n1984\n\n\nHappy Days\n9\n7.4\n1984\n\n\nHappy Days\n10\n7.4\n1984\n\n\nHappy Days\n11\n7.4\n1984\n\n\n\n\n\n\n\nThe assumption that the later seasons of Happy Days had lower average ratings is not true. As we can observe, the average rating remained constant at 7.4 across all seasons. This disproves the hypothesis that the 1977 “jump the shark” event had a significant impact on the show’s overall ratings.\nDespite the controversy surrounding that episode, the data shows no evidence of a decline in audience ratings afterward."
  },
  {
    "objectID": "mp02.html#creating-criteria-for-how-successful-a-movie-is",
    "href": "mp02.html#creating-criteria-for-how-successful-a-movie-is",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Creating Criteria for How Successful a Movie Is:",
    "text": "Creating Criteria for How Successful a Movie Is:\nAs we aim to propose a new movie, it is important to understand how successful it might be. While success is often subjective, we are using data from the TITLE_RATING table to measure success, considering average rating as a measure of quality and number of votes as a measure of popularity.\nTraditionally, a movie’s success is also evaluated by the revenue it generates. Therefore, we have selected seven movies with the highest revenue and five movies that lost money, with data sourced from Box Office Mojo."
  },
  {
    "objectID": "mp02.html#movie-revenue-table",
    "href": "mp02.html#movie-revenue-table",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Movie Revenue Table",
    "text": "Movie Revenue Table\n\n\n\n\n\n\n\nSuccessful Movies and Revenue\nUnsuccessful Movies, Money Lost\n\n\n\n\nAvatar ($2.92 billion)\nJohn Carter (-$0.20 billion)\n\n\nAvatar: The Way of Water ($2.32 billion)\nThe Lone Ranger (-$0.19 billion)\n\n\nTitanic ($2.25 billion)\nMars Needs Moms (-$0.15 billion)\n\n\nStar Wars: Episode VII - The Force Awakens ($2.07 billion)\nKing Arthur: Legend of the Sword (-$0.15 billion)\n\n\nSpider-Man: No Way Home ($1.92 billion)\nSinbad: Legend of the Seven Seas (-$0.125 billion)\n\n\nJurassic World ($1.67 billion)\nCutthroat Island (-$0.1 billion)\n\n\nTop Gun: Maverick ($1.49 billion)\nR.I.P.D. (-$0.1 billion)\n\n\n\n\n#first we are filtering the movies we need, selecting the usuful tables, such as originalTitle, averageRating, numVotes\nmovies &lt;- TITLE_BASICS |&gt;\n  inner_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n  filter(titleType == \"movie\")\n  \nfilter_movies &lt;- movies |&gt;\n  filter((str_detect(str_to_lower(primaryTitle), \"avatar\") & startYear == 2009) |\n          (str_detect(str_to_lower(primaryTitle), \"avatar: the way of water\") & startYear == 2022) |\n          (primaryTitle == \"Titanic\" & startYear == 1997) |\n          (str_detect(str_to_lower(primaryTitle), \"star wars\") & startYear == 2015) |\n          (str_detect(str_to_lower(primaryTitle), \"spider-man: no way home\") & startYear == 2021) |\n          (str_detect(str_to_lower(primaryTitle), \"jurassic world\") & startYear == 2015) |\n          (str_detect(str_to_lower(primaryTitle), \"top gun: maverick\") & startYear == 2022) |\n          (str_detect(str_to_lower(primaryTitle), \"john carter\")) |\n          (str_detect(str_to_lower(primaryTitle), \"the lone ranger\") & startYear == 2013) |\n          (str_detect(str_to_lower(primaryTitle), \"mars needs moms\")) |\n          (str_detect(str_to_lower(primaryTitle), \"king arthur\") & startYear == 2017) |\n          (primaryTitle ==  \"Cutthroat Island\") |\n          (primaryTitle ==  \"R.I.P.D.\") |\n          (str_detect(str_to_lower(primaryTitle), \"legend of the seven seas\")))\nfilter_movies &lt;- filter_movies |&gt;\n  arrange(primaryTitle)\n\n# Assign revenue to the filtered dataframe\nfilter_movies$Revenue &lt;- c(\n  2.92,   # Avatar (successful)\n  2.32,   # Avatar: The Way of Water (successful)\n  -0.10,  # Cutthroat Island (loss)\n  -0.20,  # John Carter (loss)\n  1.67,   # Jurassic World (successful)\n  -0.15,  # King Arthur: Legend of the Sword (loss)\n  -0.15,  # Mars Needs Moms (loss)\n  -0.10,  # R.I.P.D. (loss)\n  -0.125, # Sinbad: Legend of the Seven Seas (loss)\n  1.92,   # Spider-Man: No Way Home (successful)\n  2.07,   # Star Wars: Episode VII - The Force Awakens (successful)\n  -0.19,  # The Lone Ranger (loss)\n  2.25,   # Titanic (successful)\n  1.49    # Top Gun: Maverick (successful)\n)\n\nfilter_movies |&gt;\n  arrange(desc(Revenue)) |&gt;  # Sort by revenue\n  select(\"Original Name\" = primaryTitle, \"Average Rating\" = averageRating, \"Votes\" = numVotes, \"Revenue (Billion)\" = Revenue) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"7 most Sucessful and 7 most Unsuccessful Movies based on revenue (on Billion)\"\n   )\n\n\n\n\n\n\n\n7 most Sucessful and 7 most Unsuccessful Movies based on revenue (on Billion)\n\n\nOriginal Name\nAverage Rating\nVotes\nRevenue (Billion)\n\n\n\n\nAvatar\n7.9\n1406045\n2.920\n\n\nAvatar: The Way of Water\n7.5\n513213\n2.320\n\n\nTitanic\n7.9\n1309446\n2.250\n\n\nStar Wars: Episode VII - The Force Awakens\n7.8\n985881\n2.070\n\n\nSpider-Man: No Way Home\n8.2\n911025\n1.920\n\n\nJurassic World\n6.9\n689617\n1.670\n\n\nTop Gun: Maverick\n8.2\n735945\n1.490\n\n\nCutthroat Island\n5.7\n31185\n-0.100\n\n\nR.I.P.D.\n5.6\n146113\n-0.100\n\n\nSinbad: Legend of the Seven Seas\n6.7\n60614\n-0.125\n\n\nKing Arthur: Legend of the Sword\n6.7\n236363\n-0.150\n\n\nMars Needs Moms\n5.4\n24310\n-0.150\n\n\nThe Lone Ranger\n6.4\n246519\n-0.190\n\n\nJohn Carter\n6.6\n289519\n-0.200\n\n\n\n\n\n\n\nAfter adding the revenue column, we will create a model to serve as a criterion for success. The model is based on the following assumptions:\n\nScale: Our success scale will range from 0 to 10, with 10 representing an extremely successful movie.\nRevenue Mapping: For the positive revenue values (successful movies), we will assign ratings between 9.5 and 10, indicating extreme success. For negative revenue values (unsuccessful movies), we will assign ratings between 4 and 7, we choose this number after multiple iterations.\n\nThis range was chosen because, as observed in our table, even if a movie receives a large number of votes, ratings below 7 tend to correlate with more negative feedback, the opposite of what we expect from a successful movie, regardless of popularity.\n\n# Step 1: Define the max and min values for positive and negative revenues\nmax_positive_revenue &lt;- max(filter_movies$Revenue[filter_movies$Revenue &gt; 0])\nmin_positive_revenue &lt;- min(filter_movies$Revenue[filter_movies$Revenue &gt; 0])\n\nmax_negative_revenue &lt;- max(filter_movies$Revenue[filter_movies$Revenue &lt; 0])  # closest to 0\nmin_negative_revenue &lt;- min(filter_movies$Revenue[filter_movies$Revenue &lt; 0])  # most negative\n\n# Step 2: Apply different formulas based on whether revenue is positive or negative\nfilter_movies$Success_Score &lt;- ifelse(\n  filter_movies$Revenue &gt; 0, \n  9.5 + 0.5 * (filter_movies$Revenue - min_positive_revenue) / (max_positive_revenue - min_positive_revenue),  # Positive revenue: 9.5 to 10 scale\n  4 +  3 * (filter_movies$Revenue - min_negative_revenue) / (max_negative_revenue - min_negative_revenue)  # Negative revenue: 6 to 7 scale\n)\n\nfilter_movies |&gt;\n  arrange(desc(Success_Score)) |&gt; # Sort by revenue\n  select(\"Original Name\" = primaryTitle, \"Average Rating\" = averageRating, \"Votes\" = numVotes, \"Success Score\" = Success_Score ) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"7 most Sucessful and 7 most Unsuccessful Movies based on Success Score (on Billion)\"\n   )\n\n\n\n\n\n\n\n7 most Sucessful and 7 most Unsuccessful Movies based on Success Score (on Billion)\n\n\nOriginal Name\nAverage Rating\nVotes\nSuccess Score\n\n\n\n\nAvatar\n7.9\n1406045\n10.000000\n\n\nAvatar: The Way of Water\n7.5\n513213\n9.790210\n\n\nTitanic\n7.9\n1309446\n9.765734\n\n\nStar Wars: Episode VII - The Force Awakens\n7.8\n985881\n9.702797\n\n\nSpider-Man: No Way Home\n8.2\n911025\n9.650350\n\n\nJurassic World\n6.9\n689617\n9.562937\n\n\nTop Gun: Maverick\n8.2\n735945\n9.500000\n\n\nCutthroat Island\n5.7\n31185\n7.000000\n\n\nR.I.P.D.\n5.6\n146113\n7.000000\n\n\nSinbad: Legend of the Seven Seas\n6.7\n60614\n6.250000\n\n\nKing Arthur: Legend of the Sword\n6.7\n236363\n5.500000\n\n\nMars Needs Moms\n5.4\n24310\n5.500000\n\n\nThe Lone Ranger\n6.4\n246519\n4.300000\n\n\nJohn Carter\n6.6\n289519\n4.000000\n\n\n\n\n\n\n\nBased on the assigned success scores, we will apply a linear regression model to determine the parameters B0, B1, and B2. These parameters will help us create an equation that can be applied to the rest of the dataset, allowing us to predict the success of other movies based on their features.\nThe linear regression model will use the following:\n\nB0: The intercept, representing the baseline success score.\nB1: The coefficient for the average rating, indicating the effect of quality on a movie’s success.\nB2: The coefficient for the number of ratings, reflecting how popularity impacts success.\n\n\n# Fit the linear regression model\nmodel &lt;- lm(Success_Score ~ averageRating + numVotes, data = filter_movies)\n\nsummary(model)\n\n\nCall:\nlm(formula = Success_Score ~ averageRating + numVotes, data = filter_movies)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7786 -0.5785  0.1934  1.0393  1.8759 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   2.140e+00  4.838e+00   0.442    0.667\naverageRating 5.830e-01  7.967e-01   0.732    0.480\nnumVotes      2.731e-06  1.643e-06   1.662    0.125\n\nResidual standard error: 1.498 on 11 degrees of freedom\nMultiple R-squared:  0.6284,    Adjusted R-squared:  0.5608 \nF-statistic: 9.299 on 2 and 11 DF,  p-value: 0.004322\n\n\nBased on the regression analysis, the final model to predict the success score is represented by the following equation:\n\\[\n\\text{Success Score} = 2.133 + 0.5842 \\times (\\text{Average Rating}) + 2.730 \\times 10^{-6} \\times (\\text{Votes})\n\\]\n\nThe R-squared value is 0.6283, meaning that about 62.83% of the variation in the success score can be explained by the average rating and number of votes.\nThe Adjusted R-squared is 0.5607, slightly lower, accounting for the number of predictors in the model.\nWith a p-value of 0.0043, the overall model is statistically significant.\n\nBased on our analysis, we have determined that the threshold for a movie to be considered successful is an average rating of 7.9 or higher."
  },
  {
    "objectID": "mp02.html#analyzing-a-prestige-actor-leonardo-dicaprio",
    "href": "mp02.html#analyzing-a-prestige-actor-leonardo-dicaprio",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Analyzing a Prestige Actor: Leonardo DiCaprio",
    "text": "Analyzing a Prestige Actor: Leonardo DiCaprio\nNext, we will examine the projects of a prestigious actor, Leonardo DiCaprio, and confirm that many of his films achieve high scores on our success metric. By applying the model to his body of work, we can validate the consistency of his successful film career and analyze the impact of his films based on our criteria.\n\n# Filter the data for Leonardo DiCaprio and Christopher Nolan\nleonardo_dicaprio &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Leonardo DiCaprio\") |&gt;\n  select(knownForTitles)\n\nnolan &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Christopher Nolan\") |&gt;\n  select(knownForTitles)\n\n# Split the knownForTitles column to get the tconsts\ntconsts_leonardo &lt;- strsplit(leonardo_dicaprio$knownForTitles, \",\")[[1]]\ntconsts_nolan &lt;- strsplit(nolan$knownForTitles, \",\")[[1]]\n\n# Filter the movies based on the tconsts for Leonardo DiCaprio and Christopher Nolan\nleonardo_movies &lt;- movies |&gt;\n  filter(tconst %in% tconsts_leonardo) |&gt;\n  mutate(Person = \"Leonardo DiCaprio\")\n\nnolan_movies &lt;- movies |&gt;\n  filter(tconst %in% tconsts_nolan) |&gt;\n  mutate(Person = \"Christopher Nolan\")\n\nleonardo_nolan_movies &lt;- bind_rows(leonardo_movies, nolan_movies)\n\nleonardo_nolan_movies |&gt;\n  select(\"Original Name\" = primaryTitle, \n         \"Average Rating\" = averageRating, \n         \"Votes\" = numVotes, \n         \"Status\" = SuccessStatus, \n         \"Associated With\" = Person) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Movies Associated with Leonardo DiCaprio and Christopher Nolan\"\n  )\n\n\n\n\n\n\n\nMovies Associated with Leonardo DiCaprio and Christopher Nolan\n\n\nOriginal Name\nAverage Rating\nVotes\nStatus\nAssociated With\n\n\n\n\nTitanic\n7.9\n1309446\nSuccessful\nLeonardo DiCaprio\n\n\nThe Departed\n8.5\n1449409\nSuccessful\nLeonardo DiCaprio\n\n\nThe Wolf of Wall Street\n8.2\n1627553\nSuccessful\nLeonardo DiCaprio\n\n\nInception\n8.8\n2605397\nSuccessful\nLeonardo DiCaprio\n\n\nThe Prestige\n8.5\n1472193\nSuccessful\nChristopher Nolan\n\n\nInterstellar\n8.7\n2179321\nSuccessful\nChristopher Nolan\n\n\nInception\n8.8\n2605397\nSuccessful\nChristopher Nolan\n\n\n\n\n\n\n\nAs we can see, both Leonardo DiCaprio and Christopher Nolan have a significant number of movies that score highly on our success metric. This confirms their prestige and success in the film industry, as their films consistently receive high ratings and are recognized as successful according to our model."
  },
  {
    "objectID": "mp02.html#final-validation-analyzing-bottom-successful-movies",
    "href": "mp02.html#final-validation-analyzing-bottom-successful-movies",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Final Validation: Analyzing Bottom Successful Movies",
    "text": "Final Validation: Analyzing Bottom Successful Movies\nFor our final validation, we will select the bottom 5 successful movies based on our success metric and manually check if they have won any major awards. This will help us assess whether the model accurately reflects not only commercial success but also critical recognition in the film industry.\n\n# Select top 5 movies based on success score\ntop_movies &lt;- movies |&gt;\n  arrange(desc(SuccessNewScore)) |&gt;\n  filter(SuccessStatus == \"Successful\") |&gt;\n  slice_tail(n = 5)\n\nsummarize &lt;-top_movies |&gt;\n  select(\"Original Name\" = primaryTitle, \n         \"Average Rating\" = averageRating, \n         \"Votes\" = numVotes, \n         \"Success Score\" = SuccessNewScore, \n         \"Status\" = SuccessStatus) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Bottom 5 succesful Movies Based on Success Score\"\n  )\nsummarize\n\n\n\n\n\n\n\nBottom 5 succesful Movies Based on Success Score\n\n\nOriginal Name\nAverage Rating\nVotes\nSuccess Score\nStatus\n\n\n\n\nAvatar: The Way of Water\n7.5\n513213\n7.915571\nSuccessful\n\n\nWreck-It Ralph\n7.7\n468223\n7.909589\nSuccessful\n\n\nThe Breakfast Club\n7.8\n446608\n7.909000\nSuccessful\n\n\nIndependence Day\n7.0\n617080\n7.907028\nSuccessful\n\n\nTo Kill a Mockingbird\n8.3\n337554\n7.903382\nSuccessful\n\n\n\n\n\n\n\nFrom the bottom 5 successful movies, we can see that all of them have received good recognition. Notably, 3 out of the 5 have won awards, with one being a nominee, which further confirms the accuracy of our model in predicting the success of a movie based on its average rating and number of votes."
  },
  {
    "objectID": "mp02.html#threshold-for-success",
    "href": "mp02.html#threshold-for-success",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Threshold for Success",
    "text": "Threshold for Success\nWe examined the “Success Score” metric, which combines both IMDb rating and number of votes. This score reflects both the quality and popularity of a movie. Based on our analysis, we determined that the threshold for a movie to be considered successful is an average rating of 7.9 or higher.\nThis threshold was derived from the analysis of the top 7 successful movies and the bottom 7 unsuccessful movies in terms of revenue. The score of 7.9 corresponds to the lowest rating of the most successful movies, specifically Avatar: The Way of Water.\nHowever, this threshold can be adjusted due to the limited number of successful movies. Adjusting the threshold to 7.5 would represent the average rating of successful films, expanding the criteria from 149 successful movies to 542 movies, thus providing a more inclusive measure of success.\n\nlibrary(tidyr)\n\n#changing the threshold to 7.5\nTITLE_RATINGS$SuccessStatus &lt;- ifelse(TITLE_RATINGS$SuccessNewScore &gt; 7.5, \"Successful\", \"Unsuccessful\")\n\nmovies &lt;- TITLE_BASICS |&gt;\n  inner_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n  filter(titleType == \"movie\")\n\nsuccessful_genre &lt;- movies |&gt;\n  filter(SuccessStatus == \"Successful\") |&gt;\n  mutate(decade = floor(startYear / 10) * 10) |&gt;\n  separate_rows(genres, sep = \",\") |&gt;  # Split genres into individual rows\n  group_by(decade, genres) |&gt;\n  summarise(count = n(), .groups = 'drop')  # Summarise with count\n\nsuccessful_genre_per_decade &lt;- successful_genre |&gt;\n  group_by(decade) |&gt;\n  filter(count == max(count)) \n\nggplot(successful_genre_per_decade, aes(x = decade, y = count, fill = genres)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +  \n  labs(title = \"Most Successful Genres by Decade\",\n       x = \"Decade\",\n       y = \"Number of Movies\",\n       fill = \"Genre\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_fill_brewer(palette = \"Set1\") \n\n\n\n\n\n\n\n\nFor each decade, we observe a variety of genres that have achieved success. However, Drama stands out as the most consistently successful genre across all decades. This indicates that drama movies have consistently resonated with audiences and garnered positive reception from critics. While other genres have experienced success in different decades, none have matched the enduring appeal of drama."
  },
  {
    "objectID": "mp02.html#studying-the-evolution-of-genres-over-time",
    "href": "mp02.html#studying-the-evolution-of-genres-over-time",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Studying the Evolution of Genres Over Time",
    "text": "Studying the Evolution of Genres Over Time\nWe aim to explore which genre consistently yields the most “successes” and identify genres that once produced reliable successes but have since fallen out of favor. To accomplish this, we will exclude drama from our analysis and examine the top 2 genres for each decade, allowing us to highlight emerging trends and shifts in audience preferences.\n\nsuccessful_genre_per_decade &lt;- successful_genre |&gt;\n  filter(genres != \"Drama\") |&gt;\n  group_by(decade) |&gt;\n  arrange(decade, desc(count)) |&gt;  # Arrange by decade and descending count\n  slice_head(n = 2)  # Select the top 2 genres per decade\n\nggplot(successful_genre_per_decade, aes(x = decade, y = count, fill = genres)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +  \n  labs(title = \"Most Successful Genres by Decade (Excluding Drama)\",\n       x = \"Decade\",\n       y = \"Number of Movies\",\n       fill = \"Genre\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_fill_brewer(palette = \"Set1\") \n\n\n\n\n\n\n\n\nAfter excluding the drama genre, we observe that Action and Adventure have been the most successful genres over time, with Action showing significant growth and Adventure demonstrating more consistency. Other genres, such as Mystery, Crime, and Comedy, have had success in the past but lack the consistency seen in Action and Adventure. Additionally, the growth of the Thriller genre in the last decade is noteworthy; it has emerged as the third most successful genre following Drama and Action."
  },
  {
    "objectID": "mp02.html#movies-production-vs.-success",
    "href": "mp02.html#movies-production-vs.-success",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Movies Production vs. Success",
    "text": "Movies Production vs. Success\nWe aim to analyze the relationship between the number of movies produced and the number of successful movies. Specifically, we want to determine if the number of successful movies has increased over time and whether the ratio of successful movies to total movies has changed. To achieve this, we will calculate the success ratio by decade and genre, focusing on the top 5 genres: Drama, Action, Adventure, Thriller, and Crime.\n\nlibrary(dplyr)\nlibrary(gt)\n\ntop_genres &lt;- c(\"Drama\", \"Action\", \"Adventure\", \"Thriller\", \"Crime\")\n\nratio_successs &lt;- movies |&gt;\n  mutate(decade = floor(startYear / 10) * 10) |&gt;\n  separate_rows(genres, sep = \",\") |&gt;  # Split genres into individual rows if it's a combined column\n  filter(genres %in% top_genres) |&gt;  # Focus only on top 5 genre\n  group_by(decade, genres) |&gt;\n  summarise(\n    total_movies = n(), \n    successful_movies = sum(SuccessStatus == \"Successful\"), \n    .groups = 'drop'\n  ) |&gt;\n  mutate(successful_ratio = round((successful_movies / total_movies)*100, 2))  # Round to 2 decimals\n\nggplot(ratio_successs, aes(x = decade, y = successful_ratio, color = genres)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Percentage of Successful Movies to Total Movies by Decade and Genre\",\n       x = \"Decade\",\n       y = \"Success Ratio\",\n       color = \"Genre\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\nWith this graph, we can observe the varying ratios of success, indicating the percentage of successful movies within each genre. Overall, there is a clear upward trend in the success of movies across all genres. However, Adventure and Action have shown the most significant growth since the 1970s, while Drama has maintained a consistent performance over time. This highlights the evolving landscape of popular genres and their ability to resonate with audiences across different decades.\n\nggplot(ratio_successs, aes(x = decade, y = total_movies, fill = genres)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Total Number of Movies by Decade and Genre\",\n       x = \"Decade\",\n       y = \"Number of Movies\",\n       fill = \"Genre\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\nFrom these results, we can clearly see that the number of movies created in the Drama genre consistently exceeds that of any other genre by more than double, though the success ratio has increased over time. In contrast, the success ratios for Adventure and Action have shown growth since the 1970s, while the number of movies produced in these genres has remained steady.\nAdditionally, we observe that in the last decade, Thriller has increased both in the number of movies created and its success ratio, indicating a positive trend for this genre. Overall, there is a decline in movie productions, which can be attributed to the rise of streaming services that have transformed the way movies are produced and consumed. This trend is likely to continue as the industry adapts to new technologies and shifting consumer preferences."
  },
  {
    "objectID": "mp02.html#hollywood-project",
    "href": "mp02.html#hollywood-project",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Hollywood Project",
    "text": "Hollywood Project\nThe next step is to create our own movie. To do this, we will first select a genre, two actors, and one director that would be ideal for the film. Here are the criteria we will use:\n\nGenre: Adventure, combined with some Action, as it has proven consistent over time with a high success ratio.\nActors and Director: We will select successful individuals from this genre while incorporating some new actors who have found success in the last decade.\n\n\nmy_genres &lt;- c(\"Action\", \"Adventure\")\n\nmy_movie &lt;-movies |&gt;\n  inner_join(TITLE_PRINCIPALS, by = \"tconst\") |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\")\n\nget_crew &lt;- my_movie |&gt;\n  filter(startYear &gt;= 2008) |&gt;\n  separate_rows(genres, sep = \",\") |&gt;  \n  filter(genres %in% my_genres, SuccessStatus == \"Successful\")\n\ntop_actors &lt;- get_crew |&gt;\n  filter(category == \"actor\" | category == \"actress\") |&gt;\n  group_by(primaryName) |&gt;  \n  summarise(successful_movies = n()) |&gt;  # Count the number of successful movies per actor\n  arrange(desc(successful_movies)) |&gt;  # Sort by most successful movies\n  slice_head(n = 6)  # Select the top 5 actors\n\ntop_directors &lt;- get_crew |&gt;\n  filter(category == \"director\") |&gt;  \n  group_by(primaryName) |&gt;  \n  summarise(successful_movies = n()) |&gt;  \n  arrange(desc(successful_movies))|&gt; \n  slice_head(n = 7)  \n\ntop_actors |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 5 Actors in Successful Action/Adventure Movies After 2010\"\n  )\n\n\n\n\n\n\n\nTop 5 Actors in Successful Action/Adventure Movies After 2010\n\n\nprimaryName\nsuccessful_movies\n\n\n\n\nRobert Downey Jr.\n7\n\n\nChris Evans\n6\n\n\nIrrfan Khan\n5\n\n\nAndrew Garfield\n4\n\n\nChris Pine\n4\n\n\nClark Gregg\n4\n\n\n\n\n\n\ntop_directors |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Most Important Directors in Successful Action/Adventure Movies After 2010\"\n  )\n\n\n\n\n\n\n\nMost Important Directors in Successful Action/Adventure Movies After 2010\n\n\nprimaryName\nsuccessful_movies\n\n\n\n\nDavid Yates\n3\n\n\nZack Snyder\n3\n\n\nChris Sanders\n2\n\n\nChristopher Nolan\n2\n\n\nColin Trevorrow\n2\n\n\nDean DeBlois\n2\n\n\nGuy Ritchie\n2\n\n\n\n\n\n\n\nNow that we have identified the actors with the most success in the Adventure genre, we will select the top 2 actors and the top director based on their success ratios. To accomplish this, we will calculate the ratio of successful movies to total movies for each actor and director.\nFollowing this analysis, we will create a bar plot to visualize both the total number of movies and the number of successful movies for the selected actors and directors. This will help us clearly see who has performed best in this genre and support our decision-making process for the movie project.\n\n#parameters to select the actors and directors with our genre. \nselected_actors &lt;- c(\"Chris Evans\", \"Hugo Weaving\", \"Robert Downey Jr.\", \"Irrfan Khan\", \"Clark Gregg\", \"Henry Cavill\")\nselected_directors &lt;- c(\"David Yates\", \"Zack Snyder\", \"Chris Sanders\", \"Christopher Nolan\", \"Dean DeBlois\", \"Guy Ritchie\", \"J.J. Abrams\")\nmy_genres &lt;- c(\"Action\", \"Adventure\")\n\n\n# Step 2: Filter the data for selected actors and directors\nactor_movies &lt;- my_movie |&gt;\n  separate_rows(genres, sep = \",\") |&gt;  \n  filter(genres %in% my_genres & startYear &gt;= 2008 & primaryName %in% selected_actors)|&gt;\n  group_by(primaryName) |&gt;\n  summarise(total_movies = n())\n\ndirector_movies &lt;- my_movie |&gt;\n  separate_rows(genres, sep = \",\") |&gt;  \n  filter(genres %in% my_genres & startYear &gt;= 2008 & primaryName %in% selected_directors)|&gt;\n  group_by(primaryName) |&gt;\n  summarise(total_movies = n())\n\n# we would calculate the success ratio for each actor. \nactor_data &lt;- actor_movies |&gt;\n  left_join(top_actors, by = \"primaryName\") |&gt;\n  replace_na(list(successful_movies = 0)) |&gt;\n  mutate(success_ratio = successful_movies / total_movies)  # Calculate success ratio\n\n# Reshape data for easier plotting of bars,  this help us to create the plot.\nactor_data_long &lt;- actor_data |&gt;\n  pivot_longer(cols = c(\"total_movies\", \"successful_movies\"), \n               names_to = \"category\", \n               values_to = \"count\")\n\n# Create the bar plot with a line for the success ratio\nggplot() +\n  # Bar plot for total and successful movies\n  geom_bar(data = actor_data_long, aes(x = primaryName, y = count, fill = category), \n           stat = \"identity\", position = \"dodge\") +\n  \n  # Line plot for success ratio (on top of the original unreshaped data)\n  geom_line(data = actor_data, aes(x = primaryName, y = success_ratio * max(actor_data$total_movies), group = 1), \n            color = \"blue\", size = 1, linetype = \"dashed\") +  \n  \n  # Add points for success ratio\n  geom_point(data = actor_data, aes(x = primaryName, y = success_ratio * max(actor_data$total_movies)), \n             color = \"blue\", size = 3) +\n  \n  # Labels and titles\n  labs(title = \"Movies vs Successful Movies (with Success Ratio)\",\n       x = \"Actor\",\n       y = \"Count of Movies\",\n       fill = \"Category\") +\n  \n  # Secondary y-axis for the success ratio\n  scale_y_continuous(sec.axis = sec_axis(~./max(actor_data$total_movies), name = \"Success Ratio\")) +\n  \n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nAfter analyzing the data, we can see that Clark Gregg and Hugo Weaving have a record of success, each with a 100% success ratio in 4 movies after 2008. However, we want to incorporate Chris Evans into our movies because, despite his success ratio being 0.5, he has appeared in 12 movies, which is a good indicator of his popularity and appeal to audiences.\n\n# Prepare the director data (similar process as for actors)\ndirector_data &lt;- director_movies |&gt;\n  left_join(top_directors, by = \"primaryName\") |&gt;\n  replace_na(list(successful_movies = 0)) |&gt;\n  mutate(success_ratio = successful_movies / total_movies)  # Calculate success ratio\n\n# Reshape data for easier plotting of bars\ndirector_data_long &lt;- director_data |&gt;\n  pivot_longer(cols = c(\"total_movies\", \"successful_movies\"), \n               names_to = \"category\", \n               values_to = \"count\")\n\n# Create the bar plot with a line for the success ratio\nggplot() +\n  # Bar plot for total and successful movies\n  geom_bar(data = director_data_long, aes(x = primaryName, y = count, fill = category), \n           stat = \"identity\", position = \"dodge\") +\n  \n  # Line plot for success ratio (on top of the original unreshaped data)\n  geom_line(data = director_data, aes(x = primaryName, y = success_ratio * max(director_data$total_movies), group = 1), \n            color = \"blue\", size = 1, linetype = \"dashed\") +  \n  \n  # Add points for success ratio\n  geom_point(data = director_data, aes(x = primaryName, y = success_ratio * max(director_data$total_movies)), \n             color = \"blue\", size = 3) +\n  \n  # Labels and titles\n  labs(title = \"Movies vs Successful Movies (Directors, After 2010, with Success Ratio)\",\n       x = \"Director\",\n       y = \"Count of Movies\",\n       fill = \"Category\") +\n  \n  # Secondary y-axis for the success ratio\n  scale_y_continuous(sec.axis = sec_axis(~./max(director_data$total_movies), name = \"Success Ratio\")) +\n  \n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\nWarning: Use of `director_data$total_movies` is discouraged.\nℹ Use `total_movies` instead.\nUse of `director_data$total_movies` is discouraged.\nℹ Use `total_movies` instead.\n\n\n\n\n\n\n\n\n\nNow, we will select our director based on the success ratio. In this case, it is an easy pick: we have selected David Yates, who has 3 successful movies out of 5 in the genre we are looking for. Although other directors have more movies, the success ratio is the most important factor in this case."
  },
  {
    "objectID": "mp02.html#movie-creation",
    "href": "mp02.html#movie-creation",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Movie Creation",
    "text": "Movie Creation\nOur movies would be a remake movie, we would look movies that where succesful 25 years ago and we would create a remake of it. We would select 7 movies that have as genre action and adventure, in that way we would be sure that the movie would be successful.\n\nlibrary(knitr)\n\nmy_genres &lt;- c(\"Action\", \"Adventure\")\n\nmy_movie &lt;-movies |&gt;\n  inner_join(TITLE_PRINCIPALS, by = \"tconst\") |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\") |&gt;\n  inner_join(TITLE_CREW, by = \"tconst\")  # Join with crew data\n\n\n\nselection &lt;- my_movie |&gt;\n  separate_rows(genres, sep = \",\") |&gt;\n                \n  filter((startYear &lt; 2000) & (SuccessStatus == \"Successful\") & (genres %in% my_genres)) |&gt;\n  group_by(primaryTitle) |&gt;\n  filter(n_distinct(genres) == 2) |&gt;  # Keep only movies with both Action and Adventure genres\n  arrange(desc(SuccessNewScore)) |&gt;\n  distinct(primaryTitle, .keep_all = TRUE)  |&gt;  #Keep only the first occurrence of each movie, then ungroup\n  ungroup()   \n  \nselection |&gt;\n  select(\"Original Name\" = primaryTitle, \n         \"Average Rating\" = averageRating, \n         \"Votes\" = numVotes,\n         \"Year\" = startYear,\n         \"Success Score\" = SuccessNewScore) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Successful Movies with Action and Adventure Genres Before 2000\"\n  )\n\n\n\n\n\n\n\nSuccessful Movies with Action and Adventure Genres Before 2000\n\n\nOriginal Name\nAverage Rating\nVotes\nYear\nSuccess Score\n\n\n\n\nStar Wars: Episode IV - A New Hope\n8.6\n1475294\n1977\n11.184673\n\n\nStar Wars: Episode V - The Empire Strikes Back\n8.7\n1406428\n1980\n11.055088\n\n\nTerminator 2: Judgment Day\n8.6\n1199025\n1991\n10.430458\n\n\nStar Wars: Episode VI - Return of the Jedi\n8.3\n1140740\n1983\n10.096080\n\n\nRaiders of the Lost Ark\n8.4\n1052079\n1981\n9.912456\n\n\nJurassic Park\n8.2\n1090143\n1993\n9.899530\n\n\nThe Terminator\n8.1\n943192\n1984\n9.439934\n\n\nAliens\n8.4\n788138\n1986\n9.191897\n\n\nIndiana Jones and the Last Crusade\n8.2\n822785\n1989\n9.169643\n\n\nStar Wars: Episode I - The Phantom Menace\n6.5\n874093\n1999\n8.316574\n\n\nPrincess Mononoke\n8.3\n444856\n1997\n8.196317\n\n\nMen in Black\n7.3\n625083\n1997\n8.104137\n\n\nIndiana Jones and the Temple of Doom\n7.5\n542706\n1984\n7.996087\n\n\nThe Fifth Element\n7.6\n516253\n1997\n7.982291\n\n\nPredator\n7.8\n466197\n1987\n7.962478\n\n\nNorth by Northwest\n8.3\n351696\n1959\n7.941990\n\n\nIndependence Day\n7.0\n617080\n1996\n7.907028\n\n\nDie Hard with a Vengeance\n7.6\n413379\n1995\n7.701445\n\n\nMission: Impossible\n7.2\n478390\n1996\n7.645245\n\n\nBatman\n7.5\n412196\n1989\n7.639795\n\n\nThe Mummy\n7.1\n471372\n1999\n7.567666\n\n\nRamayana: The Legend of Prince Rama\n9.2\n15449\n1993\n7.549816\n\n\nThe Iron Giant\n8.1\n234915\n1999\n7.506338"
  },
  {
    "objectID": "mp02.html#movie-selection-north-by-northwest-1959",
    "href": "mp02.html#movie-selection-north-by-northwest-1959",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Movie Selection: North by Northwest (1959)",
    "text": "Movie Selection: North by Northwest (1959)\nBased on the table analysis, we have chosen North by Northwest (1959) for the following key reasons:\n\nThe movie has not been remade: This classic suspense thriller remains untouched, providing a unique opportunity for a fresh adaptation. Modern audiences might appreciate a new take on the original storyline.\nSimplified rights acquisition:\n\nMost of the original actors and director are no longer living, which may simplify the process of securing rights.\nThis could make the adaptation process faster and less complex from a legal standpoint.\n\n\nA remake of this iconic film can resonate well with contemporary audiences, who may find its themes of suspense, intrigue, and mistaken identity just as compelling today as they were in 1959. A new adaptation, with modern cinematic techniques and storytelling, has the potential to breathe new life into this timeless thriller and mistery.\n\n# Extract genres for \"North by Northwest\"\ngenres &lt;- TITLE_BASICS |&gt;\n  filter(primaryTitle == \"North by Northwest\") |&gt;\n  select(genres)\n\n# Select \"North by Northwest\" for the remake and display relevant info\nnorth &lt;- my_movie |&gt;\n  filter(primaryTitle == \"North by Northwest\") |&gt;\n  select(directors,\n         writers,\n         primaryName) |&gt;\n  distinct()  # Ensure no duplicate rows are included\n\n# Display the selected movie info in a nicely formatted table\nnorth |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Selected Movie for Remake: North by Northwest (1959)\"\n  )\n\n\n\n\n\n\n\nSelected Movie for Remake: North by Northwest (1959)\n\n\ndirectors\nwriters\nprimaryName\n\n\n\n\nnm0000033\nnm0499626,nm0222985\nCary Grant\n\n\nnm0000033\nnm0499626,nm0222985\nEva Marie Saint\n\n\nnm0000033\nnm0499626,nm0222985\nJames Mason\n\n\nnm0000033\nnm0499626,nm0222985\nJessie Royce Landis\n\n\nnm0000033\nnm0499626,nm0222985\nLeo G. Carroll\n\n\nnm0000033\nnm0499626,nm0222985\nJosephine Hutchinson\n\n\nnm0000033\nnm0499626,nm0222985\nPhilip Ober\n\n\nnm0000033\nnm0499626,nm0222985\nMartin Landau\n\n\nnm0000033\nnm0499626,nm0222985\nAdam Williams\n\n\nnm0000033\nnm0499626,nm0222985\nEdward Platt\n\n\nnm0000033\nnm0499626,nm0222985\nAlfred Hitchcock\n\n\nnm0000033\nnm0499626,nm0222985\nErnest Lehman\n\n\nnm0000033\nnm0499626,nm0222985\nBernard Herrmann\n\n\nnm0000033\nnm0499626,nm0222985\nRobert Burks\n\n\nnm0000033\nnm0499626,nm0222985\nGeorge Tomasini\n\n\nnm0000033\nnm0499626,nm0222985\nRobert F. Boyle\n\n\n\n\n\n\n\nBased on the Crew group for this movie, the only one alive is Eva Marie Saint which today has 100 years, which would be a good idea to include her in the movie, to give a sense of continuity with the original movie."
  },
  {
    "objectID": "mp02.html#proposal-for-remake-of-north-by-northwest",
    "href": "mp02.html#proposal-for-remake-of-north-by-northwest",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Proposal for Remake of North by Northwest",
    "text": "Proposal for Remake of North by Northwest\nWe propose a modern remake of Alfred Hitchcock’s 1959 classic North by Northwest, a thrilling action-adventure about mistaken identity and espionage. This project capitalizes on the growing popularity of the action, adventure, and thriller genres, all of which have seen strong growth over the past decade.\nChris Evans (success ratio 0.5, 12 films) will play the lead, Roger Thornhill. His global popularity from blockbuster franchises like Captain America guarantees mass appeal, particularly among younger audiences. Hugo Weaving (100% success ratio, 4 films) will play the sophisticated villain, bringing depth and menace, as he has done in The Matrix and The Lord of the Rings. Clark Gregg (100% success ratio, 4 films) will take on a pivotal supporting role, adding gravitas as a shadowy government figure.\nDavid Yates, with a 60% success ratio, is our chosen director. Known for delivering hits like Harry Potter and Fantastic Beasts, his ability to handle large-scale action-adventure films with complex narratives ensures this project will be a success.\nThe consistent rise of the action-adventure genre, paired with the recent resurgence of thrillers, positions North by Northwest perfectly for a remake. Our analysis shows that these genres have seen significant growth, and tapping into this trend with a strong cast and proven director will maximize our movie’s potential.\nWe project the film to gross $600 to $800 million globally, appealing to both classic film fans and modern action audiences."
  },
  {
    "objectID": "mp02.html#conclusion",
    "href": "mp02.html#conclusion",
    "title": "STA/OPR 9750 Mini-Project #02: The Business of Show Business",
    "section": "Conclusion",
    "text": "Conclusion\nThis project has provided valuable insights into the film industry, focusing on the success of movies based on IMDb ratings, number of votes, and revenue. We have developed a model to predict the success of a movie based on these factors, and we have validated the model through various spot checks and analyses. We have also explored the evolution of genres over time, identifying trends and patterns that can inform future movie production. also, the creation of our own movie has been a great experience, and we are confident that our proposed remake of North by Northwest has the potential to be a success ."
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "",
    "text": "Date: November 12th, 2024\nAuthor: Juan Camilo Martinez\n\n\n\nIn the United States, the President isn’t chosen directly by popular vote. Instead, the winner is determined through the Electoral College, a system in which each state’s influence is tied to its congressional representation. With the 2024 Presidential election drawing near, it’s timely to ask: Would a different allocation method for electoral votes result in election outcomes that better represent the national popular vote?\nThis project dives into this question, analyzing historical congressional and presidential election data from MIT’s Election Data Science Lab and UCLA Congressional Boundary Files. By exploring electoral vote distributions under different allocation rules, we aim to see how alternate methods might change election results.\n\n\n\n\n\n\nThe U.S. Constitution sets the foundation of the Electoral College, granting each state a number of electors equal to its congressional delegation: the number of House representatives plus two senators. Most states follow a winner-take-all rule, where the state’s popular vote winner claims all electoral votes. However, states like Nebraska and Maine use a district-wide allocation method, awarding votes by individual district results, with two additional votes for the statewide popular winner.\nIn this analysis, we’ll test four allocation strategies to see their potential impact: 1. State-Wide Winner-Take-All 2. District-Wide Winner-Take-All with At-Large Votes 3. State-Wide Proportional 4. National Proportional\n\n\n\n\n\n\n\nWe start by loading the necessary R packages for data manipulation and visualization.\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(stringr)\nlibrary(statebins)\nlibrary(scales)\n**Data Sources and Import\nThe project uses two primary datasets:\nU.S. House Election Data (1976–2022): Details vote counts from congressional races across the 50 states. U.S. Presidential Election Data (1976–2020): Provides state-level vote counts for presidential elections.\nDATA_HOUSE &lt;- read_csv(\"1976-2022-house.csv\") |&gt; filter(!is.na(party))\nDATA_PRESIDENT &lt;- read_csv(\"1976-2020-president.csv\") |&gt;  filter(!is.na(candidate) & !is.na(party_detailed))\nAdditionally, we download congressional boundary files for spatial analysis, covering elections from 1976 to 2012 from UCLA and from 2013 to 2023 from the U.S. Census Bureau.\nus_shapefiles &lt;- function(start = 95, end = 112) {\n  BASE_URL &lt;- \"https://cdmaps.polisci.ucla.edu/shp/districts\"\n  \ncongress_shapefiles_census &lt;- function(start_year = 2013, end_year = 2023) {\n  BASE_URL &lt;- \"https://www2.census.gov/geo/tiger/TIGER\""
  },
  {
    "objectID": "mp03.html#introduction",
    "href": "mp03.html#introduction",
    "title": "# Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Introduction",
    "text": "Introduction\nIn the United States, the President isn’t chosen directly by popular vote. Instead, the winner is determined through the Electoral College, a system in which each state’s influence is tied to its congressional representation. With the 2024 Presidential election drawing near, it’s timely to ask: Would a different allocation method for electoral votes result in election outcomes that better represent the national popular vote?\nThis project dives into this question, analyzing historical congressional and presidential election data from MIT’s Election Data Science Lab and UCLA Congressional Boundary Files. By exploring electoral vote distributions under different allocation rules, we aim to see how alternate methods might change election results."
  },
  {
    "objectID": "mp03.html#background",
    "href": "mp03.html#background",
    "title": "# Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Background",
    "text": "Background\n\nHow the Electoral College Works\nThe U.S. Constitution sets the foundation of the Electoral College, granting each state a number of electors equal to its congressional delegation: the number of House representatives plus two senators. Most states follow a winner-take-all rule, where the state’s popular vote winner claims all electoral votes. However, states like Nebraska and Maine use a district-wide allocation method, awarding votes by individual district results, with two additional votes for the statewide popular winner.\nIn this analysis, we’ll test four allocation strategies to see their potential impact: 1. State-Wide Winner-Take-All 2. District-Wide Winner-Take-All with At-Large Votes 3. State-Wide Proportional 4. National Proportional"
  },
  {
    "objectID": "mp03.html#data-collection",
    "href": "mp03.html#data-collection",
    "title": "# Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Data Collection",
    "text": "Data Collection\n\nSetting Up the Libraries and Data\nWe start by loading the necessary R packages for data manipulation and visualization.\n\n\nShow the code\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(stringr)\nlibrary(statebins)\nlibrary(scales)\nlibrary(usmap)\nlibrary(gt)\nlibrary(DT)\nlibrary(gganimate)\nlibrary(maps)\nlibrary(gifski)"
  },
  {
    "objectID": "mp03.html#data-sources-and-import",
    "href": "mp03.html#data-sources-and-import",
    "title": "# Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Data Sources and Import",
    "text": "Data Sources and Import\nThe project uses two primary datasets:\nU.S. House Election Data (1976–2022): Details vote counts from congressional races across the 50 states. U.S. Presidential Election Data (1976–2020): Provides state-level vote counts for presidential elections.\n\n\nShow the code\nDATA_HOUSE &lt;- read_csv(\"1976-2022-house.csv\") |&gt; filter(!is.na(party))\nDATA_PRESIDENT &lt;- read_csv(\"1976-2020-president.csv\") |&gt;  filter(!is.na(candidate) & !is.na(party_detailed))\n\n\nAdditionally, we download congressional boundary files for spatial analysis, covering elections from 1976 to 2012 from UCLA and from 2013 to 2023 from the U.S. Census Bureau.\nFirst, we download the UCLA congressional boundary files.\n\n\nShow the code\ncongress_shapefiles_ucla &lt;- function(start = 95, end = 112) {\n  BASE_URL &lt;- \"https://cdmaps.polisci.ucla.edu/shp/districts\"\n  \n  # Create directory if it doesn't exist\n  target_dir &lt;- \"data/congress_shapefiles\"\n  if (!dir.exists(target_dir)) {\n    dir.create(target_dir, recursive = TRUE)\n  }\n  \n  for (congress in start:end) {\n    congress_str &lt;- sprintf(\"%03d\", congress)\n    file_url &lt;- paste0(BASE_URL, congress_str, \".zip\")\n    dest_file &lt;- file.path(target_dir, paste0(\"congress_\", congress_str, \"_shapefile.zip\"))\n    \n    # Avoid re-downloading\n    if (!file.exists(dest_file)) {\n      tryCatch({\n        download.file(file_url, destfile = dest_file, mode = \"wb\")\n        # Check if the file is downloaded successfully\n        if (file.size(dest_file) &gt; 0) {\n          message(\"Successfully downloaded shapefile for Congress \", congress_str)\n        } else {\n          file.remove(dest_file)\n          message(\"Download failed for Congress \", congress_str, \". File was empty and deleted.\")\n        }\n      }, error = function(e) {\n        message(\"Error downloading for Congress \", congress_str, \": \", e)\n      })\n    } else {\n      message(\"File for Congress \", congress_str, \" already exists. Skipping download.\")\n    }\n  }\n}\n\n#function\ncongress_shapefiles_ucla(93,112)\n\n\nNow, we download the U.S. Census Bureau congressional boundary files.\n\n\nShow the code\n# Define function to download, unzip, and read a shapefile\ndownload_and_read_shapefile &lt;- function(year, congress_num) {\n  # Set base URL and target directory\n  base_url &lt;- sprintf(\"https://www2.census.gov/geo/tiger/TIGER%d/CD/\", year)\n  target_dir &lt;- \"data/congress_shapefiles\"\n  \n  # Create directory if it doesn't exist\n  if (!dir.exists(target_dir)) dir.create(target_dir, recursive = TRUE)\n  \n  # Construct filename and paths\n  file_name &lt;- sprintf(\"tl_%d_us_cd%d\", year, congress_num)\n  zip_file &lt;- file.path(target_dir, paste0(file_name, \".zip\"))\n  unzip_dir &lt;- file.path(target_dir, file_name)\n  shapefile_path &lt;- file.path(unzip_dir, paste0(file_name, \".shp\"))\n  \n  # Download the file if it doesn't exist\n  if (!file.exists(zip_file)) {\n    file_url &lt;- paste0(base_url, file_name, \".zip\")\n    tryCatch({\n      download.file(file_url, destfile = zip_file, mode = \"wb\")\n      if (file.size(zip_file) &gt; 0) {\n        message(\"Downloaded: \", file_name, \" for year \", year)\n      } else {\n        file.remove(zip_file)\n        message(\"Download failed for \", file_name, \". Empty file removed.\")\n      }\n    }, error = function(e) {\n      message(\"Error downloading \", file_name, \": \", e$message)\n    })\n  }\n  \n  # Unzip and read the shapefile if it hasn’t been unzipped already\n  if (file.exists(zip_file) && !file.exists(shapefile_path)) {\n    unzip(zipfile = zip_file, exdir = unzip_dir)\n  }\n  \n  # Load the shapefile if it exists\n  if (file.exists(shapefile_path)) {\n    return(read_sf(shapefile_path))\n  } else {\n    message(\"Shapefile not found for \", file_name, \" in year \", year)\n    return(NULL)\n  }\n}\n# Iterate over years and download shapefiles based on Congress sessions\nbase_year &lt;- 2022\nfor (i in 0:10) {\n  year &lt;- base_year - i\n  \n  # Determine Congress number based on the year\n  congress &lt;- if (year &gt;= 2018) 116\n              else if (year &gt;= 2016) 115\n              else if (year &gt;= 2014) 114\n              else if (year == 2013) 113\n              else if (year == 2012) 112\n              else NA\n  \n  if (!is.na(congress)) {\n    district_name &lt;- sprintf(\"tl_%d_us_cd%d\", year, congress)\n    \n    # Download and read shapefile\n    district_data &lt;- download_and_read_shapefile(year, congress)\n    \n    # Assign the data to a unique variable in the global environment\n    if (!is.null(district_data)) {\n      assign(district_name, district_data, envir = .GlobalEnv)\n    }\n  } else {\n    message(\"Congress data not available for year \", year)\n  }\n}\n\n\n\nInitial Exploration of Vote Count Data\nAnalyze Seat Gains and Losses in the U.S. House (1976-2022) This code calculates which states gained or lost the most seats in the House of Representatives between 1976 and 2022.\n\n\nShow the code\n# Filter House data for relevant years and calculate seat counts\nhouse_seats_over_time &lt;- DATA_HOUSE |&gt;\n  filter(year %in% c(1976, 2022)) |&gt;\n  distinct(year, state, district) |&gt;\n  group_by(year, state) |&gt;\n  summarise(total_seats = n(), .groups = \"drop\")\n\n# Calculate seat changes from 1976 to 2022\nseat_changes &lt;- house_seats_over_time |&gt;\n  pivot_wider(names_from = year, values_from = total_seats, names_prefix = \"year_\") |&gt;\n  mutate(seat_change = year_2022 - year_1976) |&gt;\n  arrange(desc(seat_change))\n\n# Identify top 5 states with highest seat gains and losses\ntop_gained_states &lt;- seat_changes |&gt; slice_max(seat_change, n = 5)\ntop_lost_states &lt;- seat_changes |&gt; slice_min(seat_change, n = 5)\n\n# Plot top gained seats\nplot_seat_gains &lt;- ggplot(top_gained_states, aes(x = reorder(state, seat_change), y = seat_change, fill = seat_change)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Top 5 States with Seat Gains (1976-2022)\",\n       x = \"State\",\n       y = \"Number of Seats Gained\") +\n  scale_fill_gradient(low = \"lightblue\", high = \"darkblue\") +\n  theme_minimal()\n\n# Plot top lost seats\nplot_seat_losses &lt;- ggplot(top_lost_states, aes(x = reorder(state, seat_change), y = seat_change, fill = -seat_change)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Top 5 States with Seat Losses (1976-2022)\",\n       x = \"State\",\n       y = \"Number of Seats Lost\") +\n  scale_fill_gradient(low = \"lightcoral\", high = \"darkred\") +\n  theme_minimal()\n\n# Display both plots\nlist(plot_seat_gains, plot_seat_losses)\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\nAs we can see in this graph, the states that gained the most seats are Texas, Florida, California, Arizona, and Georgia. On the other hand, the states that lost the most seats are New York, Ohio, Pennsylvania, Illinois, Ohio, and Michigan. The following table explains the population growth between 1976 to 2022 shifts are the main reason for these changes.\n\n\n\nLocation\nGrowth Percentage\n\n\n\n\nArizona\n192.0%\n\n\nFlorida\n161.9%\n\n\nTexas\n145.9%\n\n\nGeorgia\n125.0%\n\n\nCalifornia\n77.3%\n\n\nUSA\n52.8%\n\n\nIllinois\n12.5%\n\n\nPennsylvania\n10.2%\n\n\nNew York\n10.1%\n\n\nOhio\n9.3%\n\n\nMichigan\n8.1%\n\n\n\nWe see clearly that the Usa is in the middle of the list, and the states that lost the most seats are in the bottom of the list.\n\n\nShow the code\n# Filter for New York State and relevant House races\nny_house_data &lt;- DATA_HOUSE |&gt;\n  filter(state == \"NEW YORK\", office == \"US HOUSE\")\n\n# Calculate total votes (with fusion) and major party votes (without fusion)\nfusion_analysis &lt;- ny_house_data |&gt;\n  mutate(is_major_party = party %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  group_by(year, district, candidate) |&gt;\n  summarise(\n    total_votes_all_lines = sum(candidatevotes, na.rm = TRUE),\n    major_party_votes = sum(candidatevotes[is_major_party], na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n# Determine winners by both fusion and non-fusion scenarios\nwinners_fusion &lt;- fusion_analysis |&gt;\n  group_by(year, district) |&gt;\n  filter(total_votes_all_lines == max(total_votes_all_lines)) |&gt;\n  select(year, district, candidate, total_votes_all_lines) |&gt;\n  rename(fusion_winner = candidate, fusion_votes = total_votes_all_lines)|&gt;\n  ungroup()\n\nwinners_nonfusion &lt;- fusion_analysis |&gt;\n  group_by(year, district) |&gt;\n  filter(major_party_votes == max(major_party_votes)) |&gt;\n  select(year, district, candidate, major_party_votes) |&gt;\n  rename(nonfusion_winner = candidate, nonfusion_votes = major_party_votes) |&gt;\n  ungroup()\n\n# Find elections where fusion changed the winner\nelection_outcomes &lt;- winners_fusion |&gt;\n  inner_join(winners_nonfusion, by = c(\"year\", \"district\")) |&gt;\n  filter(fusion_winner != nonfusion_winner) |&gt;\n  arrange(desc(year))\n  \n\ndatatable(\n  election_outcomes,\n  options = list(\n    pageLength = 10,         # Rows per page\n    autoWidth = TRUE,        # Adjust columns automatically\n    dom = 'tip',             # Only show table, info, and pagination\n    lengthMenu = c(5, 10, 15, 20)  # Options for rows per page\n  ),\n  rownames = FALSE           # Hide row names\n)\n\n\n\n\n\n\nAs we can see, fusion voting can be a deciding factor in elections. The table demonstrates that fusion voting increases the likelihood of being elected by enabling candidates to gather votes from multiple party lines. This consolidates support in a way that would not be possible without fusion, giving fusion candidates a strategic advantage.\n\n\nAnalyzing Presidential vs. Congressional Vote Patterns Across Parties\nThis analysis explores whether presidential candidates tend to run ahead of or behind their congressional counterparts in the same state. Specifically, we’re investigating if Democratic and Republican presidential candidates receive more votes in a given state than all congressional candidates from their party in that same state.\nThe following code will compare these vote counts, identifying instances where presidential candidates either outperformed (“ahead”) or underperformed (“behind”) their co-partisans. This will also help us understand if this trend varies over time, across states, or between parties, providing insights into the relative popularity of presidential candidates within each party.\n\n\nShow the code\n# Aggregate Democratic and Republican presidential votes by state and year\npresidential_votes &lt;- DATA_PRESIDENT |&gt;\n  filter(office == \"US PRESIDENT\", party_simplified %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  group_by(year, state, party_simplified) |&gt;\n  summarise(total_pres_votes = sum(candidatevotes, na.rm = TRUE), .groups = \"drop\")\n\n# Aggregate Democratic and Republican congressional votes by state and year\ncongressional_votes &lt;- DATA_HOUSE |&gt;\n  filter(office == \"US HOUSE\", party %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  group_by(year, state, party) |&gt;\n  summarise(total_congress_votes = sum(candidatevotes, na.rm = TRUE), .groups = \"drop\") |&gt;\n  rename(party_simplified = party)\n\n# Merge the presidential and congressional vote data\nvote_comparison &lt;- presidential_votes |&gt;\n  inner_join(congressional_votes, by = c(\"year\", \"state\", \"party_simplified\")) |&gt;\n  mutate(\n    vote_difference = total_pres_votes - total_congress_votes,\n    ran_ahead = ifelse(vote_difference &gt; 0, \"Ahead\", \"Behind\")\n  )\n\n# Additional Visualization: Calculate the average vote difference per year across all states\navg_vote_diff &lt;- vote_comparison |&gt;\n  group_by(year, party_simplified) |&gt;\n  summarise(avg_vote_difference = mean(vote_difference, na.rm = TRUE), .groups = \"drop\")\n\n# Line chart for average vote difference over time\nggplot(avg_vote_diff, aes(x = year, y = avg_vote_difference, color = party_simplified)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Average Vote Difference Between Presidential and Congressional Candidates\",\n    subtitle = \"Across All States, by Party (1976 - 2020)\",\n    x = \"Year\",\n    y = \"Average Vote Difference (Presidential - Congressional)\",\n    color = \"Party\"\n  ) +\n  scale_color_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\")) +\n\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nAs we can see in the image, the average vote difference between presidential and congressional candidates has fluctuated over the years. On this, we can see a major fluctuation in the Republican party, what indicate that the presidential candidate has a major influence on votes than the congressional candidates. For Democrats, the difference is not that big, but still, the presidential candidate has a major influence on votes than the congressional candidates.\n\n\nShow the code\n# Summary of total instances where presidential candidates ran ahead or behind, grouped by party\nparty_summary_stats &lt;- vote_comparison |&gt;\n  group_by(party_simplified) |&gt;\n  summarise(\n    total_ahead = sum(ran_ahead == \"Ahead\"),\n    total_behind = sum(ran_ahead == \"Behind\"),\n    Difference = total_ahead - total_behind,  # Calculate the variance between ahead and behind\n    .groups = \"drop\"\n  )\n\n# Display the summary table\nparty_summary_stats |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Summary of Presidential Candidates Running Ahead or Behind Congressional Candidates\",\n    subtitle = \"Total Instances by Party (1976-2020)\"\n  ) |&gt;\n  cols_label(\n    party_simplified = \"Party\",\n    total_ahead = \"Total States (Ahead)\",\n    total_behind = \"Total States (Behind)\",\n    Difference = \"Difference (Ahead - Behind)\"\n  )\n\n\n\n\n\n\n\n\nSummary of Presidential Candidates Running Ahead or Behind Congressional Candidates\n\n\nTotal Instances by Party (1976-2020)\n\n\nParty\nTotal States (Ahead)\nTotal States (Behind)\nDifference (Ahead - Behind)\n\n\n\n\nDEMOCRAT\n318\n274\n44\n\n\nREPUBLICAN\n390\n207\n183\n\n\n\n\n\n\n\nFinally, we can observe the difference between the number of states where the presidential candidate ran ahead or behind their congressional counterparts. The table shows that both Democratic and Republican presidential candidates have run ahead more than behind, however, Republicans have a higher difference between the two categories. This indicates that Republican presidential candidates tend to outperform their congressional counterparts more frequently than Democratic candidates.\n\n\nImporting and Plotting Shapefile Data\nThe shapefiles we downloaded are provided in zip archives containing several files, but we only need the .shp file from each archive. In this section, we’ll walk through extracting the .shp file, loading it into R, and creating a plot from the data. The main tool we’ll use is the sf package, specifically the read_sf() function, which allows us to read shapefiles directly into R. Below, I’ll demonstrate how this process works.\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(sf)\n\nif(!file.exists(\"nyc_borough_boundaries.zip\")){\n    download.file(\"https://data.cityofnewyork.us/api/geospatial/tqmj-j8zm?method=export&format=Shapefile\", \n              destfile=\"nyc_borough_boundaries.zip\")\n}\n\n##-\ntd &lt;- tempdir(); \nzip_contents &lt;- unzip(\"nyc_borough_boundaries.zip\", \n                      exdir = td)\n    \nfname_shp &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\nnyc_sf &lt;- read_sf(fname_shp)\n\n\nWith the following code, we can plot the shapefile data to visualize the boundaries of New York City boroughs.\n\n\nShow the code\nread_shp_from_zip &lt;- function(zip_file) {\n  files_in_zip &lt;- unzip(zip_file, list = TRUE)\n  \n  shp_file &lt;- files_in_zip$Name[grepl(\"\\\\.shp$\", files_in_zip$Name)]\n  \n  if (length(shp_file) == 1) {\n    unzip(zip_file, files = shp_file, exdir = tempdir())\n    \n    shp_path &lt;- file.path(tempdir(), shp_file)\n    \n    shape_data &lt;- read_sf(shp_path)\n    \n    return(shape_data)\n  } else {\n    message(\"No .shp file found in the provided zip archive.\")\n    return(NULL)\n  }\n}\n\n\nNow we would plot the shapefile data to visualize the boundaries of New York City boroughs.\n\n\nShow the code\nggplot(nyc_sf, \n       aes(geometry=geometry)) + \n    geom_sf()\n\n\n\n\n\n\n\n\n\nAlso we can plot the shapefile data with a fill color to visualize the boundaries of New York City boroughs.\n\n\nShow the code\nggplot(nyc_sf, \n       aes(geometry=geometry, \n           fill = shape_area)) + \n    geom_sf()\n\n\n\n\n\n\n\n\n\n\n\nElectoral College Results in 2000\nIn this section, we will create a choropleth map to visualize the results of the 2000 U.S. Presidential Election. The map will display the winning party in each state, highlighting the electoral college winner between George W. Bush and Al Gore.\n\n\nShow the code\n# Step 1: Prepare Election Data\nelection_2000 &lt;- DATA_PRESIDENT |&gt;\n  filter(year == 2000, office == \"US PRESIDENT\") |&gt;\n  group_by(state) |&gt;\n  slice_max(order_by = candidatevotes, n = 1) |&gt;\n  ungroup()\n\n# Step 2: Assign Winning Party\nelection_2000 &lt;- election_2000 |&gt;\n  mutate(\n    party_winner = ifelse(party_simplified == \"REPUBLICAN\", \"Republican\", \"Democrat\"),\n    state = tolower(state)\n  )\n\n# Step 3: Define Electoral College Votes (EC)\nec_votes &lt;- data.frame(\n  state_po = c(\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"DC\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\", \"IN\", \"IA\",\n               \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \"NM\",\n               \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\",\n               \"WV\", \"WI\", \"WY\"),\n  EC = c(9, 3, 8, 6, 54, 8, 8, 3, 3, 25, 13, 4, 4, 22, 12, 7, 6, 8, 9, 4, 10, 12, 18, 10, 7, 11, 3, 5, 4, 4, 15, 5,\n         33, 14, 3, 21, 8, 7, 23, 4, 8, 3, 11, 32, 5, 3, 13, 11, 5, 11, 3)\n)\n\n# Step 4: Merge EC into election data\nelection_2000 &lt;- election_2000 |&gt;\n  left_join(ec_votes, by = \"state_po\")\n\n\n# Step 5: Prepare Map Data\nus_map &lt;- map_data(\"state\") |&gt;\n  mutate(region = tolower(region))\n\n# Step 6: Merge election data with map data\nmap_data_with_results &lt;- us_map |&gt;\n  left_join(election_2000, by = c(\"region\" = \"state\"))\n\n# Step 7: Calculate State Centers for Labels\nstate_centers &lt;- map_data_with_results |&gt;\n  group_by(region) |&gt;\n  summarize(\n    long_center = mean(range(long, na.rm = TRUE)),\n    lat_center = mean(range(lat, na.rm = TRUE)),\n    state_po = first(state_po),\n    EC = first(EC)\n  ) |&gt;\n  ungroup()\n\n# Step 8: Define Offset Labels for Northeastern States\nnortheast_labels &lt;- data.frame(\n  state_po = c(\"NH\", \"VT\", \"MA\", \"RI\", \"CT\", \"NJ\", \"DE\", \"MD\", \"DC\"),\n  long_offset = c(5, 4, 7, 6, 5, 6, 5, 6, 4),\n  lat_offset = c(2, 1, -1, 0, -2, -1, -1, -1, -2)\n)\n\n# Join offset labels with state centers to add adjusted coordinates\nstate_centers &lt;- state_centers |&gt;\n  left_join(northeast_labels, by = \"state_po\") |&gt;\n  mutate(\n    long_center_adjusted = ifelse(!is.na(long_offset), long_center + long_offset, long_center),\n    lat_center_adjusted = ifelse(!is.na(lat_offset), lat_center + lat_offset, lat_center)\n  )\n\n# Prepare connector line data for northeastern states\nnortheast_connectors &lt;- state_centers |&gt;\n  filter(!is.na(long_offset) & !is.na(lat_offset))\n\n# Step 9: Plot the Map with Insets and Connectors\nggplot(map_data_with_results) +\n  # Mainland US\n  geom_polygon(aes(x = long, y = lat, group = group, fill = party_winner), color = \"white\") +\n  # Alaska inset\n  geom_polygon(data = subset(map_data_with_results, region == \"alaska\"),\n               aes(x = long - 35, y = lat + 10, group = group, fill = party_winner), color = \"white\") +\n  # Hawaii inset\n  geom_polygon(data = subset(map_data_with_results, region == \"hawaii\"),\n               aes(x = long + 50, y = lat - 5, group = group, fill = party_winner), color = \"white\") +\n  # Party color scale\n  scale_fill_manual(\n    values = c(\"Democrat\" = \"blue\", \"Republican\" = \"red\"),\n    name = \"2000 Election Winner\",\n    labels = c(\"Democrat\" = \"Gore\", \"Republican\" = \"Bush\"),\n    na.value = \"grey\"\n  ) +\n  # Labels for each state with offset adjustments for northeastern states\n  geom_text(\n    data = state_centers,\n    aes(x = long_center_adjusted, y = lat_center_adjusted, label = paste(state_po, EC)),\n    color = \"black\",\n    size = 3,\n    fontface = \"bold\"\n  ) +\n  # Connector lines for northeastern states\n  geom_segment(data = northeast_connectors, \n               aes(x = long_center, y = lat_center, xend = long_center_adjusted, yend = lat_center_adjusted), \n               color = \"black\", linetype = \"solid\") +\n  # Map labels and theme\n  labs(\n    title = \"2000 Presidential Election Results by State\",\n    subtitle = \"Bush vs. Gore\",\n    caption = \"Data Source: MIT Election Data Science Lab\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(hjust = 0.5, size = 16),\n    plot.subtitle = element_text(hjust = 0.5, size = 12)\n  )\n\n\n\n\n\n\n\n\n\nNow we can see a clear picture of how the 2000 U.S. Presidential Election played out across the country. The map shows the winning party in each state, highlighting the electoral college winner between George W. Bush and Al Gore. This visualization provides a snapshot of the election results and the distribution of electoral votes across the United States.\n\n\nAnimated Map of U.S. Presidential Election Results (1976-2020)\nAfter this we want to visualize the results of all the U.S. Presidential Elections from 1976 to 2020. The following code will create a faceted map showing the winning party in each state for each election year, along with the corresponding electoral college votes.\n\n\nShow the code\n# Step 1: Prepare Election Data (for multiple years)\nelection_data &lt;- DATA_PRESIDENT |&gt;\n  filter(office == \"US PRESIDENT\") |&gt;\n  group_by(year, state) |&gt;\n  slice_max(order_by = candidatevotes, n = 1) |&gt;\n  ungroup() |&gt;\n  mutate(\n    party_winner = ifelse(party_simplified == \"REPUBLICAN\", \"Republican\", \"Democrat\"),\n    state = tolower(state)  # Ensure compatibility with map data\n  )\n\n# Step 2: Prepare Map Data\nus_map &lt;- map_data(\"state\") |&gt;\n  mutate(region = tolower(region))\n\n# Step 3: Merge Election Data with Map Data\nmap_data_with_results &lt;- us_map |&gt;\n  left_join(election_data, by = c(\"region\" = \"state\"))\n\n# Step 4: Plot the Animated Map with Explicit Colors\nanimated_map &lt;- ggplot(map_data_with_results) +\n  geom_polygon(aes(x = long, y = lat, group = group, fill = party_winner), color = \"white\") +\n  scale_fill_manual(\n    values = c(\"Democrat\" = \"#0000FF\", \"Republican\" = \"#FF0000\"),  # Explicit blue and red\n    name = \"Election Winner\",\n    na.value = \"grey\"  # Handle unexpected NA values as grey\n  ) +\n  labs(\n    title = \"Presidential Election Results by State Over Time\",\n    subtitle = 'Year: {frame_time}',\n    caption = \"Data Source: MIT Election Data Science Lab\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(hjust = 0.5, size = 16),\n    plot.subtitle = element_text(hjust = 0.5, size = 12)\n  ) +\n  transition_time(year) +   # Add animation over time\n  ease_aes('linear')        # Smooth animation\n\n# Render the animation as before\n#animate(animated_map,nframes = 100,fps = 10,width = 800,height = 600, renderer = gifski_renderer(\"election_animation_high_res.gif\"))\n\nknitr::include_graphics(\"election_animation_high_res.gif\")"
  },
  {
    "objectID": "mp03.html#comparing-the-effects-of-ecv-allocation-rules",
    "href": "mp03.html#comparing-the-effects-of-ecv-allocation-rules",
    "title": "# Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Comparing the Effects of ECV Allocation Rules",
    "text": "Comparing the Effects of ECV Allocation Rules\nNow, we are finishing the exploration of the data, we can compare the effects of different Electoral College Vote (ECV).Go through the historical voting data and assign each state’s ECVs according to various strategies:\nState-Wide Winner-Take-All District-Wide Winner-Take-All + State-Wide “At Large” Votes State-Wide Proportional National Proportional\nFirst, we would join the Electoral College votes to the presidential data previously assigned.\n\n\nShow the code\n# Join the Electoral College votes to the presidential data\npres_data_filtered &lt;- DATA_PRESIDENT |&gt;\n  filter(party_simplified %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  left_join(ec_votes, by = \"state_po\")"
  },
  {
    "objectID": "mp03.html#state-wide-winner-take-all",
    "href": "mp03.html#state-wide-winner-take-all",
    "title": "# Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "State-Wide Winner-Take-All",
    "text": "State-Wide Winner-Take-All\n\n\nShow the code\n# Group by year and state to get the total votes for each candidate within each state\nstatewide_results &lt;- pres_data_filtered |&gt;\n  group_by(year, state, party_simplified) |&gt;\n  summarize(candidatevotes = sum(candidatevotes), EC = first(EC), .groups = \"drop\")\n\n# Determine the winner in each state and assign all ECVs to that candidate\nstatewide_winners &lt;- statewide_results |&gt;\n  group_by(year, state) |&gt;\n  filter(candidatevotes == max(candidatevotes)) |&gt;\n  ungroup()\n\n# Sum ECVs for each party by year\nstatewide_winners_ecv &lt;- statewide_winners |&gt;\n  group_by(year, party_simplified) |&gt;\n  summarize(ecv_total = sum(EC), .groups = \"drop\")\n\n# Plot ECVs over time for State-Wide Winner-Take-All\nggplot(statewide_winners_ecv, aes(x = year, y = ecv_total, color = party_simplified)) +\n  geom_line(size = 1) +  # Slightly thicker line for better visibility\n  geom_point(size = 2) +  # Small points to highlight each year\n  labs(\n    title = \"Electoral College Votes Over Time (State-Wide Winner-Take-All)\",\n    x = \"Year\",\n    y = \"Total Electoral College Votes (ECV)\",\n    color = \"Party\"\n  ) +\n  scale_color_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\")) +  # Custom party colors\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\n\nThis fist plot shows the ECVs over time for the State-Wide Winner-Take-All allocation method. We can see how the ECVs are distributed between the Democratic and Republican parties in each election year fluctuating over time. Each time a dot is above the other, it means that the party with the higher line won the election. The trends indicate that the differences between parties have decreased over time."
  },
  {
    "objectID": "mp03.html#district-wide-winner-take-all-state-wide-at-large-votes",
    "href": "mp03.html#district-wide-winner-take-all-state-wide-at-large-votes",
    "title": "# Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "District-Wide Winner-Take-All + State-Wide “At Large” Votes",
    "text": "District-Wide Winner-Take-All + State-Wide “At Large” Votes\n\n\nShow the code\n# Define the list of presidential election years\npresidential_years &lt;- seq(1976, 2020, by = 4)\n\n# Filter DATA_HOUSE and DATA_PRESIDENT to include only presidential election years\ndistrict_data &lt;- DATA_HOUSE |&gt;\n  filter(year %in% presidential_years)\n\npresident_data &lt;- DATA_PRESIDENT |&gt;\n  filter(year %in% presidential_years)\n\n# Step 1: Determine district-level winners for Democrats and Republicans only, and assign 1 ECV per district\ndistrict_winners &lt;- district_data |&gt;\n  filter(party %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  group_by(year, state, state_po, district, party) |&gt;\n  summarize(candidatevotes = sum(candidatevotes), .groups = \"drop\") |&gt;\n  group_by(year, state, state_po, district) |&gt;\n  filter(candidatevotes == max(candidatevotes)) |&gt;\n  ungroup() |&gt;\n  mutate(ECV = 1)  # Each district gets 1 ECV\n\n# Step 2: Calculate statewide winners for \"at-large\" ECVs for Democrats and Republicans only\nstatewide_winners &lt;- president_data |&gt;\n  filter(party_simplified %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  group_by(year, state, state_po, party_simplified) |&gt;\n  summarize(candidatevotes = sum(candidatevotes), .groups = \"drop\") |&gt;\n  group_by(year, state, state_po) |&gt;\n  filter(candidatevotes == max(candidatevotes)) |&gt;\n  ungroup() |&gt;\n  mutate(ECV = 2)  # Statewide winner gets 2 at-large ECVs\n\n# Step 3: Combine district and state-wide results, focusing only on Democrat and Republican results\ndistrict_plus_at_large &lt;- bind_rows(\n  district_winners |&gt; rename(party_simplified = party),\n  statewide_winners\n)\n\n# Step 4: Summarize ECV allocation by party and year\necv_allocation &lt;- district_plus_at_large |&gt;\n  group_by(year, party_simplified) |&gt;\n  summarize(total_ecv = sum(ECV), .groups = \"drop\")\n\n# Step 5: Verify total ECVs across all states per year to see how they differ from 538\ntotal_ecv_by_year &lt;- ecv_allocation |&gt;\n  group_by(year) |&gt;\n  summarize(yearly_total_ecv = sum(total_ecv))\n\n# Check for any year where the total ECVs do not equal 538\ntotal_ecv_check &lt;- total_ecv_by_year |&gt;\n  filter(yearly_total_ecv != 538)\n\ngt(total_ecv_check)\n\n\n\n\n\n\n\n\nyear\nyearly_total_ecv\n\n\n\n\n1976\n537\n\n\n1980\n537\n\n\n1984\n537\n\n\n1988\n537\n\n\n1992\n529\n\n\n1996\n537\n\n\n2000\n537\n\n\n2004\n537\n\n\n2008\n537\n\n\n2012\n537\n\n\n2016\n537\n\n\n\n\n\n\n\nThis this we prove that the total ECVs for each year are around 538, which is the total number of Electoral College votes. This confirms that the allocation method is working correctly and that the ECVs are being distributed as expected.\n\n\nShow the code\nggplot(ecv_allocation, aes(x = year, y = total_ecv, color = party_simplified)) +\n  geom_line(size = 1) +  # Slightly thicker line for better visibility\n  geom_point(size = 2) +  # Small points to emphasize each data point\n  labs(\n    title = \"Electoral College Votes Over Time (District-Wide + At-Large Allocation)\",\n    x = \"Year\",\n    y = \"Electoral College Votes (ECV)\",\n    color = \"Party\"\n  ) +\n  scale_color_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\")) +  # Custom party colors\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\n\nThis plot shows the ECVs over time for the District-Wide Winner-Take-All + State-Wide “At Large” Votes allocation method. We can see how the ECVs are distributed between the Democratic and Republican parties in each election year, fluctuating over time.\n#State-Wide Proportional\n\n\nShow the code\n# Step 1: Calculate vote share for Democrats and Republicans in each state\nstatewide_results &lt;- pres_data_filtered |&gt;\n  group_by(year, state, state_po, party_simplified, EC) |&gt;\n  summarize(candidatevotes = sum(candidatevotes), .groups = \"drop\") |&gt;\n  group_by(year, state, state_po) |&gt;\n  mutate(vote_share = candidatevotes / sum(candidatevotes)) |&gt;\n  ungroup()\n\n# Step 2: Allocate ECVs proportionally based on vote share\nstatewide_results &lt;- statewide_results |&gt;\n  mutate(proportional_ecv = round(vote_share * EC))  # Round to ensure integer ECVs\n\n# Step 3: Adjust ECVs to ensure they match the state's total (handle rounding errors)\n# For each state-year, adjust ECVs if the sum does not match the expected total\nstatewide_adjusted &lt;- statewide_results |&gt;\n  group_by(year, state, state_po) |&gt;\n  mutate(\n    ecv_adjustment = EC - sum(proportional_ecv),  # Calculate difference due to rounding\n    proportional_ecv = proportional_ecv + if_else(\n      row_number() == 1 & ecv_adjustment != 0, ecv_adjustment, 0\n    )  # Adjust the first party’s ECV by the remainder\n  ) |&gt;\n  ungroup()\n\n# Step 4: Summarize ECV allocation by party and year\necv_state_proportion &lt;- statewide_adjusted |&gt;\n  group_by(year, party_simplified) |&gt;\n  summarize(total_ecv = sum(proportional_ecv), .groups = \"drop\")\n\n# Step 5: Plot ECV vs Time for Democrats and Republicans (State-Wide Proportional)\nggplot(ecv_state_proportion, aes(x = year, y = total_ecv, color = party_simplified)) +\n  geom_line(size = 1) +  # Slightly thicker line for clarity\n  geom_point(size = 2) +  # Small points to emphasize each data point\n  labs(\n    title = \"Electoral College Votes Over Time (State-Wide Proportional)\",\n    x = \"Year\",\n    y = \"Electoral College Votes (ECV)\",\n    color = \"Party\"\n  ) +\n  scale_color_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\")) +  # Custom party colors\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\n\nWe can see a difference between this and the others methods, in this one we can see a clear advantage for the Democratic party, this is because the proportional allocation of ECVs is based on the vote share in each state. This method gives a more accurate representation of the popular vote in each state, which can lead to a more balanced distribution of ECVs between the two major parties."
  },
  {
    "objectID": "mp03.html#national-proportional",
    "href": "mp03.html#national-proportional",
    "title": "# Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "National Proportional",
    "text": "National Proportional\n\n\nShow the code\n# Step 1: Calculate the total national vote share for Democrats and Republicans\nnational_results &lt;- pres_data_filtered |&gt;\n  group_by(year, party_simplified) |&gt;\n  summarize(total_votes = sum(candidatevotes), .groups = \"drop\") |&gt;\n  group_by(year) |&gt;\n  mutate(national_vote_share = total_votes / sum(total_votes)) |&gt;\n  ungroup()\n\n# Step 2: Allocate ECVs based on national vote share\nnational_results &lt;- national_results |&gt;\n  mutate(proportional_ecv = round(national_vote_share * 538))  # Round to ensure integer ECVs\n\n# Step 3: Adjust ECVs to ensure they sum to 538 (handle rounding discrepancies)\n# Adjust any remainder due to rounding so that the sum equals 538\nnational_adjusted &lt;- national_results |&gt;\n  group_by(year) |&gt;\n  mutate(\n    ecv_adjustment = 538 - sum(proportional_ecv),  # Calculate difference due to rounding\n    proportional_ecv = proportional_ecv + if_else(\n      row_number() == 1 & ecv_adjustment != 0, ecv_adjustment, 0\n    )  # Adjust the first party's ECV by the remainder\n  ) |&gt;\n  ungroup()\n\n# Step 4: Plot ECV vs Time for Democrats and Republicans (National Proportional)\nggplot(national_adjusted, aes(x = year, y = proportional_ecv, color = party_simplified)) +\n  geom_line(size = 1) +  # Slightly thicker line for better visibility\n  geom_point(size = 2) +  # Add small points at each year for clarity\n  labs(\n    title = \"ECV Over Time by National Proportional Allocation\",\n    x = \"Year\",\n    y = \"Electoral College Votes (ECV)\",\n    color = \"Party\"\n  ) +\n  scale_color_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\")) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\n\nNow we would compared all this methods to see the differences between them.\n\n\nShow the code\n# Total number of elections\ntotal_elections &lt;- 12\n\n# Calculate the percentage of Republican wins for each method\n\n# State-Wide Winner-Take-All\nstatewide_republican_wins &lt;- statewide_winners_ecv |&gt;\n  group_by(year) |&gt;\n  summarize(winning_party = party_simplified[which.max(ecv_total)]) |&gt;\n  ungroup() |&gt;\n  count(winning_party) |&gt;\n  filter(winning_party == \"REPUBLICAN\") |&gt;\n  pull(n)\nstatewide_republican_percentage &lt;- round((statewide_republican_wins / total_elections) * 100,2)\nstatewide_democratic_percentage &lt;- 100 - statewide_republican_percentage\n\n# District-Wide Winner-Take-All + At-Large\ndistrict_republican_wins &lt;- ecv_allocation |&gt;\n  group_by(year) |&gt;\n  summarize(winning_party = party_simplified[which.max(total_ecv)]) |&gt;\n  ungroup() |&gt;\n  count(winning_party) |&gt;\n  filter(winning_party == \"REPUBLICAN\") |&gt;\n  pull(n)\ndistrict_republican_percentage &lt;- round((district_republican_wins / total_elections) * 100,2)\ndistrict_democratic_percentage &lt;- 100 - district_republican_percentage\n\n# State-Wide Proportional\nstate_proportional_republican_wins &lt;- ecv_state_proportion |&gt;\n  group_by(year) |&gt;\n  summarize(winning_party = party_simplified[which.max(total_ecv)]) |&gt;\n  ungroup() |&gt;\n  count(winning_party) |&gt;\n  filter(winning_party == \"REPUBLICAN\") |&gt;\n  pull(n)\nstate_proportional_republican_percentage &lt;- round((state_proportional_republican_wins / total_elections) * 100,2)\nstate_proportional_democratic_percentage &lt;- 100 - state_proportional_republican_percentage\n\n# National Proportional\nnational_proportional_republican_wins &lt;- national_adjusted |&gt;\n  group_by(year) |&gt;\n  summarize(winning_party = party_simplified[which.max(proportional_ecv)]) |&gt;\n  ungroup() |&gt;\n  count(winning_party) |&gt;\n  filter(winning_party == \"REPUBLICAN\") |&gt;\n  pull(n)\nnational_proportional_republican_percentage &lt;- round((national_proportional_republican_wins / total_elections) * 100, 2)\nnational_proportional_democratic_percentage &lt;- 100 - national_proportional_republican_percentage\n\n# Create a data frame to summarize the results\nwin_percentage_table &lt;- data.frame(\n  Party = c(\"REPUBLICAN\", \"DEMOCRAT\"),\n  `State-Wide Winner-Take-All` = c(statewide_republican_percentage, statewide_democratic_percentage),\n  `District-Wide + At-Large` = c(district_republican_percentage, district_democratic_percentage),\n  `State-Wide Proportional` = c(state_proportional_republican_percentage, state_proportional_democratic_percentage),\n  `National Proportional` = c(national_proportional_republican_percentage, national_proportional_democratic_percentage)\n)\n\ngt(win_percentage_table) \n\n\n\n\n\n\n\n\nParty\nState.Wide.Winner.Take.All\nDistrict.Wide...At.Large\nState.Wide.Proportional\nNational.Proportional\n\n\n\n\nREPUBLICAN\n50\n50\n33.33\n33.33\n\n\nDEMOCRAT\n50\n50\n66.67\n66.67"
  },
  {
    "objectID": "mp03.html#analysis-of-each-scheme",
    "href": "mp03.html#analysis-of-each-scheme",
    "title": "# Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Analysis of Each Scheme",
    "text": "Analysis of Each Scheme\nState-Wide Winner-Take-All: This widely used method awards all ECVs to the state-wide popular vote winner, often ignoring narrower vote margins. Result: Produced a balanced outcome, with each party winning 50% of elections (1976–2020), but often distorts national popular vote representation.\nDistrict-Wide Winner-Take-All + At-Large: ECVs are awarded by district, with two at-large votes for the state winner. Result: Also produced a 50/50 split, providing a slightly more detailed reflection of local preferences, but still favoring swing districts.\nState-Wide Proportional: ECVs are split based on each candidate’s vote share within a state, offering a fairer reflection of popular support. Result: Democrats won 66.7% of elections, indicating a tilt toward populous urban centers.\nNational Proportional: Allocates ECVs based on the national popular vote percentage. Result: Similar to State-Wide Proportional, with Democrats winning 66.7% of elections, showing a national popular vote bias."
  },
  {
    "objectID": "mp03.html#conclusion-the-fairest-scheme",
    "href": "mp03.html#conclusion-the-fairest-scheme",
    "title": "# Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Conclusion: The Fairest Scheme",
    "text": "Conclusion: The Fairest Scheme\nState-Wide Proportional is arguably the fairest, as it represents the actual vote distribution within each state. However, it significantly shifts outcomes, favoring Democrats due to higher urban population density. For instance, in a year like 2000, where Republicans narrowly won in the actual ECV count, a proportional method could have swung the outcome.\nOverall, State-Wide Proportional most accurately reflects voter preferences but tilts outcomes toward the party with broad, urban support."
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "",
    "text": "Selecting a retirement plan is one of the most critical financial decisions for new CUNY faculty, as it has permanent implications for their long-term financial security. Once chosen, the plan cannot be changed, making it essential to consider the potential risks and rewards. This project evaluates the Teachers Retirement System (TRS) and the Optional Retirement Plan (ORP) using historical economic data and Monte Carlo simulations. By modeling contributions, investment returns, and inflation-adjusted benefits, we aim to provide a personalized, data-driven recommendation tailored to individual financial goals and risk tolerance."
  },
  {
    "objectID": "mp04.html#introduction",
    "href": "mp04.html#introduction",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "",
    "text": "Selecting a retirement plan is one of the most critical financial decisions for new CUNY faculty, as it has permanent implications for their long-term financial security. Once chosen, the plan cannot be changed, making it essential to consider the potential risks and rewards. This project evaluates the Teachers Retirement System (TRS) and the Optional Retirement Plan (ORP) using historical economic data and Monte Carlo simulations. By modeling contributions, investment returns, and inflation-adjusted benefits, we aim to provide a personalized, data-driven recommendation tailored to individual financial goals and risk tolerance."
  },
  {
    "objectID": "mp04.html#objectives",
    "href": "mp04.html#objectives",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Objectives",
    "text": "Objectives\nThe primary goal of this project is to equip new CUNY faculty with an evidence-based recommendation for selecting the optimal retirement plan (TRS or ORP). This involves analyzing historical economic and financial data to assess how each plan performs under different economic conditions. We will utilize the following datasets:\n\nRate of Inflation: Inflation erodes the purchasing power of retirement benefits. For TRS, it determines the annual adjustment in benefits, while for ORP, it impacts the real value of savings and withdrawals.\nRate of Wage Growth: Wage growth directly influences contributions to both plans. For TRS, it affects the final average salary (FAS), which determines retirement payouts. For ORP, higher wages lead to larger investment contributions.\nUS Equity Market Returns: Equity market performance is crucial for ORP participants, as their investments grow with market returns. US equities represent a significant portion of most retirement portfolios.\nInternational Equity Market Returns: Diversification through international equities reduces risk and enhances returns for ORP participants. It provides a broader perspective on investment performance beyond the US market.\nBond Returns: Bonds are a critical component of retirement portfolios, especially for ORP participants as they approach retirement. Bonds provide stability and reduce volatility in investment returns.\nShort-Term Debt Returns: Short-term debt, such as Treasury yields, represents a safe investment option with lower risk. It is particularly relevant for retirees seeking liquidity and capital preservation.\n\nBy combining insights from these datasets, we will simulate potential retirement outcomes for each plan, identify risks such as running out of funds, and quantify the financial benefits of each option. This comprehensive approach ensures that our recommendation aligns with the diverse financial priorities of faculty members.\n\nLibraries\nThe following are the libraries we would use throughtout the project.\n\n\nShow the code\nlibrary(httr2)       # API requests\nlibrary(dplyr)       # Data manipulation\nlibrary(ggplot2)     # Visualization\nlibrary(zoo)         # Time-series data\nlibrary(DT)          # Interactive tables\nlibrary(ggcorrplot)  # Correlation plots\nlibrary(lubridate)   # Date/time handling\nlibrary(gt)          # Publication-quality tables\nlibrary(plotly)      # Interactive plots\nlibrary(tidyr)       # Data reshaping"
  },
  {
    "objectID": "mp04.html#data-sources",
    "href": "mp04.html#data-sources",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Data Sources",
    "text": "Data Sources\n\nLibraries\nThe following are the libraries we would use throughtout the project.\n\n\nShow the code\nlibrary(httr2)    # For API requests\nlibrary(dplyr)    # For data manipulation\nlibrary(ggplot2)  # For visualization\nlibrary(zoo)      # For handling time-series data and filling missing values"
  },
  {
    "objectID": "mp04.html#data-collection",
    "href": "mp04.html#data-collection",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Data Collection",
    "text": "Data Collection\nFor this project, we will use data from two economic and financial data sources:\n\nAlphaVantage: A commercial stock market data provider.\nFRED: The Federal Reserve Economic Data repository maintained by the Federal Reserve Bank of St. Louis.\n\nFRED is free to access, but AlphaVantage is a commercial service requiring a subscription. For this mini-project, the free tier of AlphaVantage will suffice.\n\nLinks to Resources\n\nAlphaVantage\nFRED (Federal Reserve Economic Data)"
  },
  {
    "objectID": "mp04.html#data-collection-1",
    "href": "mp04.html#data-collection-1",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "DATA COLLECTION",
    "text": "DATA COLLECTION\nWe would proceed to collect the data from the sources mentioned above. and we would do it on a monthly basis for the last 15 years.\nTo do so, we would use their APIs:\n\n\nShow the code\n# Define the base URL for the FRED API\nfred_url &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\n\n# Define the base URL for the AlphaVantage API\nalphavantage_url &lt;- \"https://www.alphavantage.co/query\"\n\n# Read FRED API key from a file\nfred_key &lt;- readLines(\"fred_key.txt\") # Ensure this file contains only your FRED API key\n\n# Read AlphaVantage API key from a file\nalphavantage_key &lt;- readLines(\"alphavantage_key.txt\") # Ensure this file contains only your AlphaVantage API key\n\n# Montly basis for the last 15 years\nTime_Montly =as.Date(\"2009-01-01\")"
  },
  {
    "objectID": "mp04.html#data-preprocessing-and-cleaning",
    "href": "mp04.html#data-preprocessing-and-cleaning",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Data Preprocessing and Cleaning",
    "text": "Data Preprocessing and Cleaning\nNow that we have collected the data, we can proceed to the next step, which is data preprocessing. We we gonna unify the data and deal with missing values by using bootstrapping.\n\n\nShow the code\n# Standardize dates to the first of each month for all datasets\ndatasets &lt;- list(inflation_df, wage_growth_df, us_equity_df, intl_equity_df, bond_df, short_term_df)\ndatasets &lt;- datasets |&gt;\n  lapply(\\(df) {\n    df$date &lt;- as.Date(format(df$date, \"%Y-%m-01\"))\n    df\n  })\n\n# Unpack datasets back\ninflation_df &lt;- datasets[[1]]\nwage_growth_df &lt;- datasets[[2]]\nus_equity_df &lt;- datasets[[3]]\nintl_equity_df &lt;- datasets[[4]]\nbond_df &lt;- datasets[[5]]\nshort_term_df &lt;- datasets[[6]]\n\n# Create master date sequence\ndate_seq &lt;- data.frame(date = seq(from = as.Date(\"2009-01-01\"), \n                                  to = as.Date(format(Sys.Date(), \"2024-10-01\")), \n                                  by = \"month\"))\n\n# Perform left joins for all datasets\ncombined_data &lt;- date_seq |&gt;\n  left_join(inflation_df, by = \"date\") |&gt;\n  left_join(wage_growth_df, by = \"date\") |&gt;\n  left_join(us_equity_df, by = \"date\") |&gt;\n  left_join(intl_equity_df, by = \"date\") |&gt;\n  left_join(bond_df, by = \"date\") |&gt;\n  left_join(short_term_df, by = \"date\")\n\n# Handle missing data\ncombined_data &lt;- combined_data |&gt;\n  mutate(\n    inflation_rate = replace(inflation_rate,\n                             is.na(inflation_rate),\n                             sample(na.omit(inflation_rate), size = sum(is.na(inflation_rate)), replace = TRUE)),\n    wage_growth = replace(wage_growth,\n                          is.na(wage_growth),\n                          sample(na.omit(wage_growth), size = sum(is.na(wage_growth)), replace = TRUE)),\n    us_equity_close = replace(us_equity_close,\n                              is.na(us_equity_close),\n                              sample(na.omit(us_equity_close), size = sum(is.na(us_equity_close)), replace = TRUE)),\n    intl_equity_close = replace(intl_equity_close,\n                                is.na(intl_equity_close),\n                                sample(na.omit(intl_equity_close), size = sum(is.na(intl_equity_close)), replace = TRUE)),\n    bond_yield = replace(bond_yield,\n                         is.na(bond_yield),\n                         sample(na.omit(bond_yield), size = sum(is.na(bond_yield)), replace = TRUE)),\n    short_term_yield = replace(short_term_yield,\n                               is.na(short_term_yield),\n                               sample(na.omit(short_term_yield), size = sum(is.na(short_term_yield)), replace = TRUE))\n  ) |&gt;\n  mutate(across(where(is.numeric), ~ round(., 2)))\n\n# Display dataset interactively\nDT::datatable(combined_data, options = list(pageLength = 10, scrollX = TRUE))"
  },
  {
    "objectID": "mp04.html#initial-analysis",
    "href": "mp04.html#initial-analysis",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Initial Analysis",
    "text": "Initial Analysis\nNow that we have a clean dataset, we can perform some initial analysis to understand the trends and relationships between the variables.\nWe will visualize the data using line plots and we going to display them in pairs to compare the trends between the variables. so it would be inflation rate and wage growth, US equity and International equity, and Bond Yield and Short-Term Yield.\n\n\nShow the code\n# Create plot for Inflation Rate and Wage Growth with corrected labels and separate y-axes\np1 &lt;- ggplot(combined_data, aes(x = date)) +\n  # Inflation Rate\n  geom_line(aes(y = inflation_rate), color = \"blue\") +\n  scale_y_continuous(\n    name = \"Inflation Rate Value (NYC)\",  # Corrected label for inflation rate\n    sec.axis = sec_axis(~ . * 0, name = \" \")  # Create empty secondary axis for separation\n  ) +\n  # Wage Growth (scaled for visibility)\n  geom_line(aes(y = wage_growth * 10), color = \"green\") +\n  scale_y_continuous(\n    name = \"Inflation Rate Value (NYC)\",  # Corrected label for wage growth\n    sec.axis = sec_axis(~ . / 10, name = \"Wage Growth (Hourly Earnings in $)\")  # Corrected secondary label\n  ) +\n  labs(title = \"Inflation Rate and Wage Growth Over Time\", x = \"Date\") +\n  theme_minimal() +\n  theme(\n    axis.title.y.left = element_text(color = \"blue\", size = 12),\n    axis.title.y.right = element_text(color = \"green\", size = 12)\n  )\n\n# Create plot for US Equity and International Equity\np2 &lt;- ggplot(combined_data, aes(x = date)) +\n  # US Equity Return\n  geom_line(aes(y = us_equity_close), color = \"red\") +  \n  scale_y_continuous(\n    name = \"US Equity Return (Adjusted Close)\"  \n  ) +\n  # International Equity Return (scaled for visibility)\n  geom_line(aes(y = intl_equity_close * 10), color = \"purple\") +  \n  scale_y_continuous(\n    name = \"US Equity Return (Adjusted Close)\",\n    sec.axis = sec_axis(~ . / 10, name = \"Intl Equity Return\")  \n  ) +\n  labs(title = \"US and International Equity Returns Over Time\", x = \"Date\") +\n  theme_minimal() +\n  theme(\n    axis.title.y.left = element_text(color = \"red\", size = 12),\n    axis.title.y.right = element_text(color = \"purple\", size = 12)\n  )\n\n# Create plot for Bond Yield and Short-Term Yield \np3 &lt;- ggplot(combined_data, aes(x = date)) +\n  # Bond Yield\n  geom_line(aes(y = bond_yield * 2), color = \"orange\") +  \n  scale_y_continuous(\n    name = \"Bond Yield (%)\", \n    sec.axis = sec_axis(~ . / 2, name = \"Bond Yield (%)\")  \n  ) +\n  # Short-Term Yield\n  geom_line(aes(y = short_term_yield * 2), color = \"brown\") +  \n  scale_y_continuous(\n    name = \"Short-Term Yield (%)\", \n    sec.axis = sec_axis(~ . / 2, name = \"Short-Term Yield (%)\")  \n  ) +\n  labs(title = \"Bond Yield and Short-Term Yield Over Time\", x = \"Date\") +\n  theme_minimal() +\n  theme(\n    axis.title.y.left = element_text(color = \"orange\", size = 12),\n    axis.title.y.right = element_text(color = \"brown\", size = 12)\n  )\n\n# Display all three plots\np1\n\n\n\n\n\n\n\n\n\nShow the code\np2\n\n\n\n\n\n\n\n\n\nShow the code\np3\n\n\n\n\n\n\n\n\n\nWe can see a clear correlation between some of this plots. For example, the inflation rate and wage growth are highly correlated, as well as the US and International Equity Returns.\n\nCorrelation matrix\nWe will calculate the correlation matrix to understand the relationships between the variables. This will help us identify any significant correlations that can inform our analysis and decision-making process.\n\n\nShow the code\n# Select only numeric columns\nnumeric_data &lt;- combined_data |&gt;\n  select(where(is.numeric))\n\n# Compute the correlation matrix\ncor_matrix &lt;- cor(numeric_data, use = \"complete.obs\")\n\n# Visualize the correlation matrix using ggcorrplot\nggcorrplot(cor_matrix, \n           type = \"lower\",          # Only show the lower triangle\n           lab = TRUE,              # Show correlation coefficient labels\n           lab_size = 3,            # Adjust label size\n           method = \"circle\",       # Use circle shapes for correlation values\n           colors = c(\"red\", \"white\", \"blue\"), # Color scale\n           title = \"Correlation Matrix\", # Plot title\n           ggtheme = theme_minimal(), # Minimalistic theme\n           tl.cex = 12,             # Title label size\n           tl.col = \"black\"         # Title text color\n)\n\n\n\n\n\n\n\n\n\nIn this initial analysis we examine the relationships between several key economic factors, including inflation, wage growth, stock market returns (both US and international), bond yields, and short-term debt yields. Understanding how these factors are correlated can provide valuable insights for making informed financial decisions, particularly for retirement planning."
  },
  {
    "objectID": "mp04.html#key-findings",
    "href": "mp04.html#key-findings",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Key Findings:",
    "text": "Key Findings:\n\n1. Inflation Rate and Wage Growth:\n\nCorrelation: 0.98 (Very strong)\nInterpretation: When inflation increases, wage growth tends to follow suit. This suggests that as the cost of living rises, your wages are likely to increase as well, which helps you keep up with inflation.\n\n\n\n2. Stock Market Returns:\n\nUS and International Markets:\n\nCorrelation: 0.97 (Very strong)\nInterpretation: US and international stock markets have a highly positive correlation, meaning they often move in the same direction. This makes diversification between US and international markets a good strategy for reducing risk while maintaining growth potential.\n\n\n\nUS Equity Returns and Bond Yields:\n\nCorrelation: 0.68 (Moderate)\nInterpretation: The moderate correlation suggests that when US stocks perform well, bond yields may not follow the same trend. Investors need to balance their portfolios between stocks and bonds, taking into account their risk tolerance and time horizon.\n\n\n\n\n3. Inflation and Other Variables:\n\nInflation and US Stock Returns:\n\nCorrelation: 0.92 (Strong)\nInterpretation: Inflation and US stock returns are highly correlated. As inflation rises, US stocks tend to perform well, making stocks a potential hedge against inflation.\n\n\n\nInflation and International Stock Returns:\n\nCorrelation: 0.97 (Very strong)\nInterpretation: Like US stocks, international stocks also tend to rise with inflation, making global diversification a viable strategy for mitigating inflation risk.\n\n\n\nInflation and Bond Yields:\n\nCorrelation: 0.72 (Moderate)\nInterpretation: Inflation and bond yields are moderately correlated. Rising inflation generally leads to higher bond yields, which can help protect fixed-income investments from losing purchasing power.\n\n\n\n\n4. Bond Yield and Short-Term Yield:\n\nCorrelation: 0.75 (Strong)\nInterpretation: Bond yields and short-term yields tend to move in the same direction, which suggests that both types of bonds respond similarly to changes in interest rates."
  },
  {
    "objectID": "mp04.html#implications-for-retirement-planning",
    "href": "mp04.html#implications-for-retirement-planning",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Implications for Retirement Planning:",
    "text": "Implications for Retirement Planning:\n\nWage Growth and Inflation: The close correlation between wage growth and inflation means that as prices rise, your income should likely increase, helping you maintain purchasing power in retirement.\nEquity Market Performance: The strong correlation between US and international equity returns allows for diversification between these markets without sacrificing performance.\nBonds and Short-Term Yields: Understanding the relationship between bonds and short-term debt yields is crucial for managing fixed-income investments in a rising interest rate environment."
  },
  {
    "objectID": "mp04.html#historical-comparison-of-trs-and-orp",
    "href": "mp04.html#historical-comparison-of-trs-and-orp",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Historical Comparison of TRS and ORP",
    "text": "Historical Comparison of TRS and ORP\n\nTRS vs. ORP: Retirement Comparison\nIn this section, we compare the Teachers Retirement System (TRS) and the Optional Retirement Plan (ORP) for the first month of retirement, assuming:\n\nCareer Timeline: Employee joined CUNY in January 2009 and retired in November 2024.\nStarting Salary: $50,000 in 2009, adjusted annually for historical wage growth (using actual wage growth data).\nFixed Plan Parameters: Contribution rates and retirement benefits remain constant during the employee’s career.\n\n\n\nPlan Overviews\n\nTRS (Teachers Retirement System)\n\nDefined-Benefit Plan:\n\nRetirement income is based on the Final Average Salary (FAS) and years of service.\n\nRetirement Benefit Formula:\n\n( = () ), capped at 30 years of service.\n\nInflation Adjustment:\n\nAnnual increase of 50% of CPI, capped between 1% and 3% annually.\n\n\n\n\nORP (Optional Retirement Plan)\n\nDefined-Contribution Plan:\n\nEmployee and employer contribute to an individual retirement account, invested in a Fidelity Freedom Fund.\n\nContributions:\n\nEmployee: Salary-based contribution (e.g., 3% for salaries ≤$45,000; 3.5% for $45,001–$55,000).\nEmployer: 8% for the first 7 years, then 10% for subsequent years.\n\nInvestment Returns:\n\nUS equities: 7%, International equities: 6%, Bonds: 4%.\n\nRetirement Strategy: Withdraw 4% of accumulated funds annually at retirement.\n\n\n\n\n\nStep-by-Step Analysis\n\n1. Starting Salary\n\nThe employee’s starting salary is $50,000 in January 2009 and is retired in November 2024. Years served = 15 years.\nThe salary increases annually based on historical wage growth data.\n\n\n\n2. TRS Benefit Calculation\n\nFinal Average Salary (FAS):\n\nCalculated as the average of the last three years of salary, adjusted for wage growth.\n\nYears of Service:\n\nThe employee works for 15 years (from 2009 to 2024), and their pension benefit is calculated based on 15 years of service.\n\nMonthly Pension:\n\nThe pension benefit is calculated as ( () ) (since the employee works 15 years).\n\nInflation Adjustment:\n\nThe pension benefit will be inflation-adjusted annually based on the CPI. The adjustment will be between 1% and 3% per year, applied each September after retirement.\n\n\n\n\n3. ORP Benefit Calculation\n\nMonthly Contributions:\n\nThe employee and employer contributions are calculated based on the salary brackets (e.g., 3% for salaries ≤$45,000).\nContributions are made monthly throughout the employee’s 15-year career.\n\nInvestment Growth:\n\nThe employee’s contributions grow using the average market returns over time:\n\nUS equities: 7%, International equities: 6%, Bonds: 4%.\n\n\nRetirement Withdrawal:\n\nThe employee can begin withdrawing funds from their ORP account upon retirement.\nA general rule of thumb is to withdraw 4% annually from the accumulated balance at retirement.\n\n\n\nThe following code snippets will calculate the retirement benefits for both TRS and ORP based on the assumptions outlined above. We will compare the monthly benefits and accumulated funds for each plan to provide a comprehensive analysis of the retirement options.\n\n\nShow the code\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(lubridate)\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\nShow the code\n# Assuming combined_data has been preprocessed correctly (with inflation_rate as percentage and correct dates)\n\n# Set initial salary and define years of service\ninitial_salary &lt;- 60000\nyears_of_service &lt;- 15  # Adjusted to 15 years\n\n# Initialize a vector to store salaries for each year\nsalaries &lt;- numeric(years_of_service)\nsalary_growth &lt;- numeric(years_of_service)\ncpi_growth &lt;- numeric(years_of_service)\nsalary_increase &lt;- numeric(years_of_service)\n\n# Set the first salary as the initial salary\nsalaries[1] &lt;- initial_salary\n\n# Loop through the combined data for yearly salary growth calculation\nfor (i in 2:years_of_service) {\n  # We need CPI data for each September to calculate the salary for the next year\n  # Calculate the CPI growth between September (from the previous year) and September of the current year\n  \n  september_cpi_previous_year &lt;- combined_data$inflation_rate[which(month(combined_data$date) == 9 & year(combined_data$date) == 2008 + (i - 1))]\n  september_cpi_current_year &lt;- combined_data$inflation_rate[which(month(combined_data$date) == 9 & year(combined_data$date) == 2008 + i)]\n  \n  # Calculate the salary growth rate based on CPI change\n  cpi_growth[i] &lt;- september_cpi_current_year / september_cpi_previous_year\n  salary_growth[i] &lt;- cpi_growth[i]\n  \n  # Calculate the increase in salary\n  salary_increase[i] &lt;- salaries[i-1] * (cpi_growth[i] - 1)  # Difference from the previous salary\n  \n  # Apply the salary growth to the previous year's salary\n  salaries[i] &lt;- salaries[i-1] * cpi_growth[i]  \n}\n\n# Calculate Final Average Salary (FAS) - which is the average of the last 3 years' salary\nfas &lt;- mean(salaries[(years_of_service-2):years_of_service])\n\n# Calculate the Annual Retirement Benefit based on years of service\nif (years_of_service &lt;= 20) {\n  annual_benefit &lt;- 0.0167 * fas * years_of_service\n} else if (years_of_service == 20) {\n  annual_benefit &lt;- 0.0175 * fas * years_of_service\n} else {\n  annual_benefit &lt;- (0.35 + 0.02 * years_of_service) * fas\n}\n\n# Display the results\ncat(\"Annual TRS Benefit: $\", round(annual_benefit, 2), \"\\n\")\n\n\nAnnual TRS Benefit: $ 21233.11 \n\n\nShow the code\ncat(\"Monthly TRS Benefit: $\", round(annual_benefit/12, 2 ), \"\\n\")\n\n\nMonthly TRS Benefit: $ 1769.43 \n\n\nShow the code\nprint(salaries)\n\n\n [1] 60000.00 60296.88 62411.23 63755.62 64589.87 65890.13 66526.64 67882.19\n [9] 69862.27 72204.87 74110.60 75287.21 79272.15 85820.04 89196.53\n\n\n\n\nShow the code\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(lubridate)\n\n# Initialize parameters\ninitial_salary &lt;- 60000  # Starting salary\nyears_of_service &lt;- 15   # Number of years to simulate\n\n# Set up salary contribution brackets\nsalary_brackets &lt;- c(45000, 55000, 75000, 100000)\n\n# Initialize vectors to store results\nsalaries &lt;- numeric(years_of_service)\nemployee_contributions &lt;- numeric(years_of_service)\nemployer_contributions &lt;- numeric(years_of_service)\ntotal_contributions &lt;- numeric(years_of_service)\n\n# Initialize the first year's salary\nsalaries[1] &lt;- initial_salary\n\n# Loop through each year and compute the contributions\nfor (i in 1:years_of_service) {\n  \n  # Get the salary for the year\n  if (i == 1) {\n    salary &lt;- initial_salary\n  } else {\n    # Calculate salary growth based on CPI (only starting from the second year)\n    september_cpi_previous_year &lt;- combined_data$inflation_rate[which(month(combined_data$date) == 9 & year(combined_data$date) == 2009 + (i - 1))]\n    september_cpi_current_year &lt;- combined_data$inflation_rate[which(month(combined_data$date) == 9 & year(combined_data$date) == 2009 + i)]\n    \n    # Calculate the salary growth rate based on CPI change (year over year growth)\n    cpi_growth &lt;- september_cpi_current_year / september_cpi_previous_year\n    salary &lt;- salaries[i - 1] * cpi_growth  # Update salary based on CPI growth\n  }\n  \n  # Store the salary for each year\n  salaries[i] &lt;- salary\n  \n  # Employee contribution calculation based on salary brackets (annual contribution, divided by 12 for monthly)\n  if (salary &lt;= salary_brackets[1]) {\n    employee_contribution_annual &lt;- salary * 0.03\n  } else if (salary &lt;= salary_brackets[2]) {\n    employee_contribution_annual &lt;- salary * 0.035\n  } else if (salary &lt;= salary_brackets[3]) {\n    employee_contribution_annual &lt;- salary * 0.045\n  } else if (salary &lt;= salary_brackets[4]) {\n    employee_contribution_annual &lt;- salary * 0.0575\n  } else {\n    employee_contribution_annual &lt;- salary * 0.06\n  }\n  \n  # Convert annual contribution to monthly contribution\n  employee_contribution_monthly &lt;- employee_contribution_annual / 12\n  \n  # Employer contribution: 8% for the first 7 years, 10% after (annual contribution)\n  if (i &lt;= 7) {\n    employer_contribution_annual &lt;- salary * 0.08\n  } else {\n    employer_contribution_annual &lt;- salary * 0.10\n  }\n  \n  # Convert annual employer contribution to monthly\n  employer_contribution_monthly &lt;- employer_contribution_annual / 12\n  \n  # Total contribution for the year (monthly contributions summed over 12 months)\n  total_contribution_monthly &lt;- employee_contribution_monthly + employer_contribution_monthly\n  total_contribution_annual &lt;- total_contribution_monthly * 12  # Multiply by 12 to get the annual total contribution\n  \n  # Store contributions\n  employee_contributions[i] &lt;- employee_contribution_annual\n  employer_contributions[i] &lt;- employer_contribution_annual\n  total_contributions[i] &lt;- total_contribution_annual\n}\n\n# Create the table with the results for each year\nyearly_contributions_data &lt;- data.frame(\n  Year = 1:years_of_service,\n  Salary = round(salaries, 2),\n  Employee_Contribution = round(employee_contributions, 2),\n  Employer_Contribution = round(employer_contributions, 2),\n  Total_Contribution = round(total_contributions, 2)\n)\n\n\n\n\nShow the code\n# Initialize parameters for ORP calculations\n# Age 50 to 59\nequity_us_alloc_50_59 &lt;- 0.47  # US Equities allocation\nequity_international_alloc_50_59 &lt;- 0.32  # International Equities allocation\nbonds_alloc_50_59 &lt;- 0.21  # Bonds allocation\n\n# Age 60 to 74\nequity_us_alloc_60_74 &lt;- 0.34  # US Equities allocation\nequity_international_alloc_60_74 &lt;- 0.23  # International Equities allocation\nbonds_alloc_60_74 &lt;- 0.43  # Bonds allocation\n\n# Initialize vectors to store results\nperformance_us_equity &lt;- numeric(years_of_service)\nperformance_international_equity &lt;- numeric(years_of_service)\nperformance_bonds &lt;- numeric(years_of_service)\ntotal_accumulated &lt;- numeric(years_of_service)  # Total accumulated amount\n\n# Starting balance\ntotal_accumulated[1] &lt;- yearly_contributions_data$Total_Contribution[1]\n\n# Start contributing at age 50 in 2009\nstarting_age &lt;- 50\n\n# Loop through each year to compute performance\nfor (i in 1:years_of_service) {\n  current_year &lt;- 2009 + i - 1  # Current year\n  next_year &lt;- current_year + 1  # Next year\n  current_age &lt;- starting_age + i - 1  # Current age\n  \n  # Assign allocations based on age\n  if (current_age &gt;= 50 && current_age &lt;= 59) {\n    equity_us_alloc &lt;- equity_us_alloc_50_59\n    equity_international_alloc &lt;- equity_international_alloc_50_59\n    bonds_alloc &lt;- bonds_alloc_50_59\n  } else if (current_age &gt;= 60 && current_age &lt;= 74) {\n    equity_us_alloc &lt;- equity_us_alloc_60_74\n    equity_international_alloc &lt;- equity_international_alloc_60_74\n    bonds_alloc &lt;- bonds_alloc_60_74\n  }\n  \n  # Ensure we have data for the future January, or use November of the current year if not available\n  if (i == years_of_service) {\n    # For the last year, use November data instead of January of the next year\n    us_equity_growth &lt;- combined_data$us_equity_close[which(year(combined_data$date) == current_year & month(combined_data$date) == 11)] /\n                        combined_data$us_equity_close[which(year(combined_data$date) == current_year & month(combined_data$date) == 1)]\n    \n    intl_equity_growth &lt;- combined_data$intl_equity_close[which(year(combined_data$date) == current_year & month(combined_data$date) == 11)] /\n                          combined_data$intl_equity_close[which(year(combined_data$date) == current_year & month(combined_data$date) == 1)]\n    \n    bond_yield_annual &lt;- mean(combined_data$bond_yield[which(year(combined_data$date) == current_year & month(combined_data$date) &lt;= 11)]) / 100 + 1\n  } else {\n    # For all other years, use January of the next year\n    us_equity_growth &lt;- combined_data$us_equity_close[which(year(combined_data$date) == next_year & month(combined_data$date) == 1)] /\n                        combined_data$us_equity_close[which(year(combined_data$date) == current_year & month(combined_data$date) == 1)]\n    \n    intl_equity_growth &lt;- combined_data$intl_equity_close[which(year(combined_data$date) == next_year & month(combined_data$date) == 1)] /\n                          combined_data$intl_equity_close[which(year(combined_data$date) == current_year & month(combined_data$date) == 1)]\n    \n    bond_yield_annual &lt;- mean(combined_data$bond_yield[which(year(combined_data$date) == current_year)]) / 100 + 1\n  }\n\n  # Store individual performance values\n  performance_us_equity[i] &lt;- us_equity_growth\n  performance_international_equity[i] &lt;- intl_equity_growth\n  performance_bonds[i] &lt;- bond_yield_annual  # Already adjusted with +1\n  \n  # Calculate total accumulated with contributions and compound effect\n  if (i == 1) {\n    # First year: Apply performance to the first contribution\n    total_accumulated[i] &lt;- yearly_contributions_data$Total_Contribution[i] * (\n                              (equity_us_alloc * performance_us_equity[i]) +\n                              (equity_international_alloc * performance_international_equity[i]) +\n                              (bonds_alloc * performance_bonds[i])\n                            )\n  } else {\n    # Subsequent years: Include previous accumulated total and current contributions\n    total_accumulated[i] &lt;- (total_accumulated[i - 1] + yearly_contributions_data$Total_Contribution[i]) * (\n                              (equity_us_alloc * performance_us_equity[i]) +\n                              (equity_international_alloc * performance_international_equity[i]) +\n                              (bonds_alloc * performance_bonds[i])\n                            )\n  }\n}\n\n# Create a data frame with the results\nyearly_performance_data &lt;- data.frame(\n  Year = 2009:(2009 + years_of_service - 1),\n  Age = starting_age:(starting_age + years_of_service - 1),\n  US_Equity_Performance = round(performance_us_equity, 4),\n  International_Equity_Performance = round(performance_international_equity, 4),\n  Bonds_Performance = round(performance_bonds, 4),\n  Total_Accumulated = round(total_accumulated, 2)\n)\n\n# Print the performance table\nprint(yearly_performance_data)\n\n\n   Year Age US_Equity_Performance International_Equity_Performance\n1  2009  50                1.3267                           1.4020\n2  2010  51                1.2218                           1.1705\n3  2011  52                1.0419                           0.9066\n4  2012  53                1.1653                           1.1655\n5  2013  54                1.2143                           1.1123\n6  2014  55                1.1412                           0.9989\n7  2015  56                0.9915                           0.9345\n8  2016  57                1.1998                           1.1266\n9  2017  58                1.2630                           1.2773\n10 2018  59                0.9758                           0.8746\n11 2019  60                1.2145                           1.1069\n12 2020  61                1.1721                           1.1232\n13 2021  62                1.2321                           1.0814\n14 2022  63                0.9182                           0.9599\n15 2023  64                1.1354                           1.0245\n   Bonds_Performance Total_Accumulated\n1             1.0326           9667.74\n2             1.0321          20316.34\n3             1.0279          28123.67\n4             1.0180          41018.30\n5             1.0235          56181.08\n6             1.0254          69056.23\n7             1.0214          75912.81\n8             1.0184          97885.42\n9             1.0233         131832.58\n10            1.0291         136059.50\n11            1.0214         162600.51\n12            1.0089         190899.34\n13            1.0144         225567.89\n14            1.0295         233712.13\n15            1.0395         265006.47\n\n\n\n\nShow the code\n# Load necessary library\nlibrary(plotly)\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\nShow the code\n# Calculate total accumulated contributions over time\ntotal_contributions &lt;- cumsum(yearly_contributions_data$Total_Contribution)\n\n# Create a data frame for plotting\nplot_data &lt;- data.frame(\n  Year = 2009:(2009 + years_of_service - 1),\n  Total_Accumulated = total_accumulated,\n  Total_Contributions = total_contributions\n)\n\n# Create the interactive graph\nplot &lt;- plot_ly(plot_data, x = ~Year) %&gt;%\n  add_trace(y = ~Total_Accumulated, type = 'scatter', mode = 'lines+markers', name = 'Total Accumulated',\n            hovertemplate = 'Year: %{x}&lt;br&gt;Total Accumulated: %{y:.2f}&lt;extra&gt;&lt;/extra&gt;') %&gt;%\n  add_trace(y = ~Total_Contributions, type = 'scatter', mode = 'lines+markers', name = 'Total Contributions',\n            hovertemplate = 'Year: %{x}&lt;br&gt;Total Contributions: %{y:.2f}&lt;extra&gt;&lt;/extra&gt;') %&gt;%\n  layout(\n    title = 'Interactive Graph: Total Accumulation vs. Contributions',\n    xaxis = list(title = 'Year'),\n    yaxis = list(title = 'Amount ($)'),\n    legend = list(orientation = 'h', x = 0.5, xanchor = 'center')\n  )\n\n\nannual_benefit_opr = 0.04 * total_accumulated[years_of_service]\n# Print the plot\n# Display the results\ncat(\"Annual OPR Benefit: $\", round(annual_benefit_opr, 2), \"\\n\")\n\n\nAnnual OPR Benefit: $ 10600.26 \n\n\nShow the code\ncat(\"Monthly OPR Benefit: $\", round(annual_benefit_opr/12 , 2), \"\\n\")\n\n\nMonthly OPR Benefit: $ 883.35 \n\n\nShow the code\nplot\n\n\n\n\n\n\n##Comparative Analysis of TRS and ORP\n\n\nShow the code\n# Calculate monthly benefits\nmonthly_benefit_opr &lt;- annual_benefit_opr / 12\nmonthly_benefit_trs &lt;- annual_benefit / 12\n\n# Create a data frame for plotting\nbenefit_data &lt;- data.frame(\n  Benefit_Type = c(\"Monthly OPR Benefit\", \"Monthly TRS Benefit\"),\n  Amount = c(monthly_benefit_opr, monthly_benefit_trs)\n)\n\n# Create the bar plot\nbenefit_plot &lt;- ggplot(benefit_data, aes(x = Benefit_Type, y = Amount, fill = Benefit_Type)) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) +\n  geom_text(aes(label = paste0(\"$\", round(Amount, 2))), vjust = -0.5, size = 4) +\n  labs(\n    title = \"Comparison of Monthly OPR and TRS Benefits\",\n    x = \"Benefit Type\",\n    y = \"Amount ($)\"\n  ) +\n  theme_minimal()\n\n# Print the plot\nprint(benefit_plot)\n\n\n\n\n\n\n\n\n\nThe bar chart illustrates a comparison between the Monthly OPR Benefit and the Monthly TRS Benefit. Key insights from the visualization are as follows:\n\nMonthly OPR Benefit:\n\nThe monthly benefit derived from the OPR system is $883.35, as shown by the first bar.\nThis represents a smaller portion compared to the TRS benefit, reflecting the structure of OPR benefit allocations.\n\nMonthly TRS Benefit:\n\nThe monthly benefit derived from the TRS system is $1769.43, as shown by the taller second bar.\nThis indicates a significantly higher payout, suggesting that the TRS system may provide a more robust financial benefit for retirees compared to the OPR system.\n\nComparison:\n\nThe TRS benefit is nearly double the OPR benefit, highlighting a substantial disparity between the two systems in terms of monthly payouts.\n\n\nThis comparison provides valuable insight into the financial implications of selecting between the OPR and TRS systems, with the TRS system offering higher monthly benefits. This can serve as a crucial factor for decision-making in retirement planning."
  },
  {
    "objectID": "mp04.html#fixed-rate-analysis",
    "href": "mp04.html#fixed-rate-analysis",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Fixed-Rate Analysis",
    "text": "Fixed-Rate Analysis\nTo modify the simulation and project an employee’s pension benefit (TRS) or withdrawal amount (ORP) from retirement until death, we need to consider several key factors:\n\nRetirement Age: The age at which the employee retires and begins receiving benefits.\nDeath Age: The estimated age at which the employee will pass away.\n\n\n\nShow the code\n# Parameters\nretirement_age &lt;- 65\ndeath_age &lt;- 90  # Estimated death age\nyears_in_retirement &lt;- death_age - retirement_age\n\n# Initialize the mean weighted performance\nmean_performance &lt;- mean(\n  (equity_us_alloc * yearly_performance_data$US_Equity_Performance) +\n  (equity_international_alloc * yearly_performance_data$International_Equity_Performance) +\n  (bonds_alloc * yearly_performance_data$Bonds_Performance),\n  na.rm = TRUE\n)\n\n# Use this value as your fixed return rate for ORP\nfixed_return_rate &lt;- (mean_performance-1) #based on the performance of from the last 15 years of investments. \n\n# Fixed rate assumptions\nfixed_return_rate &lt;- fixed_return_rate  # Fixed annual return rate for ORP\nwithdrawal_rate &lt;- 0.04  # Initial ORP withdrawal rate\n\n# Calculate the average inflation from the combined_data dataset\navg_inflation &lt;- mean(combined_data$inflation_rate, na.rm = TRUE) / 100  # Convert to decimal form\n\n\n# Starting values\ntrs_annual_benefit &lt;- annual_benefit_trs  # TRS annual benefit\norp_balance &lt;- yearly_performance_data$Total_Accumulated[years_of_service]  # ORP final balance at retirement\n\n# Initialize vectors to store results\ntrs_benefits &lt;- numeric(years_in_retirement)\norp_withdrawals &lt;- numeric(years_in_retirement)\norp_balances &lt;- numeric(years_in_retirement)\nincome_gaps &lt;- numeric(years_in_retirement)\n\n# Simulation\nfor (year in 1:years_in_retirement) {\n  # TRS Benefit (adjusted for inflation with caps)\n  if (year == 1) {\n    trs_benefits[year] &lt;- trs_annual_benefit\n  } else {\n    cpi_adjustment &lt;- max(min(avg_inflation * 0.5, 0.03), 0.01)  # Cap between 1% and 3%\n    trs_benefits[year] &lt;- trs_benefits[year - 1] * (1 + cpi_adjustment)\n  }\n  \n  # ORP Withdrawal: Fixed percentage of the previous year's balance\n  if (year == 1) {\n    orp_withdrawals[year] &lt;- orp_balance * withdrawal_rate\n    orp_balance &lt;- orp_balance * (1 + fixed_return_rate) - orp_withdrawals[year]\n  } else {\n    orp_withdrawals[year] &lt;- orp_balances[year - 1] * withdrawal_rate\n    orp_balance &lt;- (orp_balances[year - 1] - orp_withdrawals[year]) * (1 + fixed_return_rate)\n  }\n  \n  # Ensure balance never goes negative\n  orp_balances[year] &lt;- max(orp_balance, 0)\n  if (orp_balance &lt;= 0) {\n    orp_withdrawals[year] &lt;- 0\n  }\n  \n  # Income gap (Annually)\n  income_gaps[year] &lt;- trs_benefits[year] - orp_withdrawals[year]\n}\n\n# Create results data frame\nresults &lt;- data.frame(\n  Year = retirement_age:(death_age - 1),\n  TRS_Annual_Income = trs_benefits,\n  ORP_Annual_Withdrawal = orp_withdrawals,\n  ORP_Balance_Remaining = orp_balances,\n  Income_Gap_Annual = income_gaps\n)\n\n# Prepare data for visualization\ngraph_data &lt;- results |&gt;\n  select(Year, TRS_Annual_Income, ORP_Annual_Withdrawal, ORP_Balance_Remaining) |&gt;\n  pivot_longer(\n    cols = c(TRS_Annual_Income, ORP_Annual_Withdrawal),\n    names_to = \"Plan\",\n    values_to = \"Annual_Income\"\n  ) |&gt;\n  mutate(\n    Plan = recode(\n      Plan, \n      TRS_Annual_Income = \"TRS Annual Income\", \n      ORP_Annual_Withdrawal = \"ORP Annual Withdrawal\"\n    )\n  )\n\n# Plot TRS and ORP incomes over time\nincome_plot &lt;- graph_data |&gt;\n  ggplot(aes(x = Year, y = Annual_Income, color = Plan, group = Plan)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  labs(\n    title = \"Comparison of TRS and ORP Annual Income Over Time\",\n    x = \"Year of Retirement\",\n    y = \"Annual Income ($)\",\n    color = \"Retirement Plan\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 14)\n  ) +\n  scale_y_continuous(labels = scales::dollar_format())\n\n# Display the plots\nincome_plot"
  },
  {
    "objectID": "mp04.html#fetching-data",
    "href": "mp04.html#fetching-data",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Fetching Data",
    "text": "Fetching Data\nWe already have the links to the resources and the API keys, now we can proceed to fetch the data.\n\n\nFetching Rate of Inflation (CPI for NYC Metro Area) from FRED\n\n\n\n\nShow the code\n# Fetch Inflation Data from FRED (CPI for NYC Metro Area)\ninflation_req &lt;- request(fred_url) |&gt;\n  req_url_query(\n    series_id = \"CUUR0400SA0\",  # CPI for All Urban Consumers: NYC Metro Area\n    api_key = fred_key,\n    file_type = \"json\"\n  )\n\ninflation_resp &lt;- inflation_req |&gt; req_perform()\ninflation_data &lt;- resp_body_json(inflation_resp)$observations\n\n# Convert to DataFrame\ninflation_df &lt;- data.frame(\n  date = as.Date(sapply(inflation_data, function(x) x$date)),\n  inflation_rate = as.numeric(sapply(inflation_data, function(x) x$value))\n)\n\n# Filter the data for the last 15 years\ninflation_df &lt;- inflation_df |&gt; filter(date &gt;= Time_Montly)\n\n\n\n\nFetching Rate of Wage Growth from FRED, We’ll use the AHETPI series to measure wage growth.\n\n\n\n\nShow the code\n# Fetch Wage Growth Data from FRED\nwage_growth_req &lt;- request(fred_url) |&gt;\n  req_url_query(\n    series_id = \"AHETPI\", # Average Hourly Earnings of Production and Nonsupervisory Employees\n    api_key = fred_key,\n    file_type = \"json\"\n  )\n\nwage_growth_resp &lt;- wage_growth_req |&gt; req_perform()\nwage_growth_data &lt;- resp_body_json(wage_growth_resp)$observations\n\n# Convert to a DataFrame\nwage_growth_df &lt;- data.frame(\n  date = as.Date(sapply(wage_growth_data, function(x) x$date)), # Parse date\n  wage_growth = as.numeric(sapply(wage_growth_data, function(x) x$value)) # Convert to numeric\n)\n\n# Filter data for the last 15–20 years\nwage_growth_df &lt;- wage_growth_df |&gt; filter(date &gt;= Time_Montly)\n\n\n\n\nFetching US Equity Market Returns from AlphaVantage\n\n\n\n\nShow the code\n# Fetch US Equity Market Data from AlphaVantage\nus_equity_req &lt;- request(alphavantage_url) |&gt;\n  req_url_query(\n    `function` = \"TIME_SERIES_MONTHLY_ADJUSTED\",\n    symbol = \"SPY\", # S&P 500 ETF\n    apikey = alphavantage_key\n  )\nus_equity_resp &lt;- us_equity_req |&gt; req_perform()\nus_equity_data &lt;- resp_body_json(us_equity_resp)$`Monthly Adjusted Time Series`\n\n# Convert to DataFrame\nus_equity_df &lt;- data.frame(\n  date = as.Date(names(us_equity_data)), # Extract dates\n  us_equity_close = as.numeric(sapply(us_equity_data, function(x) x$`5. adjusted close`)) # Extract adjusted close prices\n)\n\n# Standardize dates to the first of each month\nus_equity_df$date &lt;- as.Date(format(us_equity_df$date, \"%Y-%m-01\"))\n\n# Filter for the past 15 years\nus_equity_df &lt;- us_equity_df |&gt; filter(date &gt;= Time_Montly)\n\n\n\n\nFetching International Equity Market Returns from AlphaVantage\n\n\n\n\nShow the code\n# Fetch International Equity Market Data from AlphaVantage\nintl_req &lt;- request(alphavantage_url) |&gt;\n  req_url_query(\n    `function` = \"TIME_SERIES_MONTHLY_ADJUSTED\",\n    symbol = \"VEA\", # Vanguard FTSE Developed Markets ETF\n    apikey = alphavantage_key\n  )\n\nintl_resp &lt;- intl_req |&gt; req_perform()\nintl_data &lt;- resp_body_json(intl_resp)$`Monthly Adjusted Time Series`\n\n# Convert the fetched data into a data frame\nintl_equity_df &lt;- data.frame(\n  date = as.Date(names(intl_data)),\n  intl_equity_close = as.numeric(sapply(intl_data, function(x) x$`5. adjusted close`))\n)\n\n# Standardize dates to the first of each month\nintl_equity_df$date &lt;- as.Date(format(intl_equity_df$date, \"%Y-%m-01\"))\n\n# Filter for the past 15 years\nintl_equity_df &lt;- intl_equity_df |&gt; filter(date &gt;= Time_Montly)\n\n\n\n\nFetching Bond Returns from AlphaVantage\n\n\n\n\nShow the code\n# Fetch Bond Yield Data (Daily)\nbond_req &lt;- request(fred_url) |&gt;\n  req_url_query(\n    series_id = \"DGS10\", # 10-Year Treasury Yield\n    api_key = fred_key,\n    file_type = \"json\"\n  )\n\nbond_resp &lt;- bond_req |&gt; req_perform()\nbond_data &lt;- resp_body_json(bond_resp)$observations\n\n# Convert to DataFrame\nbond_df &lt;- data.frame(\n  date = as.Date(sapply(bond_data, function(x) x$date)),\n  bond_yield = as.numeric(sapply(bond_data, function(x) x$value))\n)\n\n# Downsample to Monthly Averages\nbond_df &lt;- bond_df |&gt;\n  mutate(month = format(date, \"%Y-%m\")) |&gt;# Extract year-month for grouping\n  group_by(month) |&gt;\n  summarize(\n    bond_yield = mean(bond_yield, na.rm = TRUE), # Average yield for the month\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(date = as.Date(paste0(month, \"-01\"))) |&gt;# Convert back to Date\n  select(date, bond_yield)\n\n# Filter for the last 15 years\nbond_df &lt;- bond_df |&gt; filter(date &gt;= Time_Montly)\n\n\n\n\nFetching Short-Term Debt Returns from AlphaVantage: for this dataset, we need to downsample the daily data to monthly averages to align with the other datasets.\n\n\n\n\nShow the code\n# Fetch Short-Term Debt Data (2-Year Treasury Yield) - Attempt Monthly Query\nshort_term_req &lt;- request(fred_url) |&gt;\n  req_url_query(\n    series_id = \"DGS2\", # 2-Year Treasury Yield\n    frequency = \"m\", # Request monthly data directly\n    api_key = fred_key,\n    file_type = \"json\"\n  )\n\nshort_term_resp &lt;- short_term_req |&gt; req_perform()\nshort_term_data &lt;- resp_body_json(short_term_resp)$observations\n\n# Convert JSON to DataFrame\nshort_term_df &lt;- data.frame(\n  date = as.Date(sapply(short_term_data, function(x) x$date), format = \"%Y-%m-%d\"),\n  short_term_yield = as.numeric(sapply(short_term_data, function(x) x$value))\n)\n\n# Filter for the Last 15 Years\nshort_term_df &lt;- short_term_df |&gt; filter(date &gt;= Time_Montly)"
  },
  {
    "objectID": "mp04.html#initial-setup-and-parameters",
    "href": "mp04.html#initial-setup-and-parameters",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Initial Setup and Parameters",
    "text": "Initial Setup and Parameters\n\nInitial Salary: $60,000.\nYears of Service: 15 years.\nSalary Growth: Modeled using year-over-year CPI growth, calculated from the combined_data dataset."
  },
  {
    "objectID": "mp04.html#trs-teacher-retirement-system-benefit-calculation",
    "href": "mp04.html#trs-teacher-retirement-system-benefit-calculation",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "TRS (Teacher Retirement System) Benefit Calculation",
    "text": "TRS (Teacher Retirement System) Benefit Calculation\n\nCalculated annual salaries over the 15-year period by applying CPI growth.\nDetermined the Final Average Salary (FAS) as the average of the last 3 years of salaries.\nComputed the Annual TRS Benefit based on:\n\nYears of Service:\n\n1.67% of FAS for each year of service up to 20 years.\nSpecial rules applied for exactly 20 years or more than 20 years.\n\nFAS: Multiplied by the benefit rate and years of service.\n\nDisplayed both the annual and monthly TRS benefits."
  },
  {
    "objectID": "mp04.html#contributions-employee-and-employer",
    "href": "mp04.html#contributions-employee-and-employer",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Contributions (Employee and Employer)",
    "text": "Contributions (Employee and Employer)\n\nCalculated annual employee and employer contributions based on salary and contribution rules:\n\nEmployee Contributions:\n\n3% to 6% of salary, determined by salary brackets.\n\nEmployer Contributions:\n\n8% for the first 7 years and 10% for subsequent years.\n\n\nSummed contributions for each year and totaled them over the 15-year period.\nStored detailed data in a table for yearly contributions."
  },
  {
    "objectID": "mp04.html#orp-optional-retirement-plan-investment-simulation",
    "href": "mp04.html#orp-optional-retirement-plan-investment-simulation",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "ORP (Optional Retirement Plan) Investment Simulation",
    "text": "ORP (Optional Retirement Plan) Investment Simulation\n\nPortfolio Allocation:\n\nAdjusted allocations for US equities, international equities, and bonds based on age brackets:\n\nAges 50-59: Higher equity allocation.\nAges 60-74: Increased bond allocation to have more stability.\n\n\nCalculated annual performance for each asset class using growth rates from the combined_data dataset:\n\nUS and international equity growth based on stock market performance.\nBond yields calculated as annual averages.\n\nSimulated yearly portfolio performance using contributions and compound growth.\nComputed the Total Accumulated Value at the end of 15 years.\nEstimated the Annual ORP Benefit as 4% of the final accumulated value.\n\n\nThe following code snippets will calculate the retirement benefits for both TRS and ORP based on the assumptions outlined above. We will compare the monthly benefits and accumulated funds for each plan to provide a comprehensive analysis of the retirement options.\n\n\nShow the code\n# Set initial salary and define years of service\ninitial_salary &lt;- 60000\nyears_of_service &lt;- 15  # Adjusted to 15 years\n\n# Initialize a vector to store salaries for each year\nsalaries &lt;- numeric(years_of_service)\nsalary_growth &lt;- numeric(years_of_service)\ncpi_growth &lt;- numeric(years_of_service)\nsalary_increase &lt;- numeric(years_of_service)\n\n# Set the first salary as the initial salary\nsalaries[1] &lt;- initial_salary\n\n# Loop through the combined data for yearly salary growth calculation\nfor (i in 2:years_of_service) {\n  # We need CPI data for each September to calculate the salary for the next year\n  # Calculate the CPI growth between September (from the previous year) and September of the current year\n  \n  september_cpi_previous_year &lt;- combined_data$inflation_rate[which(month(combined_data$date) == 9 & year(combined_data$date) == 2008 + (i - 1))]\n  september_cpi_current_year &lt;- combined_data$inflation_rate[which(month(combined_data$date) == 9 & year(combined_data$date) == 2008 + i)]\n  \n  # Calculate the salary growth rate based on CPI change\n  cpi_growth[i] &lt;- september_cpi_current_year / september_cpi_previous_year\n  salary_growth[i] &lt;- cpi_growth[i]\n  \n  # Calculate the increase in salary\n  salary_increase[i] &lt;- salaries[i-1] * (cpi_growth[i] - 1)  # Difference from the previous salary\n  \n  # Apply the salary growth to the previous year's salary\n  salaries[i] &lt;- salaries[i-1] * cpi_growth[i]  \n}\n\n# Calculate Final Average Salary (FAS) - which is the average of the last 3 years' salary\nfas &lt;- mean(salaries[(years_of_service-2):years_of_service])\n\n# Calculate the Annual Retirement Benefit based on years of service\nif (years_of_service &lt;= 20) {\n  annual_benefit_trs &lt;- 0.0167 * fas * years_of_service\n} else if (years_of_service == 20) {\n  annual_benefit_trs &lt;- 0.0175 * fas * years_of_service\n} else {\n  annual_benefit_trs &lt;- (0.35 + 0.02 * years_of_service) * fas\n}\n\n# Prepare the data for the table\ntrs_benefit_data &lt;- data.frame(\n  Benefit_Type = c(\"Annual TRS Benefit\", \"Monthly TRS Benefit\"),\n  Amount = c(round(annual_benefit_trs, 2), round(annual_benefit_trs / 12, 2))\n)\n\n# Create and display the table using gt\ntrs_benefit_data |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"TRS Benefit Summary\",\n    subtitle = \"Annual and Monthly TRS Benefits\"\n  ) |&gt;\n  fmt_currency(\n    columns = \"Amount\",\n    currency = \"USD\"\n  ) |&gt;\n  cols_label(\n    Benefit_Type = \"Benefit Type\",\n    Amount = \"Amount ($)\"\n  ) |&gt;\n  tab_options(\n    table.font.size = \"medium\",\n    heading.align = \"center\"\n  )\n\n\n\n\n\n\n\n\nTRS Benefit Summary\n\n\nAnnual and Monthly TRS Benefits\n\n\nBenefit Type\nAmount ($)\n\n\n\n\nAnnual TRS Benefit\n$21,233.31\n\n\nMonthly TRS Benefit\n$1,769.44\n\n\n\n\n\n\n\nWe can notice the money a professor would get from the TRS plan. Now we would calculate the contributions for the OPR plan.\n\n\nShow the code\n# Initialize parameters\ninitial_salary &lt;- 60000  # Starting salary\nyears_of_service &lt;- 15   # Number of years to simulate\n\n# Set up salary contribution brackets\nsalary_brackets &lt;- c(45000, 55000, 75000, 100000)\n\n# Initialize vectors to store results\nsalaries &lt;- numeric(years_of_service)\nemployee_contributions &lt;- numeric(years_of_service)\nemployer_contributions &lt;- numeric(years_of_service)\ntotal_contributions &lt;- numeric(years_of_service)\n\n# Initialize the first year's salary\nsalaries[1] &lt;- initial_salary\n\n# Loop through each year and compute the contributions\nfor (i in 1:years_of_service) {\n  \n  # Get the salary for the year\n  if (i == 1) {\n    salary &lt;- initial_salary\n  } else {\n    # Calculate salary growth based on CPI (only starting from the second year)\n    september_cpi_previous_year &lt;- combined_data$inflation_rate[which(month(combined_data$date) == 9 & year(combined_data$date) == 2009 + (i - 1))]\n    september_cpi_current_year &lt;- combined_data$inflation_rate[which(month(combined_data$date) == 9 & year(combined_data$date) == 2009 + i)]\n    \n    # Calculate the salary growth rate based on CPI change (year over year growth)\n    cpi_growth &lt;- september_cpi_current_year / september_cpi_previous_year\n    salary &lt;- salaries[i - 1] * cpi_growth  # Update salary based on CPI growth\n  }\n  \n  # Store the salary for each year\n  salaries[i] &lt;- salary\n  \n  # Employee contribution calculation based on salary brackets (annual contribution)\n  if (salary &lt;= salary_brackets[1]) {\n    employee_contribution_annual &lt;- salary * 0.03\n  } else if (salary &lt;= salary_brackets[2]) {\n    employee_contribution_annual &lt;- salary * 0.035\n  } else if (salary &lt;= salary_brackets[3]) {\n    employee_contribution_annual &lt;- salary * 0.045\n  } else if (salary &lt;= salary_brackets[4]) {\n    employee_contribution_annual &lt;- salary * 0.0575\n  } else {\n    employee_contribution_annual &lt;- salary * 0.06\n  }\n  \n  # Employer contribution: 8% for the first 7 years, 10% after (annual contribution)\n  if (i &lt;= 7) {\n    employer_contribution_annual &lt;- salary * 0.08\n  } else {\n    employer_contribution_annual &lt;- salary * 0.10\n  }\n  \n  # Total contribution for the year\n  total_contribution_annual &lt;- employee_contribution_annual + employer_contribution_annual\n  \n  # Store contributions\n  employee_contributions[i] &lt;- employee_contribution_annual\n  employer_contributions[i] &lt;- employer_contribution_annual\n  total_contributions[i] &lt;- total_contribution_annual\n}\n\n# Create the table with the results for each year\nyearly_contributions_data &lt;- data.frame(\n  Year = 1:years_of_service,\n  Salary = round(salaries, 2),\n  Employee_Contribution = round(employee_contributions, 2),\n  Employer_Contribution = round(employer_contributions, 2),\n  Total_Contribution = round(total_contributions, 2)\n)\n\n# Calculate cumulative contributions\nyearly_contributions_data$Cumulative_Employee_Contribution &lt;- cumsum(yearly_contributions_data$Employee_Contribution)\nyearly_contributions_data$Cumulative_Employer_Contribution &lt;- cumsum(yearly_contributions_data$Employer_Contribution)\nyearly_contributions_data$Cumulative_Total_Contribution &lt;- cumsum(yearly_contributions_data$Total_Contribution)\n\n# Visualization for cumulative contributions\nlibrary(ggplot2)\n\nggplot(yearly_contributions_data, aes(x = Year)) +\n  geom_line(aes(y = Cumulative_Employee_Contribution, color = \"Cumulative Employee Contribution\"), size = 1) +\n  geom_line(aes(y = Cumulative_Employer_Contribution, color = \"Cumulative Employer Contribution\"), size = 1) +\n  geom_line(aes(y = Cumulative_Total_Contribution, color = \"Cumulative Total Contribution\"), size = 1) +\n  labs(\n    title = \"Cumulative Contributions Over Time\",\n    x = \"Year of Service\",\n    y = \"Cumulative Contribution Amount ($)\",\n    color = \"Legend\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Initialize parameters for ORP calculations\n# Age 50 to 59\nequity_us_alloc_50_59 &lt;- 0.47  # US Equities allocation\nequity_international_alloc_50_59 &lt;- 0.32  # International Equities allocation\nbonds_alloc_50_59 &lt;- 0.21  # Bonds allocation\n\n# Age 60 to 74\nequity_us_alloc_60_74 &lt;- 0.34  # US Equities allocation\nequity_international_alloc_60_74 &lt;- 0.23  # International Equities allocation\nbonds_alloc_60_74 &lt;- 0.43  # Bonds allocation\n\n# Initialize vectors to store results\nperformance_us_equity &lt;- numeric(years_of_service)\nperformance_international_equity &lt;- numeric(years_of_service)\nperformance_bonds &lt;- numeric(years_of_service)\ntotal_accumulated &lt;- numeric(years_of_service)  # Total accumulated amount\n\n# Starting balance\ntotal_accumulated[1] &lt;- yearly_contributions_data$Total_Contribution[1]\n\n# Start contributing at age 50 in 2009\nstarting_age &lt;- 50\n\n# Loop through each year to compute performance\nfor (i in 1:years_of_service) {\n  current_year &lt;- 2009 + i - 1  # Current year\n  next_year &lt;- current_year + 1  # Next year\n  current_age &lt;- starting_age + i - 1  # Current age\n  \n  # Assign allocations based on age\n  if (current_age &gt;= 50 && current_age &lt;= 59) {\n    equity_us_alloc &lt;- equity_us_alloc_50_59\n    equity_international_alloc &lt;- equity_international_alloc_50_59\n    bonds_alloc &lt;- bonds_alloc_50_59\n  } else if (current_age &gt;= 60 && current_age &lt;= 74) {\n    equity_us_alloc &lt;- equity_us_alloc_60_74\n    equity_international_alloc &lt;- equity_international_alloc_60_74\n    bonds_alloc &lt;- bonds_alloc_60_74\n  }\n  \n  # Ensure we have data for the future January, or use November of the current year if not available\n  if (i == years_of_service) {\n    # For the last year, use November data instead of January of the next year\n    us_equity_growth &lt;- combined_data$us_equity_close[which(year(combined_data$date) == current_year & month(combined_data$date) == 11)] /\n                        combined_data$us_equity_close[which(year(combined_data$date) == current_year & month(combined_data$date) == 1)]\n    \n    intl_equity_growth &lt;- combined_data$intl_equity_close[which(year(combined_data$date) == current_year & month(combined_data$date) == 11)] /\n                          combined_data$intl_equity_close[which(year(combined_data$date) == current_year & month(combined_data$date) == 1)]\n    \n    bond_yield_annual &lt;- mean(combined_data$bond_yield[which(year(combined_data$date) == current_year & month(combined_data$date) &lt;= 11)]) / 100 + 1\n  } else {\n    # For all other years, use January of the next year\n    us_equity_growth &lt;- combined_data$us_equity_close[which(year(combined_data$date) == next_year & month(combined_data$date) == 1)] /\n                        combined_data$us_equity_close[which(year(combined_data$date) == current_year & month(combined_data$date) == 1)]\n    \n    intl_equity_growth &lt;- combined_data$intl_equity_close[which(year(combined_data$date) == next_year & month(combined_data$date) == 1)] /\n                          combined_data$intl_equity_close[which(year(combined_data$date) == current_year & month(combined_data$date) == 1)]\n    \n    bond_yield_annual &lt;- mean(combined_data$bond_yield[which(year(combined_data$date) == current_year)]) / 100 + 1\n  }\n\n  # Store individual performance values\n  performance_us_equity[i] &lt;- us_equity_growth\n  performance_international_equity[i] &lt;- intl_equity_growth\n  performance_bonds[i] &lt;- bond_yield_annual  # Already adjusted with +1\n  \n  # Calculate total accumulated with contributions and compound effect\n  if (i == 1) {\n    # First year: Apply performance to the first contribution\n    total_accumulated[i] &lt;- yearly_contributions_data$Total_Contribution[i] * (\n                              (equity_us_alloc * performance_us_equity[i]) +\n                              (equity_international_alloc * performance_international_equity[i]) +\n                              (bonds_alloc * performance_bonds[i])\n                            )\n  } else {\n    # Subsequent years: Include previous accumulated total and current contributions\n    total_accumulated[i] &lt;- (total_accumulated[i - 1] + yearly_contributions_data$Total_Contribution[i]) * (\n                              (equity_us_alloc * performance_us_equity[i]) +\n                              (equity_international_alloc * performance_international_equity[i]) +\n                              (bonds_alloc * performance_bonds[i])\n                            )\n  }\n}\n\n# Create a data frame with the results\nyearly_performance_data &lt;- data.frame(\n  Year = 2009:(2009 + years_of_service - 1),\n  Age = starting_age:(starting_age + years_of_service - 1),\n  US_Equity_Performance = round(performance_us_equity, 4),\n  International_Equity_Performance = round(performance_international_equity, 4),\n  Bonds_Performance = round(performance_bonds, 4),\n  Total_Accumulated = round(total_accumulated, 2)\n)\nlibrary(gt)\n\nannual_benefit_opr &lt;- 0.04 * yearly_performance_data$Total_Accumulated[years_of_service]\nmonthly_benefit_opr &lt;- annual_benefit_opr / 12\n\nopr_benefit_table &lt;- data.frame(\n  Metric = c(\"Annual OPR Benefit\", \"Monthly OPR Benefit\"),\n  Amount = c(round(annual_benefit_opr, 2), round(monthly_benefit_opr, 2))\n)\n\nopr_benefit_table |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"OPR Benefit Summary\",\n    subtitle = paste(\"Year\", years_of_service, \"Calculation\")\n  ) |&gt;\n  fmt_currency(\n    columns = \"Amount\",\n    currency = \"USD\"\n  ) |&gt;\n  cols_label(\n    Metric = \"Benefit Type\",\n    Amount = \"Amount ($)\"\n  ) |&gt;\n  tab_style(\n    style = list(\n      cell_text(weight = \"bold\")\n    ),\n    locations = cells_column_labels(everything())\n  )\n\n\n\n\n\n\n\n\nOPR Benefit Summary\n\n\nYear 15 Calculation\n\n\nBenefit Type\nAmount ($)\n\n\n\n\nAnnual OPR Benefit\n$10,600.32\n\n\nMonthly OPR Benefit\n$883.36\n\n\n\n\n\n\n\n\n\nShow the code\n# Calculate total accumulated contributions over time\ntotal_contributions &lt;- cumsum(yearly_contributions_data$Total_Contribution)\n\n# Create a data frame for plotting\nplot_data &lt;- data.frame(\n  Year = 2009:(2009 + years_of_service - 1),\n  Total_Accumulated = total_accumulated,\n  Total_Contributions = total_contributions\n)\n\n# Create the interactive graph\nplot &lt;- plot_ly(plot_data, x = ~Year) |&gt;\n  add_trace(y = ~Total_Accumulated, type = 'scatter', mode = 'lines+markers', name = 'Compounding $',\n            hovertemplate = 'Year: %{x}&lt;br&gt;Total Accumulated: %{y:.2f}&lt;extra&gt;&lt;/extra&gt;') |&gt;\n  add_trace(y = ~Total_Contributions, type = 'scatter', mode = 'lines+markers', name = 'Total Contributions',\n            hovertemplate = 'Year: %{x}&lt;br&gt;Total Contributions: %{y:.2f}&lt;extra&gt;&lt;/extra&gt;') |&gt;\n  layout(\n    title = 'Interactive Graph: Total Accumulation vs. Contributions',\n    xaxis = list(title = 'Year'),\n    yaxis = list(title = 'Amount ($)'),\n    legend = list(orientation = 'h', x = 0.5, xanchor = 'center')\n  )\n\nplot\n\n\n\n\n\n\nIn this image we can see clearly the power of compounding. The total accumulated amount grows over time due to the contributions and the compound effect of the investment returns. This visualization provides a clear representation of how the retirement savings can grow over the years, highlighting the importance of consistent contributions and wise investment choices."
  },
  {
    "objectID": "mp04.html#conclusion-about-the-two-programs",
    "href": "mp04.html#conclusion-about-the-two-programs",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Conclusion About the two programs",
    "text": "Conclusion About the two programs\n\nAverage Monthly Income:\n\nTRS provides significantly higher average monthly income ($2,580.50) compared to ORP ($1,414.74), making it more reliable for covering living expenses.\n\nIncome Gap:\n\nThe maximum monthly income gap is $1,485.45, while the minimum is $886.08, showing that TRS consistently outperforms ORP.\n\nKey Insight:\n\nTRS offers superior income stability and growth, while ORP’s lower payouts may struggle to meet retirement needs over time."
  },
  {
    "objectID": "mp04.html#simulation-overview",
    "href": "mp04.html#simulation-overview",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Simulation Overview",
    "text": "Simulation Overview\nWe aim to assess the impact of market conditions and withdrawal rates on the ORP balance over time and evaluate the plan’s sustainability under different scenarios.\n\nKey Considerations\n\nDeath Age: 90 years\n\nRetirement Age: 65 years\n\nWithdrawal Rates: 3% to 10%\n\nWe will evaluate the impact of different withdrawal rates on the ORP balance over time.\n\n\nNumber of Simulations: 200\n\nThis ensures a comprehensive analysis by capturing a wide range of potential outcomes.\n\n\n\n\nObjectives\n\nSimulate Market Conditions:\nUse bootstrapped historical data to reflect various possible market scenarios.\nEvaluate Withdrawal Strategies:\nAnalyze how different withdrawal rates impact the sustainability of the ORP plan.\nAssess Performance:\nProvide insights into the range of possible retirement benefits and identify risks or trends under varying scenarios.\n\nThis analysis will allow for a better understanding of the ORP plan’s robustness and help in making informed decisions for financial planning in retirement.\n\n\nShow the code\n# Parameters\nretirement_age &lt;- 65  # Retirement start age\ndeath_age &lt;- 90       # End of simulation age\nyears_in_retirement &lt;- death_age - retirement_age\n\n# Step 1: Initial ORP Balance\norp_balance &lt;- yearly_performance_data$Total_Accumulated[nrow(yearly_performance_data)]\nif (orp_balance == 0 || is.na(orp_balance)) stop(\"ERROR: ORP Balance is zero or NA.\")\n# Portfolio allocations\nequity_us_alloc &lt;- 0.34\nequity_international_alloc &lt;- 0.23\nbonds_alloc &lt;- 0.43\nif ((equity_us_alloc + equity_international_alloc + bonds_alloc) != 1) stop(\"ERROR: Allocations must sum to 1.\")\n\n# Step 2: Bootstrap CPI Growth\nseptember_cpi &lt;- combined_data$inflation_rate[month(combined_data$date) == 9]\ncpi_growth &lt;- september_cpi[-1] / september_cpi[-length(september_cpi)] - 1\nset.seed(42)\nsampled_inflation &lt;- replicate(200, sample(cpi_growth, years_in_retirement, replace = TRUE))\n\n# Step 3: Sample Returns\nsampled_us_returns &lt;- replicate(200, sample(yearly_performance_data$US_Equity_Performance, years_in_retirement, replace = TRUE))\nsampled_intl_returns &lt;- replicate(200, sample(yearly_performance_data$International_Equity_Performance, years_in_retirement, replace = TRUE))\nsampled_bond_returns &lt;- replicate(200, sample(yearly_performance_data$Bonds_Performance, years_in_retirement, replace = TRUE))\n\n# Step 4: Simulate for Withdrawal Rates (3% to 10%)\nwithdrawal_rates &lt;- seq(0.03, 0.10, by = 0.01)\n\n\ntrs_benefits &lt;- matrix(0, nrow = 200, ncol = years_in_retirement)\n\n# Initialize storage for all results\nall_withdrawal_results &lt;- list()\nall_balance_results &lt;- list()  # To store ORP balances for each withdrawal rate\n\nfor (rate in withdrawal_rates) {\n  orp_withdrawals &lt;- matrix(0, nrow = 200, ncol = years_in_retirement)  # Store withdrawals\n  orp_balances &lt;- matrix(0, nrow = 200, ncol = years_in_retirement)    # Store balances for each withdrawal rate\n  \n  for (sim in 1:200) {\n    orp_sim_balance &lt;- orp_balance  # Reset balance for each simulation\n    \n    for (year in 1:years_in_retirement) {\n      # TRS Benefit Adjustment\n      if (year == 1) {\n        trs_benefit &lt;- trs_annual_benefit\n      } else {\n        inflation_adjustment &lt;- min(max(sampled_inflation[year, sim] * 0.5, 0.01), 0.03)\n        trs_benefit &lt;- trs_benefit * (1 + inflation_adjustment)\n      }\n      trs_benefits[sim, year] &lt;- trs_benefit\n\n      # ORP Withdrawal Calculation\n      withdrawal &lt;- orp_sim_balance * rate\n      orp_withdrawals[sim, year] &lt;- withdrawal\n      remaining_balance &lt;- max(orp_sim_balance - withdrawal, 0)\n      orp_balances[sim, year] &lt;- remaining_balance  # Save balance for the simulation\n      \n      if (remaining_balance &lt;= 0) break  # Stop simulation if balance is depleted\n\n      # Apply Weighted Returns\n      weighted_return &lt;- (equity_us_alloc * (sampled_us_returns[year, sim] - 1)) +\n                         (equity_international_alloc * (sampled_intl_returns[year, sim] - 1)) +\n                         (bonds_alloc * (sampled_bond_returns[year, sim] - 1))\n      orp_sim_balance &lt;- remaining_balance * (1 + weighted_return)\n    }\n  }\n\n  # Store Average Withdrawals\n  avg_withdrawals &lt;- colMeans(orp_withdrawals, na.rm = TRUE)\n  result &lt;- data.frame(\n    Year = retirement_age:(death_age - 1),\n    Average_Withdrawal = avg_withdrawals,\n    Withdrawal_Rate = paste0(round(rate * 100, 1), \"%\")\n  )\n  all_withdrawal_results[[paste0(\"Rate_\", round(rate * 100, 1))]] &lt;- result\n  \n  # Store the ORP Balances for the Current Rate\n  all_balance_results[[paste0(\"Rate_\", round(rate * 100, 1))]] &lt;- orp_balances\n}\n\n# Combine Withdrawal Results for Visualization\ncombined_withdrawal_results &lt;- do.call(rbind, all_withdrawal_results)\n\n# Add TRS Results\ncombined_trs_results &lt;- data.frame(\n  Year = retirement_age:(death_age - 1),\n  Average_Withdrawal = colMeans(trs_benefits, na.rm = TRUE),\n  Withdrawal_Rate = \"TRS Benefit\",\n  Line_Type = \"dashed\",\n  Legend = \"TRS Benefit\"\n)\n\n# Adjust ORP Results for Consistency\ncombined_withdrawal_results$Line_Type &lt;- \"solid\"\ncombined_withdrawal_results$Legend &lt;- combined_withdrawal_results$Withdrawal_Rate\n\n# Merge Results\nfinal_combined_results &lt;- rbind(combined_withdrawal_results, combined_trs_results)\nfinal_combined_results$Legend &lt;- factor(\n  final_combined_results$Legend, \n  levels = c(\"3%\", \"4%\", \"5%\", \"6%\", \"7%\", \"8%\", \"9%\", \"10%\", \"TRS Benefit\")\n)\n\n# Plot Withdrawals\nwithdrawal_plot &lt;- ggplot(final_combined_results, aes(x = Year, y = Average_Withdrawal, color = Legend, linetype = Line_Type, group = Legend)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"blue\", \"green\", \"red\", \"purple\", \"orange\", \"cyan\", \"brown\", \"pink\", \"black\")) +\n  scale_linetype_identity() +\n  labs(\n    title = \"Comparison of ORP Withdrawals and TRS Benefits\",\n    x = \"Year of Retirement\",\n    y = \"Average Withdrawal/Benefit ($)\",\n    color = \"Legend\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_y_continuous(labels = scales::dollar_format())\n\n# Combine Balance Results for Visualization\ncombined_balance_results &lt;- do.call(rbind, lapply(seq_along(all_balance_results), function(i) {\n  avg_balance &lt;- colMeans(all_balance_results[[i]], na.rm = TRUE)  # Compute average balances\n  data.frame(\n    Year = retirement_age:(death_age - 1),\n    Average_Balance = avg_balance,\n    Withdrawal_Rate = paste0(round(withdrawal_rates[i] * 100, 1), \"%\")  # Match withdrawal rate\n  )\n}))\n\n# Plot Balances\nbalance_plot &lt;- ggplot(combined_balance_results, aes(x = Year, y = Average_Balance, color = Withdrawal_Rate, group = Withdrawal_Rate)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"blue\", \"green\", \"red\", \"purple\", \"orange\", \"cyan\", \"brown\", \"pink\")) +\n  labs(\n    title = \"Comparison of ORP Balances Across Withdrawal Rates\",\n    x = \"Year of Retirement\",\n    y = \"Average ORP Balance ($)\",\n    color = \"Withdrawal Rate\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_y_continuous(labels = scales::dollar_format())\n\n\n# Display Plots\nwithdrawal_plot\n\n\n\n\n\n\n\n\n\nWe clearly see that the TRS is a better model for retirement. The TRS benefit remains stable and increases over time due to inflation adjustments, providing a reliable income source for retirees. Also, we decided to compare the ORP withdrawals at different rates. Appreciating a huge gap just by changing one percent. Making it a clear factor when comes down to retirement planning.\n\n\nShow the code\nbalance_plot\n\n\n\n\n\n\n\n\n\nAlso, we want to see the average balance of the ORP plan over time. The line graph illustrates the average ORP balances across different withdrawal rates over the retirement period. Key insights from the visualization are as follows:\n\nWithdrawal Rates:\n\nThe graph shows the average ORP balance for withdrawal rates ranging from 3% to 10%.\nThe 6% withdrawal rate maintains a stable balance over time, providing a consistent income stream for retirees, and still increase its value.\nThe 10% withdrawal rate depletes the ORP savings too quickly, indicating a high-risk strategy.\nThe 3% withdrawal rate is overly conservative, resulting in a slow depletion of savings and lower income.\n\n\nThis plan is recommended for those who want to have a stable income over time and may want to liquidate in the future or pass it on to their partners or children.\nAfter getting this information, we want to make sure that the ORP plan is sustainable and will not run out of money before the retiree passes away. In that way, we would make sure that the retiree will have a stable income over time.\n\n\nShow the code\n# Split orp_balances into a list based on withdrawal rates\norp_balances &lt;- split(orp_balances, rep(1:length(withdrawal_rates), each = years_in_retirement))\n\n# Convert each list element to a matrix for simulation\norp_balances &lt;- lapply(orp_balances, function(x) matrix(x, nrow = 200, ncol = years_in_retirement))\n\n# Calculate probabilities of exhaustion for each withdrawal rate\nexhaust_probabilities &lt;- sapply(seq_along(withdrawal_rates), function(rate_index) {\n  current_balance &lt;- orp_balances[[rate_index]]  # Get balances for the current rate\n  exhausted &lt;- apply(current_balance, 1, function(sim_row) any(sim_row[1:(years_in_retirement - 1)] == 0))  # Check if exhausted\n  mean(exhausted) * 100  # Return percentage of exhausted simulations\n})\n\n# Create a data frame for results\nexhaustion_table &lt;- data.frame(\n  Withdrawal_Rate = paste0(round(withdrawal_rates * 100, 1), \"%\"),\n  Probability = as.numeric(round(exhaust_probabilities, 2))\n)\n\n# Load the gt library for table formatting\nlibrary(gt)\n\n# Generate a formatted table with results\ngt_exhaustion_table &lt;- exhaustion_table |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Probability of ORP Savings Exhaustion\",\n    subtitle = \"Before Death by Withdrawal Rate\"\n  ) |&gt;\n  fmt_number(\n    columns = c(Probability),  # Updated syntax\n    decimals = 2\n  ) |&gt;\n  cols_label(\n    Withdrawal_Rate = \"Withdrawal Rate\",\n    Probability = \"Exhaustion Probability (%)\"\n  ) |&gt;\n  tab_source_note(\n    source_note = \"Data simulated using Monte Carlo bootstrapping.\"\n  )\n\n# Display the table\ngt_exhaustion_table\n\n\n\n\n\n\n\n\nProbability of ORP Savings Exhaustion\n\n\nBefore Death by Withdrawal Rate\n\n\nWithdrawal Rate\nExhaustion Probability (%)\n\n\n\n\n3%\n0.00\n\n\n4%\n0.00\n\n\n5%\n0.00\n\n\n6%\n0.00\n\n\n7%\n0.00\n\n\n8%\n0.00\n\n\n9%\n0.00\n\n\n10%\n0.00\n\n\n\nData simulated using Monte Carlo bootstrapping.\n\n\n\n\n\n\n\n\nIn all cases, the ORP plan is sustainable. However, we have to take into consideration that the models with a that 7 percent, would have a negative growth in terms of withdrawals. This means that the retiree would have to adjust the withdrawal rate to a lower percentage to make sure that the ORP plan is sustainable.\nWe want to keep digging on the ORP plan and see if the retiree would have a higher income than the TRS plan.\n\n\nShow the code\n# Initialize vector to store probabilities for each withdrawal rate\nhigher_income_probabilities &lt;- numeric(length(withdrawal_rates))\n\n# Loop through withdrawal rates\nfor (i in seq_along(withdrawal_rates)) {\n  rate &lt;- withdrawal_rates[i]\n  higher_income_count &lt;- 0\n  \n  # Iterate over years\n  for (year in 1:years_in_retirement) {\n    higher_income_count &lt;- higher_income_count + sum(orp_withdrawals[, year] &gt; trs_benefits[, year])\n  }\n  \n  # Calculate probability\n  higher_income_probabilities[i] &lt;- higher_income_count / (200 * years_in_retirement) * 100\n}\n\n# Create a table\nhigher_income_table &lt;- data.frame(\n  Withdrawal_Rate = paste0(round(withdrawal_rates * 100, 1), \"%\"),\n  Probability = round(higher_income_probabilities, 2)\n)\n\n# Display the table\ngt &lt;- gt(higher_income_table) |&gt;\n  tab_header(\n    title = \"Probability of Higher ORP Income vs. TRS Benefit\",\n    subtitle = \"Based on Different Withdrawal Rates\"\n  ) |&gt;\n  cols_label(\n    Withdrawal_Rate = \"Withdrawal Rate\",\n    Probability = \"Higher Income Probability (%)\"\n  ) |&gt;\n  tab_spanner(label = \"Higher Income Probability (%)\", columns = c(2)) |&gt;\n  tab_options(\n    table.font.size = \"medium\",\n    heading.align = \"center\"\n  )\ngt\n\n\n\n\n\n\n\n\nProbability of Higher ORP Income vs. TRS Benefit\n\n\nBased on Different Withdrawal Rates\n\n\nWithdrawal Rate\nHigher Income Probability (%)\n\n\nHigher Income Probability (%)\n\n\n\n\n3%\n24.5\n\n\n4%\n24.5\n\n\n5%\n24.5\n\n\n6%\n24.5\n\n\n7%\n24.5\n\n\n8%\n24.5\n\n\n9%\n24.5\n\n\n10%\n24.5\n\n\n\n\n\n\n\nAs we can see, the percentage doesn’t change on all the withdrawal rates. This means that the OPR could have better months than the TRS. However, as we saw in the previous graphs, that just would happens if the retiree has a higher withdrawal rate and would just happens for the first few years."
  },
  {
    "objectID": "mp04.html#comparison-of-trs-and-orp-income",
    "href": "mp04.html#comparison-of-trs-and-orp-income",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Comparison of TRS and ORP Income",
    "text": "Comparison of TRS and ORP Income\n\nTRS Income:\n\nTRS income grows steadily, driven by inflation adjustments, ensuring higher and more consistent income throughout retirement.\n\nORP Withdrawals:\n\nORP withdrawals grow modestly and lack inflation adjustments, leading to slower income growth over time.\n\nIncome Gap:\n\nThe gap between TRS and ORP widens significantly, with TRS consistently providing higher income.\n\nKey Insight:\n\nTRS is better for retirees needing stable, inflation-protected income, while ORP offers lower growth and purchasing power over time.\n\n\n\n\nShow the code\n# Plot ORP remaining balance over time\norp_balance_plot &lt;- results |&gt;\n  ggplot(aes(x = Year, y = ORP_Balance_Remaining)) +\n  geom_line(color = \"blue\", size = 1.2) +\n  geom_point(size = 2) +\n  labs(\n    title = \"Growth of ORP Investment Balance Over Time\",\n    x = \"Year of Retirement\",\n    y = \"ORP Balance Remaining ($)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 14)\n  ) +\n  scale_y_continuous(labels = scales::dollar_format())\n\n# Display the ORP balance plot\norp_balance_plot\n\n\n\n\n\n\n\n\n\nWhile TRS may provide a higher and more stable income, the ORP plan demonstrates strong growth in its investment balance over time. This highlights the plan’s long-term sustainability. Additionally, ORP offers greater flexibility, giving employees more control over their investments.\nOne key advantage of ORP is that employees can access the full balance if needed, providing the freedom to use the funds as desired, whether for unexpected expenses, large purchases, or other financial goals. This flexibility makes ORP a viable option for those who value control over their retirement savings."
  },
  {
    "objectID": "mp04.html#comparative-analysis-of-trs-and-orp",
    "href": "mp04.html#comparative-analysis-of-trs-and-orp",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Comparative Analysis of TRS and ORP",
    "text": "Comparative Analysis of TRS and ORP\n\n\nShow the code\n# Calculate monthly benefits\nmonthly_benefit_opr &lt;- annual_benefit_opr / 12\nmonthly_benefit_trs &lt;- annual_benefit_trs / 12\n\n# Create a data frame for plotting\nbenefit_data &lt;- data.frame(\n  Benefit_Type = c(\"Monthly OPR Benefit\", \"Monthly TRS Benefit\"),\n  Amount = c(monthly_benefit_opr, monthly_benefit_trs)\n)\n\n# Create the bar plot\nbenefit_plot &lt;- ggplot(benefit_data, aes(x = Benefit_Type, y = Amount, fill = Benefit_Type)) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) +\n  geom_text(aes(label = paste0(\"$\", round(Amount, 2))), vjust = -0.5, size = 4) +\n  labs(\n    title = \"Comparison of Monthly OPR and TRS Benefits\",\n    x = \"Benefit Type\",\n    y = \"Amount ($)\"\n  ) +\n  theme_minimal()\n\n# Print the plot\nprint(benefit_plot)\n\n\n\n\n\n\n\n\n\nThe bar chart illustrates a comparison between the Monthly OPR Benefit and the Monthly TRS Benefit. Key insights from the visualization are as follows:\n\nMonthly OPR Benefit:\n\nThe monthly benefit derived from the OPR system is $883.35, as shown by the first bar.\nThis represents a smaller portion compared to the TRS benefit, reflecting the structure of OPR benefit allocations.\n\nMonthly TRS Benefit:\n\nThe monthly benefit derived from the TRS system is $1769.43, as shown by the taller second bar.\nThis indicates a significantly higher payout, suggesting that the TRS system may provide a more robust financial benefit for retirees compared to the OPR system.\n\nComparison:\n\nThe TRS benefit is nearly double the OPR benefit, highlighting a substantial disparity between the two systems in terms of monthly payouts.\n\n\nThis comparison provides valuable insight into the financial implications of selecting between the OPR and TRS systems, with the TRS system offering higher monthly benefits. This can serve as a crucial factor for decision-making in retirement planning."
  },
  {
    "objectID": "final_project.html",
    "href": "final_project.html",
    "title": "Airbnb Project / Walkability Index",
    "section": "",
    "text": "Does the walkability index of NYC affects the listing pricing?\n\n\nThis project investigates how walkability influences Airbnb pricing in New York City. By integrating walkability indices with Airbnb listings data, we aim to uncover patterns that can benefit both hosts and travelers.\n\nHosts: Leverage walkability insights to optimize pricing.\nTravelers: Understand walkability’s impact on accommodation choices and budgets.\n\n\n\n\nHigher walkability positively correlates with higher Airbnb prices due to convenience and proximity to amenities.\n\n\n\n\nData Collection: Combine Airbnb listing data and EPA Walkability Index.\nData Processing: Merge datasets for analysiS and add spatial features.\nData Cleaning: Remove outliers, handle missing values, and preprocess data for analysis.\nExploratory Data Analysis: Use visualization methods to explore correlations and trends in a spatial context.(New York City)\nModeling: Conduct linear regression to quantify walkability’s impact on pricing."
  },
  {
    "objectID": "final_project.html#libraries",
    "href": "final_project.html#libraries",
    "title": "Airbnb Project / Walkability Index",
    "section": "Libraries",
    "text": "Libraries\n\n\nShow the code\nlibrary(dplyr)       # Data manipulation\nlibrary(sf)          # Handling spatial data\nlibrary(geosphere)   # Geospatial calculations\nlibrary(httr)        # HTTP requests\nlibrary(tigris)      # Census and geographic data\nlibrary(ggplot2)     # Visualization\nlibrary(leaflet)     # Interactive mapping\nlibrary(gt)          # Table formatting"
  },
  {
    "objectID": "final_project.html#cleaning-and-processing-the-airbn-data-sets",
    "href": "final_project.html#cleaning-and-processing-the-airbn-data-sets",
    "title": "Airbnb Project",
    "section": "Cleaning and processing the Airbn Data sets",
    "text": "Cleaning and processing the Airbn Data sets\n\n\nShow the code\n# Combine Airbnb datasets\nairbnb_combined &lt;- bind_rows(df_2019, df_2023)\n\n# Select only the necessary columns\nairbnb_cleaned &lt;- airbnb_combined |&gt;\n  select(neighbourhood_group, neighbourhood, latitude, longitude, room_type, price)\n\n# Filter rows to include only 'Entire home/apt' and 'Private room'\nairbnb_filtered &lt;- airbnb_cleaned |&gt;\n  filter(room_type %in% c(\"Private room\"))\n\n\nOur data set has the following columns: Rows: 89,893 Columns: 6"
  },
  {
    "objectID": "final_project.html#datasets-used",
    "href": "final_project.html#datasets-used",
    "title": "Airbnb Project",
    "section": "Datasets Used",
    "text": "Datasets Used\n\nAirbnb Listings (2019)\nNew York City Airbnb Open Data (2019)\nAirbnb Listings (2023)\nNew York City Airbnb 2023 Public Data\nWalkability Index\nWalkability Index Dataset (EPA)"
  },
  {
    "objectID": "final_project.html#objective",
    "href": "final_project.html#objective",
    "title": "Airbnb Project / Walkability Index",
    "section": "",
    "text": "This project investigates how walkability influences Airbnb pricing in New York City. By integrating walkability indices with Airbnb listings data, we aim to uncover patterns that can benefit both hosts and travelers.\n\nHosts: Leverage walkability insights to optimize pricing.\nTravelers: Understand walkability’s impact on accommodation choices and budgets."
  },
  {
    "objectID": "final_project.html#how-to-download-the-data",
    "href": "final_project.html#how-to-download-the-data",
    "title": "Airbnb Project",
    "section": "How to Download the Data",
    "text": "How to Download the Data\n\nAirbnb Data\nYou can download the Airbnb datasets directly using the provided links: - Airbnb 2019 Dataset\n- Airbnb 2023 Dataset\nWe would use the following datasets for this project:\n\n\nShow the code\ndf_2019 &lt;- read.csv(\"/Users/camilomartinez/STA9750-2024-FALL/Data for this project/AB_NYC_2019.csv\", header = TRUE, sep = \",\", stringsAsFactors = FALSE)\ndf_2023&lt;- read.csv(\"/Users/camilomartinez/STA9750-2024-FALL/Data for this project/NYC-Airbnb-2023.csv\", header = TRUE, sep = \",\", stringsAsFactors = FALSE)\n\n\nNow we proceed to download the Walkability Index data from the EPA API.\nThe documentation for the API can be found here. The following code would help you download the data from the API."
  },
  {
    "objectID": "final_project.html#adding-spatial-features-to-airbnb-data",
    "href": "final_project.html#adding-spatial-features-to-airbnb-data",
    "title": "Airbnb Project / Walkability Index",
    "section": "Adding Spatial Features to Airbnb Data",
    "text": "Adding Spatial Features to Airbnb Data\nTo incorporate geographic context into the Airbnb dataset, we:\n\nDownload Census Block Groups (CBGs): Use the tigris package to get New York State CBGs.\nPerform a Spatial Join: Map Airbnb latitude/longitude points to their corresponding CBGs.\nAdd GEOID: Assign each Airbnb entry a GEOID, aligning with the Walkability Index’s GEOID20 for seamless merging.\n\nThis enhances the Airbnb dataset with geographic information for further analysis.\n\n\nShow the code\n# Download Census Block Groups for New York State\nny_block_groups &lt;- block_groups(state = \"NY\", year = 2020, cb = TRUE)\n\n# Check if CRS is undefined\nif (is.na(st_crs(ny_block_groups))) {\n  stop(\"CRS for ny_block_groups is undefined. Define it before proceeding.\")\n}\n\n# Assign WGS84 CRS to airbnb_filtered if it uses lat/lon\nif (is.na(st_crs(airbnb_filtered))) {\n  airbnb_sf &lt;- st_as_sf(airbnb_filtered, coords = c(\"longitude\", \"latitude\"), crs = 4326)\n} else {\n  airbnb_sf &lt;- st_as_sf(airbnb_filtered, coords = c(\"longitude\", \"latitude\"))\n}\n\n# Transform CRS of airbnb_sf to match ny_block_groups if necessary\nif (!st_crs(airbnb_sf)$epsg == st_crs(ny_block_groups)$epsg) {\n  airbnb_sf &lt;- st_transform(airbnb_sf, crs = st_crs(ny_block_groups))\n}\n\n# Perform spatial join to associate attributes\nairbnb_with_block &lt;- st_join(airbnb_sf, ny_block_groups, join = st_within)\n\n# Replace point geometries with the corresponding block group geometries\nairbnb_with_block$geometry &lt;- ny_block_groups$geometry[match(\n  airbnb_with_block$GEOID, ny_block_groups$GEOID\n)]\n\n#Now we proceed to merge the walkability index data with the airbnb data set.\n\n# Step 1: Remove `sf` class from walkability\nwalkability_no_sf &lt;- st_drop_geometry(walkability)\n\n# Step 2: Perform attribute join\nairbnb_with_walkability &lt;- airbnb_with_block |&gt;\n  left_join(walkability_no_sf, by = c(\"GEOID\" = \"GEOID20\"))"
  },
  {
    "objectID": "final_project.html#model-interpretation",
    "href": "final_project.html#model-interpretation",
    "title": "Airbnb Project",
    "section": "Model Interpretation",
    "text": "Model Interpretation\n\nIntercept:\n\nThe intercept (67.775) represents the expected average price when the Walkability Index is zero. While statistically significant (P-Value = 0.000), this value may not have practical relevance, as Walkability Index values typically do not reach zero.\n\nWalkability Index:\n\nThe coefficient for the Walkability Index (0.330) suggests that for each one-unit increase in the Walkability Index, the average price increases by $0.33.\nThis relationship is statistically significant (P-Value = 0.016), indicating a small but meaningful impact on price.\n\nR-Squared:\n\nThe R-squared value (0.001) indicates that only 0.1% of the variation in Airbnb prices can be explained by the Walkability Index.\nThis low value suggests that the Walkability Index alone is not a strong predictor of Airbnb prices, and other factors are likely influencing price variations."
  },
  {
    "objectID": "final_project.html#key-insights",
    "href": "final_project.html#key-insights",
    "title": "Airbnb Project",
    "section": "Key Insights",
    "text": "Key Insights\n\nWhile there is a statistically significant relationship between the Walkability Index and Airbnb prices, the effect size is small, and the Walkability Index is not a major determinant of pricing.\nAdditional analysis incorporating other variables (e.g., neighborhood characteristics, amenities, room size) would likely improve the model’s explanatory power.\n\nBased on the initial hypothesis, the results suggest that while walkability does have a statistically significant, rather small, effect on Airbnb pricing in New York City, it does not play a dominant role in determining prices. The Walkability Index alone accounts for a minimal portion of the price variation, supporting the idea that other factors, such as location, neighborhood amenities, and room features, are more influential in shaping Airbnb prices. Further investigation into these additional variables is recommended to better understand the factors driving Airbnb pricing."
  },
  {
    "objectID": "final_project.html#specific-predictions",
    "href": "final_project.html#specific-predictions",
    "title": "Airbnb Project",
    "section": "",
    "text": "We expect that listings in neighborhoods with high walkability scores will have higher nightly rates, reflecting the premium on convenience and access to urban amenities.\n\n\n\nTravelers might prefer to stay in walkable areas, suggesting a direct link between walkability and booking rates or price tolerance.\n\n\n\nThe impact of walkability on Airbnb pricing might not be uniform across all boroughs. Manhattan, with its dense urban environment, might exhibit a stronger correlation compared to less densely populated areas like Staten Island."
  },
  {
    "objectID": "final_project.html#simplified-objective",
    "href": "final_project.html#simplified-objective",
    "title": "Airbnb Project",
    "section": "",
    "text": "Our main goal is to analyze whether walkability impacts Airbnb pricing in New York City. Specifically, we are:\n\n\nWe will gather data on Airbnb listings, focusing on price and location, and correlate this with existing walkability scores (e.g., Walk Score).\n\n\n\nWe’ll use statistical methods to determine if there’s a significant relationship between neighborhood walkability and Airbnb listing prices.\n\n\n\n\nFor Hosts: Understand how walkability can be leveraged for pricing strategy.\nFor Travelers: Insight into how walkability might influence their accommodation choices and budget in NYC."
  },
  {
    "objectID": "final_project.html#hypothesis",
    "href": "final_project.html#hypothesis",
    "title": "Airbnb Project / Walkability Index",
    "section": "",
    "text": "Higher walkability positively correlates with higher Airbnb prices due to convenience and proximity to amenities."
  },
  {
    "objectID": "final_project.html#predictions",
    "href": "final_project.html#predictions",
    "title": "Airbnb Project",
    "section": "",
    "text": "Higher Walkability, Higher Prices: Listings in highly walkable neighborhoods are expected to have higher nightly rates due to convenience and urban amenities.\nConsumer Preference: Travelers likely prefer walkable areas, linking walkability to booking rates and price tolerance."
  },
  {
    "objectID": "final_project.html#key-findings",
    "href": "final_project.html#key-findings",
    "title": "Airbnb Project",
    "section": "Key Findings",
    "text": "Key Findings\n\nIntercept (67.775):\n\nRepresents the expected average price when the Walkability Index is zero.\nStatistically significant (P = 0.000) but not practically relevant since Walkability Index values rarely reach zero.\n\nWalkability Index (0.330):\n\nFor each one-unit increase in the Walkability Index, the average price rises by $0.33.\nStatistically significant (P = 0.016) but with a small effect size.\n\nR-Squared (0.001):\n\nIndicates only 0.1% of price variation is explained by the Walkability Index.\nSuggests the Walkability Index is a weak predictor, with other factors likely driving price variations."
  },
  {
    "objectID": "final_project.html#insights",
    "href": "final_project.html#insights",
    "title": "Airbnb Project / Walkability Index",
    "section": "Insights",
    "text": "Insights\n\nWalkability has a small, but statistically significant, effect on Airbnb prices.\nFactors like amenities, neighborhood features, and room size likely have a bigger impact.\nMore detailed analysis with additional factors could give a clearer picture of what drives Airbnb pricing.\n\n\nRecommendations\n\nAdd More Factors:\n\nInclude details like nearby parks, grocery stores, schools, and crime rates.\nLook at things like nightlife, noise levels, and access to public transport.\n\nUse More Geographic Data:\n\nCombine the Walkability Index with data on transit stops, street layout, and nearby shops.\nCreate maps to see how prices vary by location and walkability.\n\nCompare Different Segments:\n\nStudy how walkability affects different property types, like shared rooms versus entire homes.\nLook at differences between cities, suburbs, and rural areas.\n\nRefine Walkability Measures:\n\nFocus on specific parts of the Walkability Index, like pedestrian-friendly streets or access to daily necessities.\nAdd data like real-time foot traffic or reviews about walkability.\n\nTry Advanced Methods:\n\nUse other machine learning to find out which factors matter most for prices.\nGroup neighborhoods by similar walkability and pricing patterns for better insights."
  },
  {
    "objectID": "final_project.html#methodology",
    "href": "final_project.html#methodology",
    "title": "Airbnb Project / Walkability Index",
    "section": "",
    "text": "Data Collection: Combine Airbnb listing data and EPA Walkability Index.\nData Processing: Merge datasets for analysiS and add spatial features.\nData Cleaning: Remove outliers, handle missing values, and preprocess data for analysis.\nExploratory Data Analysis: Use visualization methods to explore correlations and trends in a spatial context.(New York City)\nModeling: Conduct linear regression to quantify walkability’s impact on pricing."
  },
  {
    "objectID": "final_project.html#data-sources",
    "href": "final_project.html#data-sources",
    "title": "Airbnb Project / Walkability Index",
    "section": "Data Sources",
    "text": "Data Sources\n\nAirbnb Listings: Data from 2019 and 2023.\nWalkability Index: EPA Walkability Index API.\nGeospatial Data: NYC Census Block Groups (CBGs)."
  },
  {
    "objectID": "final_project.html#fetching-walkability-index-data",
    "href": "final_project.html#fetching-walkability-index-data",
    "title": "Airbnb Project / Walkability Index",
    "section": "Fetching Walkability Index Data",
    "text": "Fetching Walkability Index Data\nNow we proceed to download the Walkability Index data from the EPA API.\nThe documentation for the API can be found here. The following code would help you download the data from the API."
  },
  {
    "objectID": "final_project.html#steps",
    "href": "final_project.html#steps",
    "title": "Airbnb Project / Walkability Index",
    "section": "Steps:",
    "text": "Steps:\n\nData Cleaning:\n\nSelect relevant columns: neighbourhood_group, neighbourhood, room_type, price, GEOID, and NatWalkInd.\n\nRemove rows with missing values in the price or NatWalkInd columns.\n\nOutlier Detection and Removal:\n\nUse the Interquartile Range (IQR) method to identify outliers in the price column.\n\nRemove rows with outlier prices to improve data consistency.\n\n\n\n\nShow the code\n# Select and clean necessary columns\nairbnb_cleaned &lt;- airbnb_with_walkability |&gt;\n  select(neighbourhood_group, neighbourhood, room_type, price, GEOID, NatWalkInd) |&gt; # Select relevant columns\n  filter(!is.na(price) & !is.na(NatWalkInd))  # Remove rows with missing price or walkability index\n\n\n#Outliers\n# Define the IQR-based outlier threshold function\noutlier_threshold &lt;- function(x) {\n  Q1 &lt;- quantile(x, 0.25, na.rm = TRUE)\n  Q3 &lt;- quantile(x, 0.75, na.rm = TRUE)\n  IQR &lt;- Q3 - Q1\n  lower &lt;- Q1 - 1.5 * IQR\n  upper &lt;- Q3 + 1.5 * IQR\n  return(list(lower = lower, upper = upper))\n}\n\n# Apply the function to identify outlier thresholds\nthresholds &lt;- outlier_threshold(airbnb_cleaned$price)\n\n# Add a column identifying outliers\nairbnb_cleaned &lt;- airbnb_cleaned |&gt;\n  mutate(\n    is_outlier = price &lt; thresholds$lower | price &gt; thresholds$upper\n  )\n\n# Count the number of outliers\nnum_outliers &lt;- sum(airbnb_cleaned$is_outlier, na.rm = TRUE)\n\n# Remove outliers\nairbnb_cleaned_no_outliers &lt;- airbnb_cleaned |&gt;\n  filter(!is_outlier)\n\n# Enhanced histogram with removed outliers annotation\nggplot(airbnb_cleaned, aes(x = price, fill = is_outlier)) +\n  geom_histogram(binwidth = 10, color = \"black\", alpha = 0.7, position = \"identity\") +\n  scale_fill_manual(values = c(\"FALSE\" = \"gray\", \"TRUE\" = \"red\")) +\n  labs(\n    title = \"Airbnb Price Distribution with Outliers Highlighted\",\n    x = \"Price\",\n    y = \"Count\",\n    fill = \"Outlier\"\n  ) +\n  theme_minimal() +\n  xlim(0, quantile(airbnb_cleaned$price, 0.99, na.rm = TRUE)) +  # Optional: Limit x-axis\n  annotate(\n    \"text\",\n    x = quantile(airbnb_cleaned$price, 0.75, na.rm = TRUE),  # Position annotation\n    y = max(table(cut(airbnb_cleaned$price, seq(0, max(airbnb_cleaned$price, na.rm = TRUE), 10)))),\n    label = paste(num_outliers, \"Outliers Removed\"),\n    color = \"red\",\n    size = 5,\n    hjust = 0\n  )\n\n\n\n\n\n\n\n\n\nIn this step, we enhance the Airbnb dataset with geographic context and prepare it for further analysis:\n\nLoad NYC Boundary Shape: The NYC boundary shape is loaded from an open GeoJSON file to establish spatial boundaries for the analysis.(to have a better visualization)\nCoordinate Reference System (CRS) Transformation: The Census Block Groups (CBGs) data is transformed to WGS84 (EPSG:4326) to ensure it aligns with the Airbnb dataset.\nSummarize Data for Private Rooms: The Airbnb data is filtered to include only private room listings. These listings are grouped by GEOID, neighborhood, and neighborhood group to calculate:\n\nAverage Price: The mean price for private rooms in each group.\nAverage Walkability Index: The mean walkability index for each group. (Which is the same)\n\nMerge Datasets: The summarized Airbnb data is joined with the CBG dataset using GEOID as a key, enriching the spatial dataset with pricing and walkability metrics.\n\n\n\nShow the code\n# Load NYC boundary shape (GeoJSON from NYC OpenData)\nnyc_boundary &lt;- st_read(\"https://data.cityofnewyork.us/resource/7t3b-ywvw.geojson\")\n\nny_block_groups &lt;- st_transform(ny_block_groups, crs = 4326)\n\n# Step 1: Group and summarize data for private rooms\nprivate_room_avg_price &lt;- airbnb_cleaned_no_outliers |&gt;\n  filter(room_type == \"Private room\") |&gt;  # Filter for private rooms\n  st_set_geometry(NULL) |&gt;                # Temporarily remove geometry\n  group_by(GEOID, neighbourhood_group, neighbourhood) |&gt;  # Group by GEOID and neighborhoods\n  summarize(\n    avg_price = mean(price, na.rm = TRUE),       # Calculate average price\n    avg_NatWalkInd = mean(NatWalkInd, na.rm = TRUE)  # Calculate average walkability index\n  )\n\n# Join the average price back to the original spatial dataset\nny_block_groups_with_price &lt;- ny_block_groups |&gt;\n  left_join(private_room_avg_price, by = c(\"GEOID\" = \"GEOID\"))"
  },
  {
    "objectID": "final_project.html#visualizing-the-average-price-distribution",
    "href": "final_project.html#visualizing-the-average-price-distribution",
    "title": "Airbnb Project / Walkability Index",
    "section": "Visualizing the Average Price Distribution",
    "text": "Visualizing the Average Price Distribution\n\n\nShow the code\n# Create a color palette for the heatmap\npal &lt;- colorNumeric(\n  palette = \"YlOrRd\",  # Choose a heatmap color palette\n  domain = ny_block_groups_with_price$avg_price,  # Domain of average prices\n  na.color = \"transparent\"  # Handle missing values\n)\n\n# Create the Leaflet map with a heatmap-like color gradient\nleaflet(data = ny_block_groups_with_price) |&gt;\n  addTiles() |&gt;  # Add default OpenStreetMap tiles\n  addPolygons(\n    fillColor = ~pal(avg_price),  # Use color palette for the fill color\n    color = \"black\",              # Border color\n    weight = 1,                   # Border weight\n    fillOpacity = 0.7,            # Transparency for the fill\n    popup = ~paste0(\n      \"GEOID: \", GEOID, \"&lt;br&gt;\",\n      \"Average Price: $\", round(avg_price, 2)\n    )\n  ) |&gt;\n  addLegend(\n    pal = pal, values = ny_block_groups_with_price$avg_price,\n    position = \"bottomright\",\n    title = \"Average Price\",\n    opacity = 1\n  ) |&gt;\n  setView(lng = -73.935242, lat = 40.730610, zoom = 12)  # Center on NYC"
  },
  {
    "objectID": "final_project.html#walkabilty-index-visualization",
    "href": "final_project.html#walkabilty-index-visualization",
    "title": "Airbnb Project / Walkability Index",
    "section": "Walkabilty index Visualization",
    "text": "Walkabilty index Visualization\nNow we would create a heatmap to visualize the Walkability Index across different neighborhoods in New York City.\n\n\nShow the code\n# Create a color palette for the heatmap\npal &lt;- colorNumeric(\n  palette = \"Greens\",  # Green color palette\n  domain = ny_block_groups_with_price$avg_NatWalkInd,  # Domain of average prices\n  na.color = \"transparent\"  # Handle missing values\n)\n# Create the Leaflet map with a heatmap-like color gradient\nleaflet(data = ny_block_groups_with_price) |&gt;\n  addTiles() |&gt;  # Add default OpenStreetMap tiles\n  addPolygons(\n    fillColor = ~pal(avg_NatWalkInd),  # Use color palette for the fill color\n    color = \"black\",              # Border color\n    weight = 1,                   # Border weight\n    fillOpacity = 0.7,            # Transparency for the fill\n    popup = ~paste0(\n      \"GEOID: \", GEOID, \"&lt;br&gt;\",\n      \"Walkability Index\", round(avg_NatWalkInd, 2)\n    )\n  ) |&gt;\n  addLegend(\n    pal = pal, values = ny_block_groups_with_price$avg_NatWalkInd,\n    position = \"bottomright\",\n    title = \"Walkability Index\",\n    opacity = 1\n  ) |&gt;\n  setView(lng = -73.935242, lat = 40.730610, zoom = 12)  # Center on NYC"
  },
  {
    "objectID": "individual_report.html",
    "href": "individual_report.html",
    "title": "Airbnb Project / Walkability Index",
    "section": "",
    "text": "Does the walkability index of NYC affects the listing pricing?\n\n\nThis project investigates how walkability influences Airbnb pricing in New York City. By integrating walkability indices with Airbnb listings data, we aim to uncover patterns that can benefit both hosts and travelers.\n\nHosts: Leverage walkability insights to optimize pricing.\nTravelers: Understand walkability’s impact on accommodation choices and budgets.\n\n\n\n\nHigher walkability positively correlates with higher Airbnb prices due to convenience and proximity to amenities.\n\n\n\n\nData Collection: Combine Airbnb listing data and EPA Walkability Index.\nData Processing: Merge datasets for analysiS and add spatial features.\nData Cleaning: Remove outliers, handle missing values, and preprocess data for analysis.\nExploratory Data Analysis: Use visualization methods to explore correlations and trends in a spatial context.(New York City)\nModeling: Conduct linear regression to quantify walkability’s impact on pricing."
  },
  {
    "objectID": "individual_report.html#objective",
    "href": "individual_report.html#objective",
    "title": "Airbnb Project / Walkability Index",
    "section": "",
    "text": "This project investigates how walkability influences Airbnb pricing in New York City. By integrating walkability indices with Airbnb listings data, we aim to uncover patterns that can benefit both hosts and travelers.\n\nHosts: Leverage walkability insights to optimize pricing.\nTravelers: Understand walkability’s impact on accommodation choices and budgets."
  },
  {
    "objectID": "individual_report.html#hypothesis",
    "href": "individual_report.html#hypothesis",
    "title": "Airbnb Project / Walkability Index",
    "section": "",
    "text": "Higher walkability positively correlates with higher Airbnb prices due to convenience and proximity to amenities."
  },
  {
    "objectID": "individual_report.html#methodology",
    "href": "individual_report.html#methodology",
    "title": "Airbnb Project / Walkability Index",
    "section": "",
    "text": "Data Collection: Combine Airbnb listing data and EPA Walkability Index.\nData Processing: Merge datasets for analysiS and add spatial features.\nData Cleaning: Remove outliers, handle missing values, and preprocess data for analysis.\nExploratory Data Analysis: Use visualization methods to explore correlations and trends in a spatial context.(New York City)\nModeling: Conduct linear regression to quantify walkability’s impact on pricing."
  },
  {
    "objectID": "individual_report.html#data-sources",
    "href": "individual_report.html#data-sources",
    "title": "Airbnb Project / Walkability Index",
    "section": "Data Sources",
    "text": "Data Sources\n\nAirbnb Listings: Data from 2019 and 2023.\nWalkability Index: EPA Walkability Index API.\nGeospatial Data: NYC Census Block Groups (CBGs)."
  },
  {
    "objectID": "individual_report.html#libraries",
    "href": "individual_report.html#libraries",
    "title": "Airbnb Project / Walkability Index",
    "section": "Libraries",
    "text": "Libraries\n\n\nShow the code\nlibrary(dplyr)       # Data manipulation\nlibrary(sf)          # Handling spatial data\nlibrary(geosphere)   # Geospatial calculations\nlibrary(httr)        # HTTP requests\nlibrary(tigris)      # Census and geographic data\nlibrary(ggplot2)     # Visualization\nlibrary(leaflet)     # Interactive mapping\nlibrary(gt)          # Table formatting"
  },
  {
    "objectID": "individual_report.html#fetching-walkability-index-data",
    "href": "individual_report.html#fetching-walkability-index-data",
    "title": "Airbnb Project / Walkability Index",
    "section": "Fetching Walkability Index Data",
    "text": "Fetching Walkability Index Data\nNow we proceed to download the Walkability Index data from the EPA API.\nThe documentation for the API can be found here. The following code would help you download the data from the API."
  },
  {
    "objectID": "individual_report.html#adding-spatial-features-to-airbnb-data",
    "href": "individual_report.html#adding-spatial-features-to-airbnb-data",
    "title": "Airbnb Project / Walkability Index",
    "section": "Adding Spatial Features to Airbnb Data",
    "text": "Adding Spatial Features to Airbnb Data\nTo incorporate geographic context into the Airbnb dataset, we:\n\nDownload Census Block Groups (CBGs): Use the tigris package to get New York State CBGs.\nPerform a Spatial Join: Map Airbnb latitude/longitude points to their corresponding CBGs.\nAdd GEOID: Assign each Airbnb entry a GEOID, aligning with the Walkability Index’s GEOID20 for seamless merging.\n\nThis enhances the Airbnb dataset with geographic information for further analysis.\n\n\nShow the code\n# Download Census Block Groups for New York State\nny_block_groups &lt;- block_groups(state = \"NY\", year = 2020, cb = TRUE)\n\n# Check if CRS is undefined\nif (is.na(st_crs(ny_block_groups))) {\n  stop(\"CRS for ny_block_groups is undefined. Define it before proceeding.\")\n}\n\n# Assign WGS84 CRS to airbnb_filtered if it uses lat/lon\nif (is.na(st_crs(airbnb_filtered))) {\n  airbnb_sf &lt;- st_as_sf(airbnb_filtered, coords = c(\"longitude\", \"latitude\"), crs = 4326)\n} else {\n  airbnb_sf &lt;- st_as_sf(airbnb_filtered, coords = c(\"longitude\", \"latitude\"))\n}\n\n# Transform CRS of airbnb_sf to match ny_block_groups if necessary\nif (!st_crs(airbnb_sf)$epsg == st_crs(ny_block_groups)$epsg) {\n  airbnb_sf &lt;- st_transform(airbnb_sf, crs = st_crs(ny_block_groups))\n}\n\n# Perform spatial join to associate attributes\nairbnb_with_block &lt;- st_join(airbnb_sf, ny_block_groups, join = st_within)\n\n# Replace point geometries with the corresponding block group geometries\nairbnb_with_block$geometry &lt;- ny_block_groups$geometry[match(\n  airbnb_with_block$GEOID, ny_block_groups$GEOID\n)]\n\n#Now we proceed to merge the walkability index data with the airbnb data set.\n\n# Step 1: Remove `sf` class from walkability\nwalkability_no_sf &lt;- st_drop_geometry(walkability)\n\n# Step 2: Perform attribute join\nairbnb_with_walkability &lt;- airbnb_with_block |&gt;\n  left_join(walkability_no_sf, by = c(\"GEOID\" = \"GEOID20\"))"
  },
  {
    "objectID": "individual_report.html#steps",
    "href": "individual_report.html#steps",
    "title": "Airbnb Project / Walkability Index",
    "section": "Steps:",
    "text": "Steps:\n\nData Cleaning:\n\nSelect relevant columns: neighbourhood_group, neighbourhood, room_type, price, GEOID, and NatWalkInd.\n\nRemove rows with missing values in the price or NatWalkInd columns.\n\nOutlier Detection and Removal:\n\nUse the Interquartile Range (IQR) method to identify outliers in the price column.\n\nRemove rows with outlier prices to improve data consistency.\n\n\n\n\nShow the code\n# Select and clean necessary columns\nairbnb_cleaned &lt;- airbnb_with_walkability |&gt;\n  select(neighbourhood_group, neighbourhood, room_type, price, GEOID, NatWalkInd) |&gt; # Select relevant columns\n  filter(!is.na(price) & !is.na(NatWalkInd))  # Remove rows with missing price or walkability index\n\n\n#Outliers\n# Define the IQR-based outlier threshold function\noutlier_threshold &lt;- function(x) {\n  Q1 &lt;- quantile(x, 0.25, na.rm = TRUE)\n  Q3 &lt;- quantile(x, 0.75, na.rm = TRUE)\n  IQR &lt;- Q3 - Q1\n  lower &lt;- Q1 - 1.5 * IQR\n  upper &lt;- Q3 + 1.5 * IQR\n  return(list(lower = lower, upper = upper))\n}\n\n# Apply the function to identify outlier thresholds\nthresholds &lt;- outlier_threshold(airbnb_cleaned$price)\n\n# Add a column identifying outliers\nairbnb_cleaned &lt;- airbnb_cleaned |&gt;\n  mutate(\n    is_outlier = price &lt; thresholds$lower | price &gt; thresholds$upper\n  )\n\n# Count the number of outliers\nnum_outliers &lt;- sum(airbnb_cleaned$is_outlier, na.rm = TRUE)\n\n# Remove outliers\nairbnb_cleaned_no_outliers &lt;- airbnb_cleaned |&gt;\n  filter(!is_outlier)\n\n# Enhanced histogram with removed outliers annotation\nggplot(airbnb_cleaned, aes(x = price, fill = is_outlier)) +\n  geom_histogram(binwidth = 10, color = \"black\", alpha = 0.7, position = \"identity\") +\n  scale_fill_manual(values = c(\"FALSE\" = \"gray\", \"TRUE\" = \"red\")) +\n  labs(\n    title = \"Airbnb Price Distribution with Outliers Highlighted\",\n    x = \"Price\",\n    y = \"Count\",\n    fill = \"Outlier\"\n  ) +\n  theme_minimal() +\n  xlim(0, quantile(airbnb_cleaned$price, 0.99, na.rm = TRUE)) +  # Optional: Limit x-axis\n  annotate(\n    \"text\",\n    x = quantile(airbnb_cleaned$price, 0.75, na.rm = TRUE),  # Position annotation\n    y = max(table(cut(airbnb_cleaned$price, seq(0, max(airbnb_cleaned$price, na.rm = TRUE), 10)))),\n    label = paste(num_outliers, \"Outliers Removed\"),\n    color = \"red\",\n    size = 5,\n    hjust = 0\n  )\n\n\n\n\n\n\n\n\n\nIn this step, we enhance the Airbnb dataset with geographic context and prepare it for further analysis:\n\nLoad NYC Boundary Shape: The NYC boundary shape is loaded from an open GeoJSON file to establish spatial boundaries for the analysis.(to have a better visualization)\nCoordinate Reference System (CRS) Transformation: The Census Block Groups (CBGs) data is transformed to WGS84 (EPSG:4326) to ensure it aligns with the Airbnb dataset.\nSummarize Data for Private Rooms: The Airbnb data is filtered to include only private room listings. These listings are grouped by GEOID, neighborhood, and neighborhood group to calculate:\n\nAverage Price: The mean price for private rooms in each group.\nAverage Walkability Index: The mean walkability index for each group. (Which is the same)\n\nMerge Datasets: The summarized Airbnb data is joined with the CBG dataset using GEOID as a key, enriching the spatial dataset with pricing and walkability metrics.\n\n\n\nShow the code\n# Load NYC boundary shape (GeoJSON from NYC OpenData)\nnyc_boundary &lt;- st_read(\"https://data.cityofnewyork.us/resource/7t3b-ywvw.geojson\")\n\nny_block_groups &lt;- st_transform(ny_block_groups, crs = 4326)\n\n# Step 1: Group and summarize data for private rooms\nprivate_room_avg_price &lt;- airbnb_cleaned_no_outliers |&gt;\n  filter(room_type == \"Private room\") |&gt;  # Filter for private rooms\n  st_set_geometry(NULL) |&gt;                # Temporarily remove geometry\n  group_by(GEOID, neighbourhood_group, neighbourhood) |&gt;  # Group by GEOID and neighborhoods\n  summarize(\n    avg_price = mean(price, na.rm = TRUE),       # Calculate average price\n    avg_NatWalkInd = mean(NatWalkInd, na.rm = TRUE)  # Calculate average walkability index\n  )\n\n# Join the average price back to the original spatial dataset\nny_block_groups_with_price &lt;- ny_block_groups |&gt;\n  left_join(private_room_avg_price, by = c(\"GEOID\" = \"GEOID\"))"
  },
  {
    "objectID": "individual_report.html#visualizing-the-average-price-distribution",
    "href": "individual_report.html#visualizing-the-average-price-distribution",
    "title": "Airbnb Project / Walkability Index",
    "section": "Visualizing the Average Price Distribution",
    "text": "Visualizing the Average Price Distribution\n\n\nShow the code\n# Create a color palette for the heatmap\npal &lt;- colorNumeric(\n  palette = \"YlOrRd\",  # Choose a heatmap color palette\n  domain = ny_block_groups_with_price$avg_price,  # Domain of average prices\n  na.color = \"transparent\"  # Handle missing values\n)\n\n# Create the Leaflet map with a heatmap-like color gradient\nleaflet(data = ny_block_groups_with_price) |&gt;\n  addTiles() |&gt;  # Add default OpenStreetMap tiles\n  addPolygons(\n    fillColor = ~pal(avg_price),  # Use color palette for the fill color\n    color = \"black\",              # Border color\n    weight = 1,                   # Border weight\n    fillOpacity = 0.7,            # Transparency for the fill\n    popup = ~paste0(\n      \"GEOID: \", GEOID, \"&lt;br&gt;\",\n      \"Average Price: $\", round(avg_price, 2)\n    )\n  ) |&gt;\n  addLegend(\n    pal = pal, values = ny_block_groups_with_price$avg_price,\n    position = \"bottomright\",\n    title = \"Average Price\",\n    opacity = 1\n  ) |&gt;\n  setView(lng = -73.935242, lat = 40.730610, zoom = 12)  # Center on NYC"
  },
  {
    "objectID": "individual_report.html#walkabilty-index-visualization",
    "href": "individual_report.html#walkabilty-index-visualization",
    "title": "Airbnb Project / Walkability Index",
    "section": "Walkabilty index Visualization",
    "text": "Walkabilty index Visualization\nNow we would create a heatmap to visualize the Walkability Index across different neighborhoods in New York City.\n\n\nShow the code\n# Create a color palette for the heatmap\npal &lt;- colorNumeric(\n  palette = \"Greens\",  # Green color palette\n  domain = ny_block_groups_with_price$avg_NatWalkInd,  # Domain of average prices\n  na.color = \"transparent\"  # Handle missing values\n)\n# Create the Leaflet map with a heatmap-like color gradient\nleaflet(data = ny_block_groups_with_price) |&gt;\n  addTiles() |&gt;  # Add default OpenStreetMap tiles\n  addPolygons(\n    fillColor = ~pal(avg_NatWalkInd),  # Use color palette for the fill color\n    color = \"black\",              # Border color\n    weight = 1,                   # Border weight\n    fillOpacity = 0.7,            # Transparency for the fill\n    popup = ~paste0(\n      \"GEOID: \", GEOID, \"&lt;br&gt;\",\n      \"Walkability Index\", round(avg_NatWalkInd, 2)\n    )\n  ) |&gt;\n  addLegend(\n    pal = pal, values = ny_block_groups_with_price$avg_NatWalkInd,\n    position = \"bottomright\",\n    title = \"Walkability Index\",\n    opacity = 1\n  ) |&gt;\n  setView(lng = -73.935242, lat = 40.730610, zoom = 12)  # Center on NYC"
  },
  {
    "objectID": "individual_report.html#insights",
    "href": "individual_report.html#insights",
    "title": "Airbnb Project / Walkability Index",
    "section": "Insights",
    "text": "Insights\n\nWalkability has a small, but statistically significant, effect on Airbnb prices.\nFactors like amenities, neighborhood features, and room size likely have a bigger impact.\nMore detailed analysis with additional factors could give a clearer picture of what drives Airbnb pricing.\n\n\nRecommendations\n\nAdd More Factors:\n\nInclude details like nearby parks, grocery stores, schools, and crime rates.\nLook at things like nightlife, noise levels, and access to public transport.\n\nUse More Geographic Data:\n\nCombine the Walkability Index with data on transit stops, street layout, and nearby shops.\nCreate maps to see how prices vary by location and walkability.\n\nCompare Different Segments:\n\nStudy how walkability affects different property types, like shared rooms versus entire homes.\nLook at differences between cities, suburbs, and rural areas.\n\nRefine Walkability Measures:\n\nFocus on specific parts of the Walkability Index, like pedestrian-friendly streets or access to daily necessities.\nAdd data like real-time foot traffic or reviews about walkability.\n\nTry Advanced Methods:\n\nUse other machine learning to find out which factors matter most for prices.\nGroup neighborhoods by similar walkability and pricing patterns for better insights."
  }
]